{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from moco import SupConLoss, my_Densenet2\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "########## Mean and std are calculated from the train dataset\n",
    "# normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n",
    "#                                      std=[0.33165374, 0.33165374, 0.33165374])\n",
    "# train_transformer = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "# #     transforms.RandomRotation(90),\n",
    "#     # random brightness and random contrast\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "#     transforms.ToTensor(),\n",
    "#     normalize\n",
    "# ])\n",
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "\n",
    "    def __init__(self, transform, transform2=None):\n",
    "        self.transform = transform\n",
    "        self.transform2 = transform if transform2 is None else transform2\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform2(x)]\n",
    "\n",
    "\n",
    "train_transform1 = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_transform2 = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_transformer = TwoCropTransform(train_transform1, train_transform2)\n",
    "\n",
    "val_transformer = transforms.Compose([\n",
    "    #     transforms.Resize(224),\n",
    "    #     transforms.CenterCrop(224),\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Load LUNA dataset'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load LUNA dataset'''\n",
    "\n",
    "# import h5py \n",
    "# import numpy as np\n",
    "# import skimage\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import TensorDataset\n",
    "# f = h5py.File('all_patches.hdf5','r')\n",
    "# f.keys()\n",
    "# img = f['ct_slices'][:]  \n",
    "# label = f['slice_class'][:] \n",
    "# f.close()\n",
    "# print(np.shape(img))\n",
    "# print('b',np.shape(label))\n",
    "# skimage.io.imshow(img[120])\n",
    "# print(label[120])\n",
    "# batchsize=4\n",
    "\n",
    "# class LungDataset(Dataset):\n",
    "#     def __init__(self, img, label, transform=None):\n",
    "#         self.img = img\n",
    "#         self.label = label\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "\n",
    "#         image = PIL_image = Image.fromarray(self.img[idx]).convert('RGB')\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         sample = {'img': image,\n",
    "#                   'label': int(self.label[idx])}\n",
    "#         return sample\n",
    "\n",
    "# trainset = LungDataset(img, label, transform= val_transformer)\n",
    "# valset = LungDataset(img, label, transform= val_transformer)\n",
    "# train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
    "# val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "# modelname = 'medical_transfer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n",
      "118\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "batchsize = 10\n",
    "\n",
    "\n",
    "def read_txt(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()\n",
    "    txt_data = [line.strip() for line in lines]\n",
    "    return txt_data\n",
    "\n",
    "\n",
    "class CovidCTDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt_path (string): Path to the txt file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        File structure:\n",
    "        - root_dir\n",
    "            - CT_COVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "            - CT_NonCOVID\n",
    "                - img1.png\n",
    "                - img2.png\n",
    "                - ......\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.txt_path = [txt_COVID, txt_NonCOVID]\n",
    "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
    "        self.num_cls = len(self.classes)\n",
    "        self.img_list = []\n",
    "        for c in range(self.num_cls):\n",
    "            cls_list = [[os.path.join(self.root_dir, self.classes[c], item), c] for item in read_txt(self.txt_path[c])]\n",
    "            self.img_list += cls_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.img_list[idx][0]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        sample = {'img': image,\n",
    "                  'label': int(self.img_list[idx][1])}\n",
    "        return sample\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainset = CovidCTDataset(root_dir='/home/wangchenxu/covid/Images-processed',\n",
    "                              txt_COVID='/home/wangchenxu/covid/Data-split/COVID/trainCT_COVID.txt',\n",
    "                              txt_NonCOVID='/home/wangchenxu/covid/Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
    "                              transform=train_transformer)\n",
    "    valset = CovidCTDataset(root_dir='/home/wangchenxu/covid/Images-processed',\n",
    "                            txt_COVID='/home/wangchenxu/covid/Data-split/COVID/valCT_COVID.txt',\n",
    "                            txt_NonCOVID='/home/wangchenxu/covid/Data-split/NonCOVID/valCT_NonCOVID.txt',\n",
    "                            transform=val_transformer)\n",
    "    testset = CovidCTDataset(root_dir='/home/wangchenxu/covid/Images-processed',\n",
    "                             txt_COVID='/home/wangchenxu/covid/Data-split/COVID/testCT_COVID.txt',\n",
    "                             txt_NonCOVID='/home/wangchenxu/covid/Data-split/NonCOVID/testCT_NonCOVID.txt',\n",
    "                             transform=val_transformer)\n",
    "    print(trainset.__len__())\n",
    "    print(valset.__len__())\n",
    "    print(testset.__len__())\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
    "    val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
    "    test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f758c5e2c40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_index, batch_samples in enumerate(train_loader):\n",
    "    batch_samples['img'] = torch.cat([batch_samples['img'][0], batch_samples['img'][1]], dim=0)\n",
    "    #         batch_samples['label'] = torch.cat([batch_samples['label'][0], batch_samples['label'][1]], dim=0)\n",
    "    #           print(type(batch_samples['label']))\n",
    "    #         print(batch_samples['label'])\n",
    "    data, target = batch_samples['img'], batch_samples['label']\n",
    "#         data = torch.cat([data[0],data[1]],dim=0)\n",
    "#         print(target)\n",
    "skimage.io.imshow(data[0, 1, :, :].numpy())\n",
    "# data[0].shape,data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training process is defined here \n",
    "\n",
    "alpha = None\n",
    "## alpha is None if mixup is not used\n",
    "alpha_name = f'{alpha}'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "def train(optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    # train_loss = 0\n",
    "    train_correct = 0\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    criterion = SupConLoss.SupConLoss()\n",
    "\n",
    "    for batch_index, batch_samples in enumerate(train_loader):\n",
    "\n",
    "        # move data to device\n",
    "        batch_samples['img'] = torch.cat([batch_samples['img'][0], batch_samples['img'][1]], dim=0)\n",
    "        #         data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "        data, target = batch_samples['img'].cuda(), batch_samples['label'].cuda()\n",
    "\n",
    "        ## adjust data to meet the input dimension of model\n",
    "        #         data = data[:, 0, :, :]\n",
    "        #         data = data[:, None, :, :]\n",
    "\n",
    "        #mixup\n",
    "        #         data, targets_a, targets_b, lam = mixup_data(data, target, alpha, use_cuda=True)\n",
    "\n",
    "        #         target = target.cuda(None, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        bsz = target.shape[0]\n",
    "\n",
    "        output, features = model(data)\n",
    "        #         out = model(data)\n",
    "        #         output = out\n",
    "        #         features = F.normalize(out, dim=1)\n",
    "\n",
    "        loss1 = criteria(output, target.repeat(2).long())\n",
    "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "        loss2 = criterion(features, target.long())\n",
    "        loss = loss1 + 0.01 * loss2\n",
    "        #mixup loss\n",
    "        #         loss = mixup_criterion(criteria, output, targets_a, targets_b, lam)\n",
    "\n",
    "        # train_loss += loss1 + loss2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.repeat(2).long().view_as(pred)).sum().item()\n",
    "\n",
    "        # Display progress and write to tensorboard\n",
    "        if batch_index % bs == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
    "                epoch, batch_index, len(train_loader),\n",
    "                100.0 * batch_index / len(train_loader), loss.item() / bs))\n",
    "\n",
    "#     print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
    "#         100.0 * train_correct / len(train_loader.dataset)))\n",
    "#     f = open('model_result/{}.txt'.format(modelname), 'a+')\n",
    "#     f.write('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset),\n",
    "#         100.0 * train_correct / len(train_loader.dataset)))\n",
    "#     f.write('\\n')\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val process is defined here\n",
    "\n",
    "def val(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "\n",
    "        predlist = []\n",
    "        scorelist = []\n",
    "        targetlist = []\n",
    "        # Predict\n",
    "        for batch_index, batch_samples in enumerate(val_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "\n",
    "            #             data = data[:, 0, :, :]\n",
    "            #             data = data[:, None, :, :]\n",
    "            output, feature = model(data)\n",
    "            #             out = model(data)\n",
    "            #             output = out\n",
    "            #             feature = F.normalize(out, dim=1)\n",
    "            test_loss += criteria(output, target.long())\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            #             print('target',target.long()[:, 2].view_as(pred))\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "\n",
    "            #             print(output[:,1].cpu().numpy())\n",
    "            #             print((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            #             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            targetcpu = target.long().cpu().numpy()\n",
    "            predlist = np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist = np.append(scorelist, score.cpu().numpy()[:, 1])\n",
    "            targetlist = np.append(targetlist, targetcpu)\n",
    "\n",
    "    return targetlist, scorelist, predlist\n",
    "\n",
    "    # Write to tensorboard\n",
    "\n",
    "#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test process is defined here \n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    results = []\n",
    "\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    # Don't update model\n",
    "    with torch.no_grad():\n",
    "        tpr_list = []\n",
    "        fpr_list = []\n",
    "\n",
    "        predlist = []\n",
    "        scorelist = []\n",
    "        targetlist = []\n",
    "        # Predict\n",
    "        for batch_index, batch_samples in enumerate(test_loader):\n",
    "            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)\n",
    "            #             data = data[:, 0, :, :]\n",
    "            #             data = data[:, None, :, :]\n",
    "            #             print(target)\n",
    "            output, feature = model(data)\n",
    "            #             out = model(data)\n",
    "            #             output = out\n",
    "            #             feature = F.normalize(out, dim=1)\n",
    "            test_loss += criteria(output, target.long())\n",
    "            score = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            #             print('target',target.long()[:, 2].view_as(pred))\n",
    "            correct += pred.eq(target.long().view_as(pred)).sum().item()\n",
    "            #             TP += ((pred == 1) & (target.long()[:, 2].view_as(pred).data == 1)).cpu().sum()\n",
    "            #             TN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
    "            # #             # FN    predict 0 label 1\n",
    "            #             FN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 1)).cpu().sum()\n",
    "            # #             # FP    predict 1 label 0\n",
    "            #             FP += ((pred == 1) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()\n",
    "            #             print(TP,TN,FN,FP)\n",
    "\n",
    "            #             print(output[:,1].cpu().numpy())\n",
    "            #             print((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            #             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())\n",
    "            targetcpu = target.long().cpu().numpy()\n",
    "            predlist = np.append(predlist, pred.cpu().numpy())\n",
    "            scorelist = np.append(scorelist, score.cpu().numpy()[:, 1])\n",
    "            targetlist = np.append(targetlist, targetcpu)\n",
    "    return targetlist, scorelist, predlist\n",
    "\n",
    "    # Write to tensorboard\n",
    "\n",
    "#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Dense169\n",
    "\n",
    "# model = models.densenet169(pretrained=True).cuda()\n",
    "model = my_Densenet2.densenet169(pretrained=True).cuda()\n",
    "# # modelname = 'Dense169'\n",
    "\"\"\"load MoCo pretrained model\"\"\"\n",
    "checkpoint = torch.load('save_model_dense1/checkpoint_luna_covid_KL+++.tar')\n",
    "# # # print(checkpoint['arch'])\n",
    "\n",
    "state_dict = checkpoint['state_dict']\n",
    "for key in list(state_dict.keys()):\n",
    "    if 'module.encoder_q' in key:\n",
    "        #         print(key[17:])\n",
    "        new_key = key[17:]\n",
    "        state_dict[new_key] = state_dict[key]\n",
    "    del state_dict[key]\n",
    "# for key in list(state_dict.keys()):\n",
    "#     if  key == 'classifier.0.weight':\n",
    "#         new_key = 'classifier.weight'\n",
    "#         state_dict[new_key] = state_dict[key]\n",
    "#         del state_dict[key]\n",
    "#     if  key == 'classifier.0.bias':\n",
    "#         new_key = 'classifier.bias'\n",
    "#         state_dict[new_key] = state_dict[key]\n",
    "#         del state_dict[key]\n",
    "#     if  key == 'classifier.2.weight' or key == 'classifier.2.bias':\n",
    "#         new_key = key.replace('classifier.2', 'classifier2')\n",
    "#         state_dict[new_key] = state_dict[key]\n",
    "#         del state_dict[key]\n",
    "# state_dict['classifier.weight'] = state_dict['classifier.weight'][:1000,:]\n",
    "# state_dict['classifier.bias'] = state_dict['classifier.bias'][:1000]\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model, num_features=128):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.classifier = nn.Linear(num_features, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.model(x)\n",
    "        logits = self.classifier(feat)\n",
    "        target = F.normalize(feat,dim = 1)\n",
    "        return logits, target\n",
    "\n",
    "\n",
    "model = ModelWrapper(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Change names and locations to the Self-Trans.pt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Load Self-Trans model\"\"\"\n",
    "\"\"\"Change names and locations to the Self-Trans.pt\"\"\"\n",
    "\n",
    "\n",
    "# model = models.densenet169(pretrained=True).cuda()\n",
    "# # pretrained_net = torch.load('model_backup/Dense169.pt')\n",
    "# # pretrained_net = torch.load('model_backup/mixup/Dense169_0.6.pt')\n",
    "# pretrained_net = torch.load('save_model_dense/Dense169_ssl_luna_moco_None_covid_moco_covid.pt')\n",
    "\n",
    "\n",
    "# model.load_state_dict(pretrained_net)\n",
    "\n",
    "# modelname = 'Dense169_ssl_luna_moco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/43 (0%)]\tTrain Loss: 0.380022\n",
      "Train Epoch: 1 [10/43 (23%)]\tTrain Loss: 0.087460\n",
      "Train Epoch: 1 [20/43 (47%)]\tTrain Loss: 0.104142\n",
      "Train Epoch: 1 [30/43 (70%)]\tTrain Loss: 0.277624\n",
      "Train Epoch: 1 [40/43 (93%)]\tTrain Loss: 0.118302\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99753976 0.96711105 0.85944206 0.80725873 0.35308003 0.94436198\n",
      " 0.97952729 0.80727512 0.79342455 0.21205734 0.89675856 0.19260852\n",
      " 0.11304358 0.42650574 0.19368348 0.81856817 0.04619696 0.27807254\n",
      " 0.07295141 0.80582196 0.02265036 0.13476534 0.0532118  0.01306698\n",
      " 0.0858332  0.15429945 0.43882629 0.29018986 0.26913086 0.23532961\n",
      " 0.38535365 0.23322743 0.41570842 0.48274893 0.19512324 0.59946746\n",
      " 0.61706692 0.90566677 0.24201873 0.40004084 0.90537781 0.90129143\n",
      " 0.60078198 0.60229141 0.37170565 0.78152055 0.99768901 0.99929428\n",
      " 0.97871715 0.06196006 0.99913883 0.85727209 0.94221509 0.765342\n",
      " 0.2553741  0.20124988 0.96508151 0.76858753 0.45202351 0.97810328\n",
      " 0.51641399 0.24110636 0.0977317  0.38327515 0.32647669 0.76560515\n",
      " 0.52062446 0.05531551 0.60028732 0.1319263  0.14696047 0.39522389\n",
      " 0.03235166 0.45635277 0.35631558 0.05467277 0.98586559 0.99682212\n",
      " 0.22945026 0.58870876 0.46613047 0.29839113 0.40191969 0.17050603\n",
      " 0.74905384 0.87617278 0.44334221 0.60383445 0.03286844 0.0219364\n",
      " 0.02752621 0.68235105 0.80960321 0.03451405 0.407397   0.495976\n",
      " 0.8215403  0.02005909 0.02753197 0.4645777  0.09232707 0.38479903\n",
      " 0.27182961 0.47653398 0.38186145 0.09721278 0.24291849 0.79785335\n",
      " 0.45342895 0.80144507 0.0271283  0.43694428 0.66527426 0.43598121\n",
      " 0.00473547 0.06125377 0.63795537 0.56487399]\n",
      "predict [1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 2 [0/43 (0%)]\tTrain Loss: 0.154984\n",
      "Train Epoch: 2 [10/43 (23%)]\tTrain Loss: 0.315724\n",
      "Train Epoch: 2 [20/43 (47%)]\tTrain Loss: 0.208157\n",
      "Train Epoch: 2 [30/43 (70%)]\tTrain Loss: 0.084924\n",
      "Train Epoch: 2 [40/43 (93%)]\tTrain Loss: 0.116283\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.94949758 0.9990977  0.98941165 0.65076762 0.70005816 0.71952844\n",
      " 0.99657696 0.61903089 0.7165857  0.31491408 0.55800992 0.62205195\n",
      " 0.69893819 0.83771515 0.84328824 0.77274334 0.52021664 0.45668873\n",
      " 0.86004287 0.81677502 0.53922272 0.69789916 0.90949833 0.61256355\n",
      " 0.30446073 0.36030906 0.56356251 0.85350788 0.64813882 0.97952902\n",
      " 0.97567356 0.97991747 0.96886122 0.60945439 0.60681981 0.45817736\n",
      " 0.5467338  0.30375397 0.94374639 0.51135808 0.98725033 0.85268354\n",
      " 0.79054284 0.21457362 0.28851691 0.97498637 0.99648041 0.9901247\n",
      " 0.9471091  0.82842743 0.92546755 0.56821746 0.28290591 0.5890494\n",
      " 0.99409866 0.77253503 0.94969881 0.62511665 0.25290963 0.82026899\n",
      " 0.96643853 0.73258644 0.97159678 0.98889357 0.71802187 0.72710371\n",
      " 0.95179749 0.95076054 0.82572633 0.82035249 0.84236622 0.3822028\n",
      " 0.88378882 0.97821063 0.91431952 0.9756909  0.98001778 0.98861247\n",
      " 0.97748953 0.94605881 0.93868434 0.96845531 0.67395657 0.77208489\n",
      " 0.96097279 0.97323859 0.49633873 0.96672708 0.99161208 0.80083239\n",
      " 0.78684258 0.35848415 0.88143981 0.72502917 0.8497591  0.87358046\n",
      " 0.74489152 0.7224074  0.95500934 0.77408135 0.84879553 0.98910016\n",
      " 0.97841346 0.94457573 0.612912   0.96179515 0.92285115 0.88657492\n",
      " 0.7078858  0.77756011 0.47866499 0.59720826 0.60848755 0.54785085\n",
      " 0.98408246 0.97576988 0.86666256 0.98468822]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 3 [0/43 (0%)]\tTrain Loss: 0.093499\n",
      "Train Epoch: 3 [10/43 (23%)]\tTrain Loss: 0.113405\n",
      "Train Epoch: 3 [20/43 (47%)]\tTrain Loss: 0.210904\n",
      "Train Epoch: 3 [30/43 (70%)]\tTrain Loss: 0.224858\n",
      "Train Epoch: 3 [40/43 (93%)]\tTrain Loss: 0.126972\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.68511909 0.84964675 0.82684135 0.76197261 0.40025693 0.60144711\n",
      " 0.60261291 0.47128117 0.64052719 0.38426861 0.50157976 0.54361153\n",
      " 0.41380551 0.79466647 0.74825543 0.26127192 0.63286465 0.67411166\n",
      " 0.66415358 0.83159941 0.59281725 0.47376168 0.72585338 0.76943189\n",
      " 0.61223108 0.65386802 0.67200732 0.85340947 0.54850084 0.68120104\n",
      " 0.71010971 0.47735059 0.92446029 0.73590744 0.67829168 0.63816696\n",
      " 0.44342521 0.63770908 0.64424646 0.50743043 0.65891224 0.69535661\n",
      " 0.73640883 0.63395089 0.40811741 0.8571493  0.97476846 0.8972975\n",
      " 0.91291165 0.38654792 0.61374742 0.62079793 0.54863244 0.27814692\n",
      " 0.78028059 0.73262048 0.45418534 0.69480449 0.644104   0.26336443\n",
      " 0.80697048 0.59155643 0.82598859 0.72739255 0.5894444  0.50155377\n",
      " 0.54189378 0.88648272 0.719428   0.40197691 0.85786581 0.79249221\n",
      " 0.63655913 0.66372281 0.75479561 0.71871179 0.828861   0.88941485\n",
      " 0.5385927  0.6557976  0.86685723 0.73380846 0.42319319 0.76275742\n",
      " 0.67183357 0.81498688 0.77839965 0.8208136  0.66276902 0.61497116\n",
      " 0.73424256 0.38125071 0.57013267 0.53171837 0.70719254 0.7069639\n",
      " 0.49841249 0.62267298 0.56408894 0.32626781 0.55693012 0.91689259\n",
      " 0.49345455 0.81320876 0.69026661 0.4270066  0.82134807 0.4500629\n",
      " 0.55531484 0.6774472  0.32046774 0.43811888 0.46168253 0.5759508\n",
      " 0.65122777 0.47386992 0.56877029 0.34697595]\n",
      "predict [1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
      "Train Epoch: 4 [0/43 (0%)]\tTrain Loss: 0.075258\n",
      "Train Epoch: 4 [10/43 (23%)]\tTrain Loss: 0.111536\n",
      "Train Epoch: 4 [20/43 (47%)]\tTrain Loss: 0.067809\n",
      "Train Epoch: 4 [30/43 (70%)]\tTrain Loss: 0.289551\n",
      "Train Epoch: 4 [40/43 (93%)]\tTrain Loss: 0.230421\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.51535088 0.99722087 0.99998605 0.16887262 0.18537585 0.36952198\n",
      " 0.99102026 0.19230631 0.73312861 0.55845273 0.45999199 0.17019302\n",
      " 0.36598027 0.90192723 0.84453338 0.96713257 0.36106011 0.99733704\n",
      " 0.99984217 0.99991143 0.99951041 0.99974722 0.9999038  0.72202641\n",
      " 0.99988866 0.98841548 0.98636127 0.95553172 0.9886691  0.9967134\n",
      " 0.69760966 0.98449337 0.9998703  0.32706195 0.44040427 0.30487427\n",
      " 0.11502878 0.36004096 0.97199148 0.99234152 0.98297471 0.95803589\n",
      " 0.34480956 0.52909064 0.52945846 0.67421806 0.86887938 0.99984729\n",
      " 0.3590923  0.28915876 0.95801276 0.36739218 0.29305103 0.30187026\n",
      " 0.99993515 0.0735334  0.99836987 0.47913259 0.37551424 0.20938927\n",
      " 0.99804521 0.99975032 0.99979943 0.99966502 0.29412273 0.44471115\n",
      " 0.4929716  0.99984825 0.94794577 0.45987937 0.93572605 0.54649299\n",
      " 0.51768935 0.65954852 0.58401555 0.9998579  0.99998868 0.99986959\n",
      " 0.48851165 0.94459957 0.9257195  0.9627198  0.82923406 0.99975342\n",
      " 0.99975342 0.98181713 0.99672073 0.99811411 0.98601568 0.99369311\n",
      " 0.99657768 0.72026426 0.9865008  0.91034853 0.56585938 0.95731091\n",
      " 0.32481116 0.98892295 0.99815398 0.97212875 0.99580932 0.93819153\n",
      " 0.99721491 0.39580935 0.25082105 0.96893644 0.64104593 0.78886133\n",
      " 0.9717012  0.9995802  0.95269769 0.91104037 0.94869202 0.96691453\n",
      " 0.99058795 0.99431348 0.45819542 0.33914804]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/43 (0%)]\tTrain Loss: 0.182787\n",
      "Train Epoch: 5 [10/43 (23%)]\tTrain Loss: 0.172733\n",
      "Train Epoch: 5 [20/43 (47%)]\tTrain Loss: 0.179224\n",
      "Train Epoch: 5 [30/43 (70%)]\tTrain Loss: 0.153313\n",
      "Train Epoch: 5 [40/43 (93%)]\tTrain Loss: 0.179481\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.0643453  0.99996912 0.99887496 0.50239152 0.51524121 0.43086267\n",
      " 0.99988663 0.70567721 0.37856418 0.49517936 0.19050559 0.55989444\n",
      " 0.46091363 0.61899197 0.72043544 0.62625057 0.28600565 0.97602582\n",
      " 0.97081125 0.64115888 0.76411206 0.83425897 0.99364775 0.68804002\n",
      " 0.64208752 0.97572416 0.97659588 0.98360115 0.89941537 0.98278749\n",
      " 0.65795296 0.6930424  0.99989212 0.52989483 0.46378466 0.09177108\n",
      " 0.58568537 0.54519635 0.46233267 0.93904591 0.92920393 0.95954388\n",
      " 0.60032433 0.47600135 0.44085544 0.64965481 0.99944967 0.99993348\n",
      " 0.86022288 0.126192   0.91702837 0.39764541 0.45100603 0.53875691\n",
      " 0.94689208 0.63400012 0.59623432 0.72667611 0.42939255 0.81655049\n",
      " 0.74444002 0.79310828 0.85517573 0.56525171 0.70706296 0.70295435\n",
      " 0.82631421 0.79214829 0.73963636 0.66937357 0.73896515 0.63721353\n",
      " 0.61572492 0.89434099 0.62099171 0.51033312 0.99997461 0.99996412\n",
      " 0.39405477 0.76076436 0.74164993 0.71365899 0.65316504 0.76820546\n",
      " 0.64110738 0.68275976 0.97492015 0.9858585  0.96378154 0.98838997\n",
      " 0.95063162 0.63517082 0.33448038 0.54306418 0.22364138 0.96312213\n",
      " 0.39983815 0.99542648 0.98330367 0.96615833 0.97364986 0.63062513\n",
      " 0.98687732 0.6739884  0.79965353 0.96780753 0.41501647 0.47410399\n",
      " 0.73818791 0.62731361 0.97382176 0.91740435 0.98510414 0.65893793\n",
      " 0.49536404 0.99736613 0.67644912 0.74702936]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      "Train Epoch: 6 [0/43 (0%)]\tTrain Loss: 0.094260\n",
      "Train Epoch: 6 [10/43 (23%)]\tTrain Loss: 0.079611\n",
      "Train Epoch: 6 [20/43 (47%)]\tTrain Loss: 0.099143\n",
      "Train Epoch: 6 [30/43 (70%)]\tTrain Loss: 0.071382\n",
      "Train Epoch: 6 [40/43 (93%)]\tTrain Loss: 0.077550\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.39345205 0.32030243 0.20382659 0.72870225 0.3521325  0.51814145\n",
      " 0.11016882 0.50425404 0.60407317 0.41110355 0.62269115 0.46801493\n",
      " 0.41942924 0.94359261 0.75056934 0.71072596 0.7507177  0.91672295\n",
      " 0.80235541 0.79873216 0.73455888 0.60732806 0.89578402 0.38173279\n",
      " 0.74275953 0.62078893 0.83580387 0.80686659 0.5845719  0.73446059\n",
      " 0.87165266 0.91793102 0.45361912 0.70216274 0.59323478 0.85052454\n",
      " 0.46649617 0.37554595 0.77861083 0.80367374 0.92968673 0.54777551\n",
      " 0.53610551 0.76636791 0.4443959  0.54233348 0.73100191 0.21785659\n",
      " 0.84679931 0.65926099 0.91941667 0.54535735 0.65155327 0.75546926\n",
      " 0.7984814  0.72661996 0.6496163  0.71354812 0.44564295 0.64494163\n",
      " 0.6342383  0.8890925  0.7750029  0.87139434 0.47430712 0.87453747\n",
      " 0.76616204 0.89301986 0.65315598 0.81767315 0.75945866 0.82419837\n",
      " 0.81416839 0.74471837 0.8784166  0.56138921 0.58338219 0.24484625\n",
      " 0.69934624 0.9194442  0.87912029 0.81530064 0.83002847 0.7472102\n",
      " 0.73253822 0.84336275 0.32668304 0.77543283 0.78294891 0.64825284\n",
      " 0.8476128  0.62575579 0.90524477 0.66986501 0.80925375 0.86125815\n",
      " 0.73631614 0.20352615 0.49545896 0.62355882 0.90179586 0.86793041\n",
      " 0.71751696 0.71221727 0.69815773 0.95645732 0.74723381 0.86015332\n",
      " 0.90882033 0.72249913 0.78367591 0.76771116 0.65164888 0.72618252\n",
      " 0.55160618 0.88520461 0.81761831 0.84195346]\n",
      "predict [0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 7 [0/43 (0%)]\tTrain Loss: 0.103864\n",
      "Train Epoch: 7 [10/43 (23%)]\tTrain Loss: 0.089025\n",
      "Train Epoch: 7 [20/43 (47%)]\tTrain Loss: 0.099549\n",
      "Train Epoch: 7 [30/43 (70%)]\tTrain Loss: 0.109538\n",
      "Train Epoch: 7 [40/43 (93%)]\tTrain Loss: 0.060891\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.68932956 0.99270004 0.98381108 0.67033005 0.41137975 0.17208223\n",
      " 0.99288756 0.50532585 0.40763384 0.27432755 0.13812698 0.41307446\n",
      " 0.34561124 0.57998794 0.73417646 0.66538614 0.62751752 0.30073875\n",
      " 0.17986296 0.5430848  0.73257554 0.70442402 0.05312014 0.5112592\n",
      " 0.73881674 0.30309305 0.11361758 0.31050146 0.23192541 0.23410884\n",
      " 0.6728459  0.60889435 0.98999679 0.4337377  0.28947917 0.74770963\n",
      " 0.31834292 0.27851304 0.35893238 0.11192725 0.61973757 0.15593672\n",
      " 0.80625319 0.68004721 0.3933062  0.59781617 0.99963212 0.99811184\n",
      " 0.70363241 0.47155112 0.93929714 0.29419783 0.55628181 0.40936515\n",
      " 0.21067028 0.43526119 0.71184492 0.5845173  0.3083235  0.53893316\n",
      " 0.78424269 0.65450841 0.83638674 0.68424785 0.60246664 0.84326774\n",
      " 0.4087742  0.27572832 0.52996975 0.57858086 0.58686787 0.56008613\n",
      " 0.64075565 0.67390496 0.86662477 0.73412329 0.99188519 0.98432827\n",
      " 0.73523843 0.82958782 0.79998755 0.6834501  0.59368867 0.69276321\n",
      " 0.43064311 0.68155628 0.19269288 0.50184602 0.22642858 0.3729977\n",
      " 0.12067053 0.30754182 0.6531598  0.35112125 0.65261352 0.07526325\n",
      " 0.36974943 0.38699061 0.28382912 0.0933126  0.31962797 0.69247526\n",
      " 0.49346152 0.67589951 0.64556044 0.44611871 0.6499998  0.42906773\n",
      " 0.22955532 0.54297441 0.064905   0.1658563  0.31140038 0.6120159\n",
      " 0.74180448 0.19237274 0.78213203 0.55483609]\n",
      "predict [1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 8 [0/43 (0%)]\tTrain Loss: 0.060081\n",
      "Train Epoch: 8 [10/43 (23%)]\tTrain Loss: 0.078016\n",
      "Train Epoch: 8 [20/43 (47%)]\tTrain Loss: 0.088162\n",
      "Train Epoch: 8 [30/43 (70%)]\tTrain Loss: 0.073955\n",
      "Train Epoch: 8 [40/43 (93%)]\tTrain Loss: 0.101010\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.21350163 0.54406303 0.5805636  0.47883818 0.26134476 0.24712183\n",
      " 0.36201036 0.32167873 0.11085346 0.07220489 0.13673453 0.18225756\n",
      " 0.20658858 0.52358186 0.45574176 0.2804516  0.36162919 0.45416066\n",
      " 0.21701348 0.58600271 0.46579504 0.39133546 0.1990587  0.49725112\n",
      " 0.35265273 0.35241646 0.17394683 0.19501542 0.32327658 0.26066896\n",
      " 0.51358938 0.44793174 0.43389976 0.41131523 0.3065027  0.33250538\n",
      " 0.39721081 0.23196504 0.30431706 0.36086509 0.32781708 0.57794231\n",
      " 0.27052528 0.2804127  0.17366344 0.35685262 0.4078953  0.80416232\n",
      " 0.4067454  0.29825529 0.47019768 0.15780498 0.24225645 0.356105\n",
      " 0.45172644 0.21120536 0.62717038 0.39639032 0.18230534 0.30205616\n",
      " 0.44459963 0.53528666 0.53129649 0.34290764 0.33880314 0.45252997\n",
      " 0.46790993 0.47486916 0.34522709 0.25874871 0.56499445 0.54955721\n",
      " 0.59646666 0.40981105 0.41751647 0.46691087 0.70820141 0.5953961\n",
      " 0.52674466 0.53453189 0.38701242 0.62984836 0.57027668 0.34010297\n",
      " 0.43267941 0.56073785 0.4352704  0.11564334 0.35469055 0.31686544\n",
      " 0.47388774 0.23779798 0.37996709 0.20967332 0.3900198  0.22994721\n",
      " 0.16720976 0.55274433 0.32758555 0.42492872 0.28724688 0.42224446\n",
      " 0.27984568 0.38585889 0.49578354 0.37414911 0.38960102 0.15758981\n",
      " 0.27149841 0.32218641 0.24441926 0.3137452  0.44469571 0.46978271\n",
      " 0.24154809 0.46871662 0.43159303 0.33584067]\n",
      "predict [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/43 (0%)]\tTrain Loss: 0.061982\n",
      "Train Epoch: 9 [10/43 (23%)]\tTrain Loss: 0.064619\n",
      "Train Epoch: 9 [20/43 (47%)]\tTrain Loss: 0.068924\n",
      "Train Epoch: 9 [30/43 (70%)]\tTrain Loss: 0.085751\n",
      "Train Epoch: 9 [40/43 (93%)]\tTrain Loss: 0.159008\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.40002111 0.98761719 0.66872543 0.35792956 0.47529998 0.41455328\n",
      " 0.98810601 0.65886784 0.36679903 0.22457378 0.41644275 0.21559051\n",
      " 0.22400801 0.44630489 0.32348222 0.29665518 0.42783803 0.69980365\n",
      " 0.83985955 0.52673304 0.55193973 0.29191327 0.6866852  0.37399238\n",
      " 0.28449973 0.39740038 0.5199815  0.65084606 0.3931998  0.50162429\n",
      " 0.64213747 0.63747025 0.55549937 0.32642218 0.39456341 0.43550861\n",
      " 0.52466756 0.37585294 0.3366988  0.73302621 0.23237173 0.53038877\n",
      " 0.33889934 0.35065845 0.40813136 0.43627051 0.55536002 0.36768481\n",
      " 0.60752034 0.19723016 0.39030144 0.41094902 0.4319405  0.38675559\n",
      " 0.45085168 0.55013436 0.31998271 0.52392298 0.32459435 0.61787719\n",
      " 0.51267785 0.4631933  0.47031447 0.65934241 0.5561626  0.58770925\n",
      " 0.2443406  0.4933742  0.44169158 0.4027656  0.46397683 0.42059466\n",
      " 0.49050584 0.47822139 0.68487382 0.42906219 0.65705794 0.59099978\n",
      " 0.63042527 0.28482121 0.43970749 0.4931142  0.63284528 0.47176236\n",
      " 0.55647457 0.50353646 0.69908494 0.58975607 0.42648494 0.79328793\n",
      " 0.7298252  0.25228772 0.48913956 0.47304738 0.47356245 0.38236797\n",
      " 0.3098155  0.86005199 0.76670569 0.70037824 0.72543794 0.5747714\n",
      " 0.69824058 0.2854197  0.55708623 0.71099818 0.22108473 0.3030622\n",
      " 0.42814383 0.41488117 0.67676216 0.69853574 0.81825244 0.56296593\n",
      " 0.35787967 0.59516752 0.62519228 0.61414456]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      "Train Epoch: 10 [0/43 (0%)]\tTrain Loss: 0.058140\n",
      "Train Epoch: 10 [10/43 (23%)]\tTrain Loss: 0.081199\n",
      "Train Epoch: 10 [20/43 (47%)]\tTrain Loss: 0.088650\n",
      "Train Epoch: 10 [30/43 (70%)]\tTrain Loss: 0.065528\n",
      "Train Epoch: 10 [40/43 (93%)]\tTrain Loss: 0.068170\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.25630665 0.99938202 0.91805959 0.46697703 0.43916997 0.28464839\n",
      " 0.99985754 0.65036678 0.41280726 0.3660816  0.48379445 0.46090659\n",
      " 0.35585213 0.72429651 0.44624984 0.59885514 0.53820074 0.95244491\n",
      " 0.93691003 0.69299501 0.45715299 0.6877839  0.91857725 0.66340077\n",
      " 0.60616958 0.9371978  0.76518571 0.7941106  0.40028685 0.76684374\n",
      " 0.55249691 0.50410092 0.66745633 0.52902615 0.46224815 0.47117618\n",
      " 0.43868601 0.32504809 0.42749918 0.77000749 0.49582255 0.58202833\n",
      " 0.49909773 0.35170099 0.59624225 0.55836076 0.53578144 0.61382848\n",
      " 0.8235628  0.32635868 0.51412338 0.23463869 0.41401756 0.56501627\n",
      " 0.56710374 0.36282545 0.60630882 0.47413722 0.55765051 0.51949084\n",
      " 0.71022302 0.6631971  0.601524   0.61045897 0.42706108 0.43482837\n",
      " 0.65161604 0.45588768 0.8494646  0.51750964 0.79098076 0.43997106\n",
      " 0.5165323  0.46604133 0.34570283 0.60783798 0.66475058 0.7240389\n",
      " 0.44382122 0.71596682 0.72270244 0.76471347 0.78284496 0.55328184\n",
      " 0.57732522 0.63663226 0.9365797  0.92492944 0.95039564 0.78960627\n",
      " 0.83483523 0.40320256 0.59360075 0.35803536 0.68897212 0.58515835\n",
      " 0.47745049 0.89109659 0.64704126 0.94168299 0.86961454 0.63699073\n",
      " 0.77372241 0.64452302 0.33038461 0.91296619 0.36071193 0.55420792\n",
      " 0.64056391 0.56809157 0.42385891 0.81374031 0.98263943 0.66260308\n",
      " 0.382379   0.97059309 0.62149519 0.69196373]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 25 FN= 11 FP= 35\n",
      "TP+FP 82\n",
      "precision 0.573170731707317\n",
      "recall 0.8103448275862069\n",
      "F1 0.6714285714285714\n",
      "acc 0.6101694915254238\n",
      "AUCp 0.6135057471264367\n",
      "AUC 0.6385057471264368\n",
      "\n",
      " The epoch is 10, average recall: 0.8103, average precision: 0.5732,average F1: 0.6714, average accuracy: 0.6102, average AUC: 0.6385\n",
      "Train Epoch: 11 [0/43 (0%)]\tTrain Loss: 0.101671\n",
      "Train Epoch: 11 [10/43 (23%)]\tTrain Loss: 0.075481\n",
      "Train Epoch: 11 [20/43 (47%)]\tTrain Loss: 0.073363\n",
      "Train Epoch: 11 [30/43 (70%)]\tTrain Loss: 0.059333\n",
      "Train Epoch: 11 [40/43 (93%)]\tTrain Loss: 0.084461\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.37263298 0.95029259 0.95494378 0.2555306  0.5726794  0.48257828\n",
      " 0.848557   0.49268994 0.37740952 0.38460848 0.40213725 0.23340149\n",
      " 0.42443475 0.851197   0.51613474 0.29353938 0.55549228 0.47316298\n",
      " 0.75062096 0.66235262 0.2246335  0.73221797 0.5185588  0.64910263\n",
      " 0.30865586 0.7484225  0.47442374 0.57302874 0.20583262 0.75949597\n",
      " 0.59553826 0.75313115 0.8122744  0.43583786 0.38861313 0.14286134\n",
      " 0.42476246 0.34135711 0.37553841 0.37395155 0.50836408 0.50750446\n",
      " 0.31354585 0.40980741 0.43575001 0.46465611 0.62571341 0.49550587\n",
      " 0.62441105 0.49316686 0.68507463 0.42910579 0.37695682 0.28729311\n",
      " 0.52641648 0.49759468 0.62011904 0.43083188 0.3486571  0.34011021\n",
      " 0.5007782  0.70966566 0.488451   0.61225045 0.62074476 0.66322112\n",
      " 0.47367686 0.67760736 0.58620352 0.46509224 0.36200625 0.20621727\n",
      " 0.64581931 0.4845902  0.65592176 0.69690281 0.96906137 0.94471103\n",
      " 0.70034122 0.72916716 0.68176317 0.72909456 0.53355265 0.44040063\n",
      " 0.57766068 0.50527036 0.58751184 0.79304111 0.77631891 0.71352679\n",
      " 0.50219983 0.40882918 0.67829746 0.39282957 0.61240548 0.47385857\n",
      " 0.64458406 0.8107034  0.7486552  0.48942894 0.77569395 0.76313657\n",
      " 0.7289449  0.53244233 0.5691964  0.76855886 0.75728965 0.47389612\n",
      " 0.64882594 0.40451613 0.4983668  0.52963638 0.79134256 0.57114416\n",
      " 0.73521179 0.52835584 0.77519011 0.7278707 ]\n",
      "predict [0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 12 [0/43 (0%)]\tTrain Loss: 0.073862\n",
      "Train Epoch: 12 [10/43 (23%)]\tTrain Loss: 0.081377\n",
      "Train Epoch: 12 [20/43 (47%)]\tTrain Loss: 0.126542\n",
      "Train Epoch: 12 [30/43 (70%)]\tTrain Loss: 0.090812\n",
      "Train Epoch: 12 [40/43 (93%)]\tTrain Loss: 0.057983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.65400976 0.92101681 0.66417992 0.74226087 0.55032045 0.26655972\n",
      " 0.96801043 0.6654495  0.22755946 0.67585677 0.53221053 0.53732961\n",
      " 0.69694597 0.96144307 0.89262199 0.57066631 0.81111258 0.54816502\n",
      " 0.6740061  0.85352635 0.67741442 0.71670407 0.53603613 0.63102311\n",
      " 0.81334794 0.695126   0.80200601 0.83137137 0.73565853 0.63397318\n",
      " 0.75472838 0.82492143 0.82016039 0.65670508 0.64125842 0.5009563\n",
      " 0.23619352 0.53090531 0.67435467 0.83021218 0.81025481 0.41927773\n",
      " 0.77064508 0.4535369  0.50856352 0.64068753 0.80694228 0.92010671\n",
      " 0.8680982  0.41906136 0.33727291 0.65483207 0.60004634 0.45028216\n",
      " 0.73584229 0.54702199 0.72922641 0.35392863 0.6038239  0.59832609\n",
      " 0.59691966 0.72999173 0.63689572 0.78940767 0.60924619 0.86430073\n",
      " 0.69447052 0.49242389 0.71667325 0.82920408 0.30225551 0.90735751\n",
      " 0.85347426 0.62903434 0.75674826 0.78142715 0.92324978 0.8505311\n",
      " 0.7760675  0.34703603 0.65122849 0.82218832 0.60680276 0.71107292\n",
      " 0.6335535  0.29306099 0.73729163 0.39765629 0.89211386 0.8569935\n",
      " 0.73901302 0.49609396 0.88332117 0.61711442 0.83640009 0.72132707\n",
      " 0.49988374 0.67102826 0.72765255 0.77823317 0.54953372 0.74139243\n",
      " 0.5306564  0.62934613 0.56971568 0.96178049 0.80055797 0.6553998\n",
      " 0.48808327 0.66135019 0.71440339 0.40973184 0.81984425 0.75945896\n",
      " 0.95731145 0.77293247 0.39665237 0.78649294]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.]\n",
      "Train Epoch: 13 [0/43 (0%)]\tTrain Loss: 0.061939\n",
      "Train Epoch: 13 [10/43 (23%)]\tTrain Loss: 0.069425\n",
      "Train Epoch: 13 [20/43 (47%)]\tTrain Loss: 0.079221\n",
      "Train Epoch: 13 [30/43 (70%)]\tTrain Loss: 0.070587\n",
      "Train Epoch: 13 [40/43 (93%)]\tTrain Loss: 0.093426\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.14902918 0.54461938 0.46874326 0.3061792  0.19063687 0.22152556\n",
      " 0.38617203 0.23792365 0.20839834 0.11412556 0.13472536 0.12920086\n",
      " 0.25622076 0.67658579 0.41364947 0.46783498 0.34832218 0.21246937\n",
      " 0.07154114 0.11337122 0.26933438 0.12224577 0.28049958 0.23687562\n",
      " 0.17606413 0.20371592 0.20080443 0.09782536 0.31761274 0.38190964\n",
      " 0.26876637 0.39532617 0.17976159 0.49562126 0.53844744 0.19973752\n",
      " 0.17806792 0.32452345 0.22449248 0.32872555 0.4565264  0.26170516\n",
      " 0.40368196 0.17297523 0.22048719 0.18473434 0.23722677 0.21255565\n",
      " 0.17191395 0.07398537 0.2690843  0.25262499 0.10555578 0.27599308\n",
      " 0.1839152  0.19949429 0.19020537 0.49160147 0.20491733 0.27677906\n",
      " 0.44419187 0.18835983 0.10368453 0.44828457 0.26521662 0.42559105\n",
      " 0.36629221 0.55959064 0.24141794 0.38865942 0.39918658 0.40703633\n",
      " 0.61547625 0.61659026 0.43544555 0.43047559 0.21726701 0.54290706\n",
      " 0.40936556 0.29389882 0.38911477 0.4342289  0.41676483 0.51168877\n",
      " 0.45090604 0.45219493 0.575611   0.68762821 0.36501709 0.65683335\n",
      " 0.5543952  0.21307716 0.4100101  0.39251465 0.44857225 0.12844114\n",
      " 0.23110975 0.73873216 0.6946649  0.18786468 0.81263077 0.31315887\n",
      " 0.53683513 0.45334268 0.35279563 0.52676737 0.21475899 0.21772924\n",
      " 0.11124623 0.48552343 0.40797538 0.5271225  0.16399977 0.5199914\n",
      " 0.59129268 0.81204599 0.30110395 0.36984706]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      "Train Epoch: 14 [0/43 (0%)]\tTrain Loss: 0.095952\n",
      "Train Epoch: 14 [10/43 (23%)]\tTrain Loss: 0.125959\n",
      "Train Epoch: 14 [20/43 (47%)]\tTrain Loss: 0.060654\n",
      "Train Epoch: 14 [30/43 (70%)]\tTrain Loss: 0.074465\n",
      "Train Epoch: 14 [40/43 (93%)]\tTrain Loss: 0.080630\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.38281751 0.60809165 0.70869106 0.33283815 0.29565296 0.33202547\n",
      " 0.62631905 0.41319057 0.43710646 0.4785319  0.40807498 0.13987598\n",
      " 0.30294275 0.61206716 0.30710146 0.17236452 0.62254375 0.42819285\n",
      " 0.42272297 0.62755108 0.47594839 0.40664616 0.53168887 0.61429811\n",
      " 0.47677791 0.35513213 0.3949365  0.52276343 0.66681027 0.35851106\n",
      " 0.6167056  0.44736934 0.47000524 0.62922275 0.27067259 0.49892908\n",
      " 0.24962035 0.3847065  0.22184239 0.51493376 0.38016507 0.6536783\n",
      " 0.43397659 0.26633728 0.13857318 0.31780282 0.43983838 0.41614467\n",
      " 0.48428375 0.30283767 0.55303258 0.41055626 0.31189337 0.28206933\n",
      " 0.387079   0.30247977 0.64428556 0.39099124 0.24463657 0.37496892\n",
      " 0.64294374 0.34531683 0.48675904 0.75529742 0.49480164 0.52755904\n",
      " 0.46567509 0.47828388 0.53243691 0.34619418 0.40919858 0.45912358\n",
      " 0.51990062 0.37704787 0.56011349 0.36665234 0.61108851 0.61129439\n",
      " 0.44992718 0.73533779 0.38819966 0.4465524  0.67342955 0.70045447\n",
      " 0.46280375 0.73224825 0.40227085 0.55258584 0.48455524 0.32349527\n",
      " 0.86541384 0.71786755 0.46602201 0.42475283 0.4051038  0.4126105\n",
      " 0.32919413 0.6119265  0.53604329 0.47354144 0.40652242 0.58641577\n",
      " 0.60191721 0.50538862 0.46465263 0.22501002 0.26680276 0.39303946\n",
      " 0.2442975  0.64108151 0.39950502 0.40765852 0.32704586 0.30251899\n",
      " 0.41981697 0.38078371 0.49430162 0.56732136]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 15 [0/43 (0%)]\tTrain Loss: 0.073961\n",
      "Train Epoch: 15 [10/43 (23%)]\tTrain Loss: 0.077173\n",
      "Train Epoch: 15 [20/43 (47%)]\tTrain Loss: 0.124753\n",
      "Train Epoch: 15 [30/43 (70%)]\tTrain Loss: 0.090738\n",
      "Train Epoch: 15 [40/43 (93%)]\tTrain Loss: 0.080857\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99996924 0.99994004 0.99997115 0.75128424 0.76644135 0.56375563\n",
      " 0.99994445 0.86007577 0.48882276 0.56380856 0.23554382 0.19083117\n",
      " 0.68040365 0.92636913 0.98488367 0.7764315  0.87448835 0.56081641\n",
      " 0.4436627  0.66308254 0.32386252 0.78980559 0.69494921 0.72357279\n",
      " 0.75837767 0.79323465 0.51124084 0.45629737 0.55860794 0.81416595\n",
      " 0.57745153 0.93871152 0.99990439 0.23359863 0.74706602 0.73524821\n",
      " 0.64650983 0.6334821  0.85140634 0.66290587 0.6497041  0.73853558\n",
      " 0.51495218 0.4791334  0.75021917 0.86734951 0.99991953 0.99989176\n",
      " 0.99996376 0.53633142 0.99994588 0.55644178 0.46635211 0.43003106\n",
      " 0.76959163 0.85746092 0.46720117 0.73096889 0.6836105  0.83144742\n",
      " 0.72112519 0.7612679  0.79296893 0.89444518 0.88633776 0.95553583\n",
      " 0.36462834 0.89799929 0.92976832 0.68736517 0.38696012 0.80205959\n",
      " 0.86877918 0.79331958 0.61145675 0.82879061 0.99984968 0.99974316\n",
      " 0.99990606 0.98956829 0.99996805 0.99998593 0.99992943 0.59820098\n",
      " 0.58973724 0.69381028 0.93574506 0.7294026  0.85668856 0.73242092\n",
      " 0.81604755 0.69830287 0.76167178 0.55752087 0.40892005 0.61007315\n",
      " 0.6110453  0.81080294 0.82017702 0.50118673 0.69327468 0.92263049\n",
      " 0.59228158 0.69854933 0.46491742 0.93917859 0.88977259 0.17111605\n",
      " 0.72510904 0.35261816 0.79945809 0.75495762 0.73552436 0.69034022\n",
      " 0.67067111 0.84264958 0.99994946 0.99988508]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [0/43 (0%)]\tTrain Loss: 0.090385\n",
      "Train Epoch: 16 [10/43 (23%)]\tTrain Loss: 0.086022\n",
      "Train Epoch: 16 [20/43 (47%)]\tTrain Loss: 0.066629\n",
      "Train Epoch: 16 [30/43 (70%)]\tTrain Loss: 0.070426\n",
      "Train Epoch: 16 [40/43 (93%)]\tTrain Loss: 0.066583\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.71882451 0.52958888 0.54195791 0.38285178 0.3735449  0.32795048\n",
      " 0.59658307 0.35533085 0.30474925 0.2291358  0.4222244  0.16373563\n",
      " 0.41876206 0.65533853 0.69602078 0.54256409 0.43079454 0.45251209\n",
      " 0.37672415 0.333507   0.33757675 0.43562779 0.44181606 0.47650898\n",
      " 0.29558161 0.34993544 0.40147659 0.45429042 0.56316024 0.3590554\n",
      " 0.43708637 0.55981117 0.702959   0.36859554 0.57184249 0.53616548\n",
      " 0.20563991 0.29182634 0.29574355 0.36460844 0.3440558  0.29226917\n",
      " 0.29403988 0.34003681 0.46239328 0.56580377 0.74188125 0.60239947\n",
      " 0.64164257 0.37001339 0.52271956 0.20445091 0.73323661 0.42421588\n",
      " 0.47134846 0.33306393 0.42827907 0.29774213 0.36262682 0.4180412\n",
      " 0.23422807 0.56676066 0.49571452 0.81036162 0.66195887 0.43351674\n",
      " 0.40946168 0.48214254 0.58873689 0.46767789 0.3914597  0.47803453\n",
      " 0.42688617 0.45498958 0.53250045 0.68238831 0.62269551 0.73358506\n",
      " 0.74587643 0.66834301 0.5658738  0.64618796 0.56674838 0.48435515\n",
      " 0.40114442 0.58291858 0.43852687 0.46112564 0.33242667 0.43510392\n",
      " 0.28370476 0.33266959 0.4243665  0.44978338 0.60745525 0.3458277\n",
      " 0.31612912 0.5934701  0.37269583 0.32921568 0.32417962 0.430323\n",
      " 0.50707549 0.47750998 0.36726481 0.33976066 0.37302941 0.27767113\n",
      " 0.45536643 0.35699025 0.42715731 0.29350558 0.54889703 0.36470458\n",
      " 0.40929219 0.22928707 0.73357117 0.67628783]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      "Train Epoch: 17 [0/43 (0%)]\tTrain Loss: 0.062324\n",
      "Train Epoch: 17 [10/43 (23%)]\tTrain Loss: 0.070221\n",
      "Train Epoch: 17 [20/43 (47%)]\tTrain Loss: 0.068561\n",
      "Train Epoch: 17 [30/43 (70%)]\tTrain Loss: 0.068260\n",
      "Train Epoch: 17 [40/43 (93%)]\tTrain Loss: 0.065573\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.24169792 0.1442287  0.13461713 0.54812115 0.37082475 0.37854102\n",
      " 0.16118355 0.51428157 0.52756017 0.15349486 0.2572552  0.27763408\n",
      " 0.29020011 0.74813253 0.45715755 0.43111929 0.47265482 0.53187066\n",
      " 0.27352011 0.36384344 0.47146651 0.30943108 0.67827028 0.40883452\n",
      " 0.27127454 0.48649639 0.55536687 0.27059239 0.59972328 0.58489269\n",
      " 0.53394473 0.62039739 0.30471024 0.23111641 0.56043863 0.48323575\n",
      " 0.32589024 0.44695547 0.67301106 0.4218609  0.3242265  0.47058532\n",
      " 0.37670109 0.32318422 0.41981021 0.5526619  0.11810166 0.21253346\n",
      " 0.20992605 0.58312321 0.16585076 0.25383532 0.4569512  0.23169413\n",
      " 0.65668344 0.48931494 0.50700271 0.60299981 0.19107716 0.45380235\n",
      " 0.64513713 0.47729504 0.85934597 0.57695246 0.57386065 0.62030673\n",
      " 0.62537897 0.27688533 0.56975132 0.4976981  0.51615161 0.48934585\n",
      " 0.5395447  0.43551043 0.59863138 0.47046876 0.20070575 0.20030402\n",
      " 0.21165864 0.54680681 0.84475803 0.39811572 0.58157784 0.55835563\n",
      " 0.45594075 0.53226537 0.39781696 0.29592803 0.3988691  0.41279346\n",
      " 0.2792111  0.28361738 0.42577177 0.43485609 0.43892044 0.45176965\n",
      " 0.1084448  0.26721007 0.48007905 0.41183794 0.66751868 0.63057148\n",
      " 0.63562173 0.50402284 0.75158012 0.19304247 0.48442897 0.13210762\n",
      " 0.47453773 0.58450586 0.37159255 0.35869226 0.51458377 0.26050431\n",
      " 0.23092501 0.49999025 0.24679676 0.1816989 ]\n",
      "predict [0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 18 [0/43 (0%)]\tTrain Loss: 0.094590\n",
      "Train Epoch: 18 [10/43 (23%)]\tTrain Loss: 0.113945\n",
      "Train Epoch: 18 [20/43 (47%)]\tTrain Loss: 0.073015\n",
      "Train Epoch: 18 [30/43 (70%)]\tTrain Loss: 0.109994\n",
      "Train Epoch: 18 [40/43 (93%)]\tTrain Loss: 0.117614\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [1.         1.         1.         0.85514444 0.51404089 0.59614414\n",
      " 0.99999988 0.99990058 0.39707741 0.43833566 0.76585019 0.12602532\n",
      " 0.62635541 0.98137856 0.99994171 0.87038964 0.28194499 0.47820571\n",
      " 0.21567643 0.67376679 0.64934784 0.42587435 0.56563312 0.552459\n",
      " 0.63003051 0.83588362 0.66947353 0.36103421 0.44578177 0.27884293\n",
      " 0.9015677  0.63239342 0.88256532 0.81369478 0.73427325 0.31673634\n",
      " 0.66563588 0.45699751 0.59600574 0.36025989 0.1627928  0.35147822\n",
      " 0.73133367 0.42644262 0.53248304 0.86416495 1.         1.\n",
      " 1.         0.620579   1.         0.85904455 0.42404285 0.77728093\n",
      " 0.86586845 0.48029968 0.88365978 0.54875529 0.65760452 1.\n",
      " 0.91419584 0.91972291 0.88666266 0.72454643 0.74159265 0.58626688\n",
      " 0.69756734 0.76025563 0.86949521 0.32040626 0.91826725 0.96633321\n",
      " 0.128133   0.9274556  0.60841525 0.68061125 1.         1.\n",
      " 1.         0.99539387 0.97770739 0.99997211 0.99999738 0.79110354\n",
      " 0.89982337 0.81140637 0.78199929 0.91232842 0.99762374 0.53490597\n",
      " 0.94995975 0.64476043 0.62138706 0.51307017 0.80083722 0.40164918\n",
      " 0.65357256 0.67060989 0.94164896 0.80709875 0.81433499 0.84199309\n",
      " 0.9301244  0.92148453 0.87194806 0.99700469 0.84124184 0.84453303\n",
      " 0.66292393 0.68146187 0.8267529  0.4919948  0.70054507 0.71733105\n",
      " 0.66193897 0.97291213 1.         1.        ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 19 [0/43 (0%)]\tTrain Loss: 0.058055\n",
      "Train Epoch: 19 [10/43 (23%)]\tTrain Loss: 0.083459\n",
      "Train Epoch: 19 [20/43 (47%)]\tTrain Loss: 0.113352\n",
      "Train Epoch: 19 [30/43 (70%)]\tTrain Loss: 0.106400\n",
      "Train Epoch: 19 [40/43 (93%)]\tTrain Loss: 0.087570\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.82262903 0.67412144 0.78878391 0.41861743 0.3689459  0.36189991\n",
      " 0.87344575 0.43769857 0.40775877 0.23596793 0.24865732 0.13659917\n",
      " 0.31309718 0.77304739 0.75497288 0.32127476 0.44797912 0.34244984\n",
      " 0.51049596 0.33979964 0.51258624 0.55779308 0.46015024 0.28241986\n",
      " 0.51028943 0.44614249 0.26039129 0.52883911 0.34033659 0.59639513\n",
      " 0.52384317 0.71507281 0.87224239 0.45099354 0.43675643 0.36109787\n",
      " 0.49185947 0.43674073 0.39130446 0.47578794 0.40379918 0.47218883\n",
      " 0.40406248 0.31990877 0.39510459 0.65756649 0.83219016 0.75719184\n",
      " 0.73920441 0.33690381 0.76025957 0.32502559 0.5617854  0.448172\n",
      " 0.50462729 0.43027011 0.5358029  0.32726994 0.37063286 0.26594952\n",
      " 0.68989778 0.9508791  0.6645472  0.66636789 0.55971718 0.69422179\n",
      " 0.39756179 0.82612336 0.64111662 0.49623698 0.61728871 0.65705657\n",
      " 0.52112252 0.49610177 0.64957613 0.78655249 0.89537394 0.89865142\n",
      " 0.76420695 0.55330753 0.74259675 0.79823375 0.8146708  0.43597046\n",
      " 0.54887724 0.44727445 0.87633115 0.89941394 0.67725289 0.51435637\n",
      " 0.76431906 0.52375269 0.51953173 0.29706928 0.63999188 0.43328086\n",
      " 0.22188717 0.4031657  0.54846299 0.61092746 0.5045206  0.67159903\n",
      " 0.69304681 0.43783775 0.45695165 0.78953278 0.3702873  0.30064955\n",
      " 0.48236609 0.5851444  0.51198608 0.46582559 0.60351497 0.4405717\n",
      " 0.81834465 0.88300496 0.88308185 0.81552058]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [0/43 (0%)]\tTrain Loss: 0.058369\n",
      "Train Epoch: 20 [10/43 (23%)]\tTrain Loss: 0.074842\n",
      "Train Epoch: 20 [20/43 (47%)]\tTrain Loss: 0.071684\n",
      "Train Epoch: 20 [30/43 (70%)]\tTrain Loss: 0.103722\n",
      "Train Epoch: 20 [40/43 (93%)]\tTrain Loss: 0.080511\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.90616542 0.99796474 0.99835157 0.10145114 0.4004519  0.40354684\n",
      " 0.9970811  0.23623382 0.32110003 0.15818179 0.27838114 0.11725904\n",
      " 0.38859278 0.45454824 0.59925586 0.34041128 0.33453518 0.37910792\n",
      " 0.44902927 0.37814665 0.40826932 0.58398414 0.41399249 0.32284576\n",
      " 0.30879205 0.24987137 0.44478747 0.41195181 0.42946887 0.41181195\n",
      " 0.26869062 0.58545864 0.99903595 0.22564535 0.15414353 0.2966615\n",
      " 0.29858124 0.2953366  0.4027929  0.43930477 0.39624792 0.35758078\n",
      " 0.09552072 0.23917373 0.13869675 0.50119001 0.99762636 0.99720675\n",
      " 0.99780148 0.43774176 0.99823701 0.1650766  0.34983972 0.32313496\n",
      " 0.30183712 0.2785376  0.27856022 0.13084178 0.23127684 0.39533755\n",
      " 0.98565918 0.99980956 0.54828006 0.70501059 0.42829898 0.73241496\n",
      " 0.5145694  0.6423282  0.65158129 0.56931353 0.63758695 0.44590577\n",
      " 0.35701281 0.4202913  0.42795745 0.9973349  0.99817252 0.99552274\n",
      " 0.25007924 0.50910944 0.60581136 0.48253557 0.56109512 0.27558407\n",
      " 0.2869471  0.71173984 0.62666649 0.79966253 0.69879097 0.30929068\n",
      " 0.75550735 0.3026821  0.40709952 0.51920658 0.37890652 0.52930391\n",
      " 0.11876233 0.8971405  0.65417933 0.54033905 0.39163056 0.40864545\n",
      " 0.73325074 0.2284129  0.25944504 0.82821417 0.28670812 0.27008274\n",
      " 0.363444   0.32997227 0.26229361 0.37176597 0.6884222  0.583924\n",
      " 0.25657609 0.6047827  0.24140699 0.82981688]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1.]\n",
      "vote_pred [1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 37 TN= 42 FN= 21 FP= 18\n",
      "TP+FP 55\n",
      "precision 0.6727272727272727\n",
      "recall 0.6379310344827587\n",
      "F1 0.6548672566371682\n",
      "acc 0.6694915254237288\n",
      "AUCp 0.6689655172413793\n",
      "AUC 0.7522988505747127\n",
      "\n",
      " The epoch is 20, average recall: 0.6379, average precision: 0.6727,average F1: 0.6549, average accuracy: 0.6695, average AUC: 0.7523\n",
      "Train Epoch: 21 [0/43 (0%)]\tTrain Loss: 0.086647\n",
      "Train Epoch: 21 [10/43 (23%)]\tTrain Loss: 0.063140\n",
      "Train Epoch: 21 [20/43 (47%)]\tTrain Loss: 0.128476\n",
      "Train Epoch: 21 [30/43 (70%)]\tTrain Loss: 0.069466\n",
      "Train Epoch: 21 [40/43 (93%)]\tTrain Loss: 0.083786\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [4.84397680e-01 7.94916689e-01 7.28472650e-01 5.77924907e-01\n",
      " 4.94452298e-01 5.48813164e-01 5.57384670e-01 4.07076776e-01\n",
      " 5.69392264e-01 1.87851965e-01 3.36269677e-01 1.80854887e-01\n",
      " 3.99071842e-01 7.76672900e-01 5.01104236e-01 6.95672214e-01\n",
      " 7.72020161e-01 4.48310047e-01 1.66781712e-03 5.71626246e-01\n",
      " 5.79289734e-01 5.79332829e-01 5.86833179e-01 3.92761439e-01\n",
      " 4.14968997e-01 7.95819283e-01 4.07651037e-01 6.51436329e-01\n",
      " 4.68751162e-01 4.90739346e-01 5.71268022e-01 7.97162175e-01\n",
      " 1.35879358e-03 2.93033302e-01 6.95273936e-01 5.13576210e-01\n",
      " 3.97515684e-01 2.77949691e-01 8.97582948e-01 6.60328150e-01\n",
      " 5.77885270e-01 5.98081112e-01 2.93043196e-01 6.62941873e-01\n",
      " 5.84071219e-01 5.68943202e-01 7.80703008e-01 4.44999695e-01\n",
      " 3.83402467e-01 4.03429121e-01 6.37246490e-01 4.27468568e-01\n",
      " 5.46471596e-01 3.40455532e-01 3.76806557e-01 4.91473943e-01\n",
      " 5.48685193e-01 3.38521898e-01 1.83702588e-01 4.76015031e-01\n",
      " 8.95725680e-04 7.58508861e-04 5.14857829e-01 7.46926904e-01\n",
      " 5.79946220e-01 7.40898728e-01 9.05611515e-01 7.41408050e-01\n",
      " 5.93822002e-01 3.94563228e-01 7.23992884e-01 1.02611411e-04\n",
      " 7.68106818e-01 8.32070529e-01 7.09639609e-01 7.63364553e-01\n",
      " 3.79498839e-01 4.53378528e-01 7.95351446e-01 4.26282316e-01\n",
      " 7.10187018e-01 8.69231045e-01 5.56328237e-01 2.68799951e-04\n",
      " 9.30431765e-04 8.01353991e-01 3.70127439e-01 7.57936001e-01\n",
      " 4.42915499e-01 5.44613183e-01 7.56890893e-01 3.23529094e-01\n",
      " 6.95051432e-01 8.50026906e-01 4.69387472e-01 8.30958724e-01\n",
      " 4.79602337e-01 8.00691068e-01 3.75447005e-01 4.66756612e-01\n",
      " 4.33837593e-01 9.38983083e-01 8.22101057e-01 4.81986672e-01\n",
      " 8.16534460e-01 7.04888225e-01 3.86123121e-01 8.57688725e-01\n",
      " 4.95707661e-01 5.36826381e-04 4.45520341e-01 7.24322617e-01\n",
      " 5.81645370e-01 4.74591345e-01 9.36917543e-01 6.14742041e-01\n",
      " 3.96136701e-01 4.20500755e-01]\n",
      "predict [0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      "Train Epoch: 22 [0/43 (0%)]\tTrain Loss: 0.113707\n",
      "Train Epoch: 22 [10/43 (23%)]\tTrain Loss: 0.112484\n",
      "Train Epoch: 22 [20/43 (47%)]\tTrain Loss: 0.096439\n",
      "Train Epoch: 22 [30/43 (70%)]\tTrain Loss: 0.076017\n",
      "Train Epoch: 22 [40/43 (93%)]\tTrain Loss: 0.093813\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.36336204 0.82617605 0.42491382 0.11168506 0.37296364 0.21919031\n",
      " 0.6387611  0.29495585 0.41270456 0.39218011 0.25503638 0.22772832\n",
      " 0.40272835 0.8975789  0.42440391 0.26884654 0.58428794 0.68208849\n",
      " 0.46770471 0.59265739 0.60950899 0.3451868  0.58213222 0.39107797\n",
      " 0.33733374 0.35809022 0.20402856 0.61558694 0.50810975 0.35702023\n",
      " 0.46981832 0.25226283 0.73366958 0.05256408 0.10432179 0.10261912\n",
      " 0.07026736 0.06037546 0.28397137 0.55433923 0.53194314 0.44086373\n",
      " 0.16082917 0.33919033 0.47486651 0.08441148 0.31386888 0.10894115\n",
      " 0.12794751 0.2482212  0.16985153 0.14994724 0.3076461  0.13512455\n",
      " 0.05212287 0.211418   0.64375162 0.07176307 0.39037132 0.31111529\n",
      " 0.78070188 0.55562866 0.83092141 0.66450775 0.81144184 0.69117808\n",
      " 0.91945571 0.37781855 0.81687433 0.35340521 0.30137995 0.57888091\n",
      " 0.42884541 0.50631881 0.32707059 0.55200374 0.64751363 0.69126469\n",
      " 0.79520762 0.62357771 0.70113188 0.89735544 0.39320689 0.58639687\n",
      " 0.58624589 0.65806103 0.43894348 0.60502946 0.56613302 0.49973357\n",
      " 0.66030598 0.10687093 0.33587095 0.0714502  0.13838758 0.19649512\n",
      " 0.1009931  0.62700373 0.27728438 0.74343365 0.44826162 0.66827565\n",
      " 0.43472141 0.60151857 0.67982501 0.51834762 0.26746669 0.38053057\n",
      " 0.44024041 0.37623677 0.24442869 0.15078032 0.22468829 0.83788949\n",
      " 0.55154067 0.4842152  0.60023886 0.15713264]\n",
      "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23 [0/43 (0%)]\tTrain Loss: 0.072154\n",
      "Train Epoch: 23 [10/43 (23%)]\tTrain Loss: 0.062719\n",
      "Train Epoch: 23 [20/43 (47%)]\tTrain Loss: 0.085492\n",
      "Train Epoch: 23 [30/43 (70%)]\tTrain Loss: 0.066601\n",
      "Train Epoch: 23 [40/43 (93%)]\tTrain Loss: 0.084182\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.15132262 0.78000093 0.71792257 0.50317526 0.0454572  0.64849788\n",
      " 0.72744405 0.90154797 0.48638982 0.11584008 0.20681332 0.16012378\n",
      " 0.43045026 0.59483707 0.65975153 0.43029946 0.44442275 0.51971167\n",
      " 0.41207743 0.43606049 0.55484176 0.33875248 0.56797528 0.53284162\n",
      " 0.51510733 0.60666519 0.59052056 0.57188946 0.54213184 0.51153588\n",
      " 0.62601906 0.508798   0.8000651  0.59014982 0.59438717 0.33811414\n",
      " 0.48402667 0.29122877 0.38244939 0.56250697 0.41885006 0.52783674\n",
      " 0.48678407 0.49531952 0.64576775 0.49175191 0.589158   0.61068684\n",
      " 0.69564295 0.56778377 0.60875338 0.14136088 0.51099342 0.50504208\n",
      " 0.53815955 0.47550583 0.5341841  0.6647346  0.0724052  0.70882255\n",
      " 0.85027635 0.63509512 0.80208051 0.84622359 0.61863279 0.80175596\n",
      " 0.70502836 0.6052528  0.81701767 0.41231051 0.73936647 0.56920695\n",
      " 0.72130728 0.83408076 0.72817308 0.51911622 0.71916413 0.7246868\n",
      " 0.88926536 0.62985915 0.53974926 0.82465738 0.84502971 0.55426723\n",
      " 0.71350735 0.70490462 0.64933205 0.64938909 0.78617954 0.60121107\n",
      " 0.33525747 0.66327864 0.53673089 0.79226726 0.43968377 0.5102585\n",
      " 0.47410181 0.61951077 0.49641269 0.77546245 0.583902   0.76716632\n",
      " 0.72539902 0.58591235 0.33295    0.60627794 0.44741222 0.50900185\n",
      " 0.82645446 0.5000577  0.50888342 0.52465981 0.8634122  0.53202003\n",
      " 0.7520678  0.64105386 0.73703462 0.80818421]\n",
      "predict [0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 24 [0/43 (0%)]\tTrain Loss: 0.094678\n",
      "Train Epoch: 24 [10/43 (23%)]\tTrain Loss: 0.078419\n",
      "Train Epoch: 24 [20/43 (47%)]\tTrain Loss: 0.058579\n",
      "Train Epoch: 24 [30/43 (70%)]\tTrain Loss: 0.063500\n",
      "Train Epoch: 24 [40/43 (93%)]\tTrain Loss: 0.077176\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.1494514  0.86746877 0.88828123 0.10246894 0.19102427 0.1535328\n",
      " 0.83852768 0.30916381 0.2309349  0.27931198 0.24184498 0.23381853\n",
      " 0.23485717 0.9512893  0.90017843 0.20335844 0.44364876 0.27393198\n",
      " 0.468476   0.6306569  0.7537306  0.4914864  0.59161252 0.61324805\n",
      " 0.61771345 0.76532996 0.76333022 0.37962618 0.56856883 0.33107105\n",
      " 0.81598604 0.84740162 0.61481863 0.25768077 0.25872886 0.31193995\n",
      " 0.32430956 0.09922734 0.47210068 0.38746756 0.65447515 0.7892319\n",
      " 0.27907661 0.09211918 0.21293707 0.40408102 0.18959558 0.13017534\n",
      " 0.11341205 0.23105468 0.07241347 0.18511946 0.26394391 0.27285564\n",
      " 0.21258239 0.4321343  0.83302569 0.29599553 0.29721323 0.09759704\n",
      " 0.43174124 0.93120402 0.6477955  0.63698483 0.93428797 0.73977947\n",
      " 0.93495983 0.9012835  0.13999265 0.19872773 0.78194398 0.84974504\n",
      " 0.09766775 0.14569871 0.66277432 0.85751414 0.93790728 0.92495865\n",
      " 0.9328388  0.8585515  0.74925381 0.89727396 0.82146382 0.60035574\n",
      " 0.77377123 0.79143411 0.87833804 0.94895571 0.71393359 0.63174003\n",
      " 0.66858929 0.29126886 0.2911613  0.11938458 0.19343522 0.20802799\n",
      " 0.16805714 0.6454677  0.65997797 0.45169067 0.63786054 0.83670962\n",
      " 0.6646654  0.63317722 0.13392162 0.74318647 0.91196895 0.09420217\n",
      " 0.80699074 0.5544098  0.13695757 0.09861644 0.09833347 0.12292898\n",
      " 0.8261649  0.90330017 0.58628082 0.96921366]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 25 [0/43 (0%)]\tTrain Loss: 0.063627\n",
      "Train Epoch: 25 [10/43 (23%)]\tTrain Loss: 0.082272\n",
      "Train Epoch: 25 [20/43 (47%)]\tTrain Loss: 0.097621\n",
      "Train Epoch: 25 [30/43 (70%)]\tTrain Loss: 0.058401\n",
      "Train Epoch: 25 [40/43 (93%)]\tTrain Loss: 0.078197\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.99780053 0.89170182 0.9188301  0.99953556 0.99922812 0.99727613\n",
      " 0.92325842 0.99874121 0.99848276 0.99228525 0.99826801 0.98923033\n",
      " 0.99744165 0.95085335 0.93134689 0.65461355 0.32094455 0.63063812\n",
      " 0.43143651 0.63049126 0.54930347 0.15788977 0.49486607 0.60812539\n",
      " 0.63120174 0.53706324 0.54515463 0.75594985 0.6482836  0.79460675\n",
      " 0.70400405 0.84993237 0.7452355  0.99896312 0.99852633 0.65852672\n",
      " 0.71256924 0.75855345 0.29616678 0.47635886 0.46383375 0.48323381\n",
      " 0.99092066 0.59997982 0.99946123 0.99904531 0.99872106 0.79690307\n",
      " 0.78583473 0.99875176 0.99470335 0.99551481 0.99741459 0.99925536\n",
      " 0.55505264 0.99945515 0.48222166 0.99898654 0.99849367 0.99639893\n",
      " 0.74361843 0.98052329 0.7896027  0.57440543 0.89677238 0.91624242\n",
      " 0.81717867 0.95421696 0.68816745 0.57595962 0.51188302 0.85777098\n",
      " 0.92427266 0.85251474 0.83906627 0.64817548 0.91354334 0.85742044\n",
      " 0.84436452 0.82155323 0.66849256 0.93729472 0.74642247 0.9147259\n",
      " 0.57460421 0.70403004 0.9557367  0.9577539  0.90596479 0.47623149\n",
      " 0.70431423 0.99928099 0.56370544 0.59622264 0.5549491  0.72150612\n",
      " 0.99638546 0.85625434 0.77755153 0.66893625 0.34547621 0.51016718\n",
      " 0.92519635 0.47526756 0.67954659 0.97270679 0.53943449 0.47509721\n",
      " 0.42250782 0.4884527  0.69916075 0.40151817 0.92170149 0.53659928\n",
      " 0.86899793 0.53888303 0.60187298 0.96560436]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 26 [0/43 (0%)]\tTrain Loss: 0.145740\n",
      "Train Epoch: 26 [10/43 (23%)]\tTrain Loss: 0.153034\n",
      "Train Epoch: 26 [20/43 (47%)]\tTrain Loss: 0.090032\n",
      "Train Epoch: 26 [30/43 (70%)]\tTrain Loss: 0.090678\n",
      "Train Epoch: 26 [40/43 (93%)]\tTrain Loss: 0.091597\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.26247439 0.75906086 0.73663729 0.12897743 0.20094366 0.12861328\n",
      " 0.74209869 0.60867906 0.24741621 0.16402882 0.23656669 0.13153078\n",
      " 0.44748631 0.75685626 0.70118868 0.13738242 0.25057927 0.49583036\n",
      " 0.52984619 0.79608083 0.59554696 0.54566646 0.65639621 0.58374977\n",
      " 0.59949684 0.53493768 0.63014221 0.58108634 0.61181921 0.58642679\n",
      " 0.69163901 0.67651963 0.70858365 0.25813627 0.21903867 0.16007984\n",
      " 0.25422874 0.39850411 0.41028687 0.51326692 0.53554922 0.73810232\n",
      " 0.11753321 0.09131304 0.13608199 0.17540975 0.13145046 0.20545207\n",
      " 0.60011208 0.46784624 0.88998377 0.18738465 0.23771502 0.17618862\n",
      " 0.7241562  0.24274653 0.59023535 0.12583834 0.16029492 0.25054446\n",
      " 0.62805682 0.62764668 0.73663133 0.72078669 0.58408266 0.85664141\n",
      " 0.62724483 0.66298872 0.7631433  0.79386348 0.60902923 0.74729836\n",
      " 0.2235581  0.19071069 0.67725426 0.79296112 0.82755375 0.90431172\n",
      " 0.70130551 0.55664259 0.67866355 0.75157553 0.56568313 0.49112761\n",
      " 0.63447773 0.54385221 0.66533059 0.31368369 0.77926004 0.63403094\n",
      " 0.58443934 0.27953923 0.07007358 0.70033205 0.55577427 0.18573292\n",
      " 0.14524466 0.74778831 0.55452091 0.5508765  0.67272484 0.68894547\n",
      " 0.63053054 0.60733348 0.52275586 0.74133992 0.63652056 0.3634443\n",
      " 0.70916331 0.53063989 0.26459208 0.52348489 0.59347594 0.3274973\n",
      " 0.4744561  0.65561205 0.67888868 0.69129509]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27 [0/43 (0%)]\tTrain Loss: 0.060517\n",
      "Train Epoch: 27 [10/43 (23%)]\tTrain Loss: 0.106530\n",
      "Train Epoch: 27 [20/43 (47%)]\tTrain Loss: 0.070724\n",
      "Train Epoch: 27 [30/43 (70%)]\tTrain Loss: 0.069308\n",
      "Train Epoch: 27 [40/43 (93%)]\tTrain Loss: 0.062005\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.38395292 0.67042023 0.72086513 0.37916505 0.07596529 0.04502109\n",
      " 0.76876146 0.5254457  0.05039241 0.04920319 0.07142048 0.0528506\n",
      " 0.09079936 0.37699571 0.24907859 0.44300094 0.33770549 0.41469151\n",
      " 0.37596923 0.61584634 0.58480227 0.4655765  0.63185996 0.46509469\n",
      " 0.5099321  0.60205609 0.66720188 0.60379785 0.39047721 0.3462857\n",
      " 0.73277354 0.53667992 0.69683391 0.02767883 0.28230819 0.35940966\n",
      " 0.31816983 0.32411602 0.45295146 0.52431542 0.44389144 0.4601455\n",
      " 0.23016284 0.34937665 0.05026173 0.04065724 0.06402276 0.27490181\n",
      " 0.52270287 0.37975484 0.47535813 0.02293336 0.02609469 0.04845661\n",
      " 0.52258557 0.03888509 0.57628375 0.06151562 0.41887423 0.03225606\n",
      " 0.57930082 0.29338479 0.6448822  0.65258354 0.72450125 0.56660205\n",
      " 0.548904   0.30892634 0.65265936 0.54654956 0.70138276 0.40871105\n",
      " 0.49907354 0.5980469  0.50193936 0.57421082 0.6900928  0.6720202\n",
      " 0.74178779 0.64705747 0.59125203 0.7716493  0.73184979 0.36633146\n",
      " 0.58321118 0.59476596 0.48212513 0.38270262 0.5406729  0.61607355\n",
      " 0.43290302 0.38906154 0.46283248 0.46484628 0.32358259 0.52505201\n",
      " 0.30349937 0.50845432 0.39665195 0.58628869 0.58846223 0.58394718\n",
      " 0.38641763 0.57239878 0.40573668 0.58054805 0.68992907 0.31081122\n",
      " 0.57001281 0.57061005 0.39245591 0.46988305 0.56223744 0.34538478\n",
      " 0.52900374 0.37493154 0.78984898 0.58011705]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.]\n",
      "Train Epoch: 28 [0/43 (0%)]\tTrain Loss: 0.067970\n",
      "Train Epoch: 28 [10/43 (23%)]\tTrain Loss: 0.088900\n",
      "Train Epoch: 28 [20/43 (47%)]\tTrain Loss: 0.078263\n",
      "Train Epoch: 28 [30/43 (70%)]\tTrain Loss: 0.071562\n",
      "Train Epoch: 28 [40/43 (93%)]\tTrain Loss: 0.134731\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.51659513 0.82405645 0.89844137 0.39811826 0.53492182 0.61087787\n",
      " 0.85288572 0.67124748 0.60556066 0.51653308 0.31543723 0.20404163\n",
      " 0.54865038 0.71552712 0.82295871 0.63855815 0.48731723 0.66830033\n",
      " 0.49940273 0.69322735 0.72438478 0.77416015 0.73107296 0.67735475\n",
      " 0.62098557 0.57735318 0.67034686 0.62652296 0.5868699  0.65463114\n",
      " 0.70152152 0.69185489 0.68403029 0.46741405 0.32646263 0.39704469\n",
      " 0.60649592 0.31991005 0.68302542 0.63468832 0.52987963 0.65089929\n",
      " 0.54691756 0.4706201  0.41715524 0.52970511 0.65749019 0.60364878\n",
      " 0.81633818 0.45649293 0.66777718 0.26312107 0.48932946 0.39328021\n",
      " 0.75177306 0.67738801 0.68752468 0.59129602 0.45119119 0.5178318\n",
      " 0.66707408 0.87465    0.60240364 0.57482773 0.72012335 0.74676698\n",
      " 0.82362217 0.86009419 0.68437928 0.7060141  0.79876375 0.82550836\n",
      " 0.60599172 0.74572551 0.67667401 0.47700313 0.89389455 0.86456841\n",
      " 0.84451926 0.70689493 0.68687499 0.79766065 0.73252004 0.87779403\n",
      " 0.62779009 0.76470006 0.63998449 0.63697499 0.812217   0.86907482\n",
      " 0.89927691 0.43256989 0.43649626 0.69755405 0.57500988 0.72934091\n",
      " 0.55169934 0.78737336 0.85916865 0.75550765 0.49398267 0.63281304\n",
      " 0.75594413 0.76174033 0.59829175 0.79060411 0.7863481  0.56048465\n",
      " 0.74861121 0.45840865 0.36107719 0.6221469  0.72518975 0.58786964\n",
      " 0.61930895 0.76693517 0.77492249 0.86213845]\n",
      "predict [1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 29 [0/43 (0%)]\tTrain Loss: 0.072323\n",
      "Train Epoch: 29 [10/43 (23%)]\tTrain Loss: 0.107251\n",
      "Train Epoch: 29 [20/43 (47%)]\tTrain Loss: 0.095560\n",
      "Train Epoch: 29 [30/43 (70%)]\tTrain Loss: 0.065292\n",
      "Train Epoch: 29 [40/43 (93%)]\tTrain Loss: 0.066054\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.76654756 0.80173945 0.87123156 0.58783621 0.80858898 0.65035862\n",
      " 0.54354393 0.81793678 0.64261156 0.74804515 0.75671089 0.7233941\n",
      " 0.57236481 0.9247613  0.8731901  0.23015988 0.3834919  0.56540745\n",
      " 0.26468241 0.70360821 0.35193804 0.50076967 0.46213311 0.48716196\n",
      " 0.43450856 0.59082532 0.6196239  0.57838279 0.35498875 0.53965014\n",
      " 0.6693297  0.64817458 0.6463837  0.46418431 0.58951974 0.83229369\n",
      " 0.88402009 0.56690729 0.62462938 0.49532771 0.59886026 0.63687283\n",
      " 0.90649021 0.54848611 0.81049395 0.80818444 0.54567868 0.71328574\n",
      " 0.73143339 0.69837368 0.71354979 0.67174441 0.85255599 0.51436722\n",
      " 0.43710649 0.70097888 0.46321061 0.65521288 0.59054786 0.54182082\n",
      " 0.61930329 0.82302886 0.72858739 0.79057449 0.57661295 0.83159029\n",
      " 0.71128422 0.6995737  0.80959392 0.86714667 0.59967923 0.54394901\n",
      " 0.7427389  0.73053247 0.77414739 0.73552161 0.74501753 0.7879318\n",
      " 0.70601785 0.381576   0.55764246 0.60217428 0.69989717 0.8934291\n",
      " 0.39205602 0.52728295 0.6733551  0.81050271 0.65221596 0.79499578\n",
      " 0.59466338 0.76217169 0.85648417 0.59877151 0.32459348 0.37759838\n",
      " 0.79993236 0.63098532 0.82285786 0.71127325 0.39269602 0.53916407\n",
      " 0.89722788 0.63093406 0.32904488 0.92605776 0.58615774 0.5134688\n",
      " 0.6624276  0.41864088 0.31391853 0.41146398 0.66513056 0.68993849\n",
      " 0.85213786 0.83693767 0.60886991 0.74021918]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 30 [0/43 (0%)]\tTrain Loss: 0.071916\n",
      "Train Epoch: 30 [10/43 (23%)]\tTrain Loss: 0.095144\n",
      "Train Epoch: 30 [20/43 (47%)]\tTrain Loss: 0.082175\n",
      "Train Epoch: 30 [30/43 (70%)]\tTrain Loss: 0.067654\n",
      "Train Epoch: 30 [40/43 (93%)]\tTrain Loss: 0.109947\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.24617335 0.86755168 0.79119182 0.1298297  0.24566819 0.17388666\n",
      " 0.87692535 0.20162249 0.35584536 0.14100407 0.17481083 0.21111567\n",
      " 0.21661647 0.81533498 0.81604278 0.57314211 0.47204033 0.44836688\n",
      " 0.53222954 0.51758069 0.48047981 0.53584969 0.62422818 0.71068823\n",
      " 0.43962643 0.39376652 0.69477379 0.5162338  0.15164398 0.32002947\n",
      " 0.46807075 0.67068654 0.67902529 0.28651783 0.27563831 0.21113855\n",
      " 0.0440279  0.37857077 0.59247231 0.40731657 0.19701131 0.42996022\n",
      " 0.11217733 0.23089455 0.21517938 0.23982745 0.15690926 0.14891855\n",
      " 0.17171644 0.21272397 0.12661962 0.17400202 0.24665421 0.17874435\n",
      " 0.49687269 0.14019652 0.53783768 0.23859063 0.25078204 0.18409966\n",
      " 0.75606149 0.92028153 0.80528873 0.77890944 0.70941925 0.70862985\n",
      " 0.6914621  0.93873465 0.69396728 0.18402576 0.80220038 0.59352267\n",
      " 0.56161356 0.27563769 0.68853748 0.29552242 0.95387572 0.78633636\n",
      " 0.78561848 0.40162343 0.56401074 0.74191666 0.78353202 0.7090041\n",
      " 0.42069259 0.7007435  0.85729712 0.74098223 0.70464581 0.87090087\n",
      " 0.76149863 0.12362595 0.47436938 0.73651087 0.34246746 0.27431032\n",
      " 0.27777833 0.52929193 0.57752651 0.65156949 0.22451432 0.53694236\n",
      " 0.49860922 0.44291294 0.57820302 0.63052016 0.6669274  0.19193447\n",
      " 0.54631537 0.2749497  0.51370096 0.4649469  0.29590005 0.36344758\n",
      " 0.75547945 0.41051766 0.34017727 0.69785869]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      "vote_pred [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 46 TN= 36 FN= 12 FP= 24\n",
      "TP+FP 70\n",
      "precision 0.6571428571428571\n",
      "recall 0.7931034482758621\n",
      "F1 0.71875\n",
      "acc 0.6949152542372882\n",
      "AUCp 0.696551724137931\n",
      "AUC 0.7787356321839081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 30, average recall: 0.7931, average precision: 0.6571,average F1: 0.7188, average accuracy: 0.6949, average AUC: 0.7787\n",
      "Train Epoch: 31 [0/43 (0%)]\tTrain Loss: 0.063912\n",
      "Train Epoch: 31 [10/43 (23%)]\tTrain Loss: 0.079738\n",
      "Train Epoch: 31 [20/43 (47%)]\tTrain Loss: 0.093830\n",
      "Train Epoch: 31 [30/43 (70%)]\tTrain Loss: 0.065049\n",
      "Train Epoch: 31 [40/43 (93%)]\tTrain Loss: 0.074664\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.06239183 0.81040144 0.86491114 0.02389848 0.03365016 0.11315719\n",
      " 0.67348373 0.01842594 0.03338718 0.04458434 0.03695326 0.02370887\n",
      " 0.03017236 0.64103138 0.36049604 0.65941781 0.57047009 0.51228875\n",
      " 0.65051115 0.53724444 0.62694705 0.39325133 0.61761624 0.58474743\n",
      " 0.57170486 0.70339674 0.62214345 0.46378699 0.40371227 0.51193124\n",
      " 0.85835123 0.7525413  0.88522333 0.03070984 0.02863939 0.01432641\n",
      " 0.39045382 0.2071334  0.46369204 0.61664891 0.69003934 0.75737387\n",
      " 0.04312488 0.27361444 0.01629129 0.03857511 0.11036821 0.05134588\n",
      " 0.02566373 0.04313623 0.01586131 0.02970284 0.04667108 0.00847716\n",
      " 0.53246015 0.02665428 0.72072995 0.03464231 0.0386329  0.03258259\n",
      " 0.7845608  0.68600917 0.67644465 0.78245068 0.81342983 0.79229462\n",
      " 0.61347389 0.54165053 0.68898761 0.03653738 0.5237391  0.77720338\n",
      " 0.52815181 0.56559157 0.8489387  0.68890649 0.89323622 0.91303653\n",
      " 0.84613568 0.65427828 0.88845021 0.67362839 0.80447143 0.6717844\n",
      " 0.50637579 0.58860165 0.71258378 0.63317591 0.73899102 0.88421148\n",
      " 0.8321507  0.01897324 0.65556097 0.7571677  0.03572582 0.03514143\n",
      " 0.02571896 0.50097913 0.43566385 0.74270886 0.65200424 0.5478065\n",
      " 0.71716219 0.50687796 0.59108257 0.56020284 0.60582232 0.24336968\n",
      " 0.85632366 0.54671216 0.40853599 0.45351064 0.02164657 0.64199162\n",
      " 0.59031159 0.42825106 0.6718815  0.88665825]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 32 [0/43 (0%)]\tTrain Loss: 0.066469\n",
      "Train Epoch: 32 [10/43 (23%)]\tTrain Loss: 0.092133\n",
      "Train Epoch: 32 [20/43 (47%)]\tTrain Loss: 0.076290\n",
      "Train Epoch: 32 [30/43 (70%)]\tTrain Loss: 0.059409\n",
      "Train Epoch: 32 [40/43 (93%)]\tTrain Loss: 0.064603\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.17876989 0.6442709  0.7681753  0.38187978 0.10969642 0.11563365\n",
      " 0.50233185 0.04365034 0.21414548 0.11669259 0.03056368 0.12964289\n",
      " 0.11192387 0.71685266 0.55659449 0.54409027 0.41372564 0.58238035\n",
      " 0.36131856 0.309021   0.40100074 0.34097278 0.44537887 0.45262912\n",
      " 0.4001928  0.31383455 0.61551183 0.44074145 0.1123911  0.57358754\n",
      " 0.66611958 0.42755747 0.48415276 0.3809213  0.27670285 0.22139753\n",
      " 0.26922497 0.2550925  0.59472018 0.4034124  0.51844299 0.43523106\n",
      " 0.56353861 0.2774142  0.25707981 0.13081236 0.04935819 0.67036003\n",
      " 0.20384033 0.06756631 0.06856109 0.11503126 0.1032735  0.39402121\n",
      " 0.15666191 0.13323405 0.41234496 0.13290855 0.19644408 0.1109009\n",
      " 0.5700717  0.37788132 0.59861612 0.53645068 0.63606572 0.70543557\n",
      " 0.69767773 0.76977992 0.72850245 0.68942851 0.39888626 0.30355442\n",
      " 0.41541079 0.70916754 0.55612564 0.36723584 0.54528409 0.37104306\n",
      " 0.518866   0.49153444 0.62733805 0.74387646 0.67117792 0.73659545\n",
      " 0.41526842 0.36731175 0.78660232 0.53565902 0.63134575 0.63768494\n",
      " 0.44064376 0.04955173 0.60299903 0.47252843 0.56181562 0.43345201\n",
      " 0.09635866 0.5111801  0.25115117 0.6851638  0.61852664 0.53518027\n",
      " 0.20918213 0.35381266 0.4533211  0.53817594 0.76068795 0.29719415\n",
      " 0.78388548 0.15420161 0.45993686 0.3993437  0.42990032 0.47501123\n",
      " 0.27642849 0.40833774 0.45058143 0.44041315]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Train Epoch: 33 [0/43 (0%)]\tTrain Loss: 0.058795\n",
      "Train Epoch: 33 [10/43 (23%)]\tTrain Loss: 0.077244\n",
      "Train Epoch: 33 [20/43 (47%)]\tTrain Loss: 0.082965\n",
      "Train Epoch: 33 [30/43 (70%)]\tTrain Loss: 0.060546\n",
      "Train Epoch: 33 [40/43 (93%)]\tTrain Loss: 0.092142\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [2.51004994e-01 5.92575014e-01 7.12111712e-01 5.85673749e-01\n",
      " 2.53413707e-01 2.40853831e-01 7.46296585e-01 4.06604618e-01\n",
      " 5.05184889e-01 7.97125220e-04 1.27977312e-01 1.79947849e-04\n",
      " 1.42586976e-01 4.50645030e-01 6.84628859e-02 2.83014089e-01\n",
      " 4.37264681e-01 7.15405047e-01 5.10170341e-01 7.72433221e-01\n",
      " 3.34295034e-01 4.96992081e-01 8.64200115e-01 4.40842479e-01\n",
      " 6.89348936e-01 6.15945935e-01 7.41761267e-01 3.91752243e-01\n",
      " 1.85443252e-01 3.64349693e-01 5.50667524e-01 2.88396418e-01\n",
      " 6.53096199e-01 2.54683822e-01 2.34975070e-01 2.65576214e-01\n",
      " 1.68711588e-01 2.09186301e-01 2.92754114e-01 7.65818536e-01\n",
      " 5.88165104e-01 3.16628665e-01 4.12954777e-01 1.93801567e-01\n",
      " 3.36675346e-01 2.39224955e-01 2.89853841e-01 2.88220048e-01\n",
      " 5.63221753e-01 4.59015340e-01 7.70995915e-01 1.53269365e-01\n",
      " 7.56913098e-04 1.74543828e-01 5.65570652e-01 2.15276741e-04\n",
      " 4.44685638e-01 1.49821819e-04 2.00713649e-01 2.26804391e-01\n",
      " 8.09382379e-01 8.04918483e-02 5.71006596e-01 5.36969960e-01\n",
      " 8.41907263e-01 5.63694596e-01 4.92254376e-01 1.19567335e-01\n",
      " 5.26104271e-01 3.93414557e-01 2.45316491e-01 3.81803960e-01\n",
      " 4.61654007e-01 4.68037307e-01 2.79291838e-01 5.94403267e-01\n",
      " 5.71191609e-01 6.00495458e-01 6.41048431e-01 8.18580687e-01\n",
      " 5.13541877e-01 7.75764942e-01 8.88112426e-01 5.27128279e-01\n",
      " 6.57032728e-01 5.39672732e-01 2.07514554e-01 2.86091775e-01\n",
      " 2.09421143e-01 2.36815035e-01 3.74955118e-01 4.48980361e-01\n",
      " 4.38825786e-01 3.88784230e-01 7.04042971e-01 5.31976104e-01\n",
      " 6.83302060e-04 3.31954628e-01 1.37916982e-01 2.78730094e-01\n",
      " 1.57039925e-01 6.80555582e-01 3.03185463e-01 5.09266615e-01\n",
      " 3.18842471e-01 1.46509647e-01 6.30430341e-01 1.76237091e-01\n",
      " 4.27263707e-01 4.44265306e-01 5.84097028e-01 2.33246773e-01\n",
      " 3.10397267e-01 4.20161068e-01 5.47593892e-01 6.28437936e-01\n",
      " 4.01600659e-01 3.09748739e-01]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.]\n",
      "Train Epoch: 34 [0/43 (0%)]\tTrain Loss: 0.062967\n",
      "Train Epoch: 34 [10/43 (23%)]\tTrain Loss: 0.090320\n",
      "Train Epoch: 34 [20/43 (47%)]\tTrain Loss: 0.087488\n",
      "Train Epoch: 34 [30/43 (70%)]\tTrain Loss: 0.114737\n",
      "Train Epoch: 34 [40/43 (93%)]\tTrain Loss: 0.059110\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.96228951 0.61069787 0.76463652 0.63441753 0.46613136 0.93325686\n",
      " 0.7971307  0.70226055 0.92563468 0.49354446 0.32882276 0.50760192\n",
      " 0.49689224 0.91833168 0.7564764  0.29536039 0.38299966 0.58144659\n",
      " 0.93681544 0.87521142 0.88547456 0.55679017 0.55648762 0.54829401\n",
      " 0.44799626 0.54147291 0.88917297 0.87444276 0.93920404 0.8763836\n",
      " 0.66224301 0.72319907 0.84519964 0.3183994  0.44132927 0.43571341\n",
      " 0.26239604 0.44757935 0.44102037 0.44281167 0.53542763 0.47689754\n",
      " 0.52599239 0.48007274 0.60558981 0.76339912 0.57055736 0.70769352\n",
      " 0.76424986 0.39601406 0.6677922  0.37813726 0.61547166 0.53290725\n",
      " 0.85406017 0.65602332 0.5279814  0.32666153 0.33462363 0.54006237\n",
      " 0.84019798 0.88819224 0.73488456 0.67036217 0.93783355 0.69591069\n",
      " 0.66859531 0.93901247 0.68347961 0.65173429 0.86948293 0.87844181\n",
      " 0.47432926 0.56239694 0.79344982 0.64484507 0.80673259 0.82675821\n",
      " 0.95777881 0.60357392 0.6069749  0.7985357  0.79018492 0.83474755\n",
      " 0.54425025 0.59967226 0.92105371 0.92280614 0.85865408 0.51986593\n",
      " 0.74037939 0.51742983 0.74040616 0.41187635 0.93292624 0.92426372\n",
      " 0.55030531 0.27348682 0.53663641 0.62979567 0.55537039 0.59041768\n",
      " 0.74969047 0.45383826 0.94689798 0.9011175  0.9358651  0.27206895\n",
      " 0.59697026 0.87707084 0.61970299 0.41015857 0.56192636 0.56852889\n",
      " 0.72255361 0.82290143 0.60360527 0.81967413]\n",
      "predict [1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35 [0/43 (0%)]\tTrain Loss: 0.065475\n",
      "Train Epoch: 35 [10/43 (23%)]\tTrain Loss: 0.098199\n",
      "Train Epoch: 35 [20/43 (47%)]\tTrain Loss: 0.054282\n",
      "Train Epoch: 35 [30/43 (70%)]\tTrain Loss: 0.076068\n",
      "Train Epoch: 35 [40/43 (93%)]\tTrain Loss: 0.083048\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.32778707 0.8131302  0.76759976 0.55605376 0.28543276 0.40905792\n",
      " 0.89076895 0.75025541 0.28873667 0.16283809 0.23216574 0.12949786\n",
      " 0.23269898 0.44461548 0.53856748 0.34096929 0.23864293 0.34789938\n",
      " 0.9789353  0.9582001  0.92521286 0.26974031 0.32955259 0.3557626\n",
      " 0.17708749 0.32654706 0.98191488 0.59903318 0.15266702 0.47299743\n",
      " 0.57939053 0.57265598 0.70051277 0.31340024 0.2306577  0.21639903\n",
      " 0.26661342 0.2851139  0.29956272 0.48441112 0.31954181 0.44797033\n",
      " 0.24571018 0.42143583 0.58106863 0.65313971 0.41787675 0.78363031\n",
      " 0.6225192  0.33617258 0.5348838  0.27785569 0.10084458 0.32257736\n",
      " 0.94700092 0.15781347 0.39891458 0.13625319 0.27370283 0.29561281\n",
      " 0.57092333 0.37810415 0.7650556  0.82420623 0.95332217 0.53294027\n",
      " 0.45189625 0.56060636 0.73106205 0.60145599 0.81875789 0.58647782\n",
      " 0.38066173 0.71938664 0.69834054 0.72972268 0.83592278 0.6736089\n",
      " 0.71971935 0.55877531 0.65084416 0.74390024 0.82411712 0.26179013\n",
      " 0.53635854 0.58866853 0.50144219 0.61513448 0.39458266 0.55958921\n",
      " 0.53298479 0.67072463 0.65884733 0.45208019 0.21583925 0.46862754\n",
      " 0.1550696  0.30894786 0.4476701  0.67212868 0.31882948 0.67897332\n",
      " 0.60454392 0.36771071 0.2040219  0.97421187 0.67204714 0.1953342\n",
      " 0.64011574 0.95220393 0.37308365 0.41091669 0.41816339 0.46050718\n",
      " 0.39770037 0.27699438 0.20799743 0.53290409]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Train Epoch: 36 [0/43 (0%)]\tTrain Loss: 0.079258\n",
      "Train Epoch: 36 [10/43 (23%)]\tTrain Loss: 0.083998\n",
      "Train Epoch: 36 [20/43 (47%)]\tTrain Loss: 0.067542\n",
      "Train Epoch: 36 [30/43 (70%)]\tTrain Loss: 0.070598\n",
      "Train Epoch: 36 [40/43 (93%)]\tTrain Loss: 0.070554\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.65491956 0.80397642 0.78356296 0.59054965 0.63102758 0.67849648\n",
      " 0.79541826 0.77252567 0.55154896 0.51283711 0.51095915 0.32800701\n",
      " 0.27969965 0.70236164 0.53987134 0.47799835 0.38163251 0.56649768\n",
      " 0.43868241 0.47360083 0.48966452 0.29216757 0.56392395 0.28946769\n",
      " 0.63540894 0.57293051 0.38822025 0.44215545 0.59759009 0.40204802\n",
      " 0.67034948 0.56322843 0.65967554 0.30813488 0.50564706 0.37150857\n",
      " 0.51020807 0.60536343 0.24541126 0.51773578 0.37033656 0.57682765\n",
      " 0.43040192 0.44660258 0.60377783 0.63074803 0.61886418 0.58548349\n",
      " 0.61130196 0.395439   0.65985537 0.50795108 0.32619223 0.31125328\n",
      " 0.37451124 0.32036465 0.52825683 0.45045304 0.29092261 0.5861876\n",
      " 0.39404646 0.74477524 0.61764711 0.73026347 0.37846339 0.55776566\n",
      " 0.71147889 0.79199499 0.62942946 0.33785146 0.71527064 0.75843912\n",
      " 0.51591301 0.47336754 0.61626065 0.61120802 0.76834631 0.78472334\n",
      " 0.73546308 0.57988966 0.52089685 0.69021857 0.56366116 0.45680764\n",
      " 0.37991542 0.42861328 0.45037159 0.60915476 0.78115791 0.51890212\n",
      " 0.74415517 0.42331058 0.70730442 0.66454357 0.53391188 0.44151539\n",
      " 0.42955068 0.64493233 0.4963308  0.46229455 0.27332959 0.3533237\n",
      " 0.62752044 0.25138542 0.45437133 0.36772162 0.31267202 0.35140041\n",
      " 0.33476236 0.29747599 0.50879472 0.28627783 0.67797154 0.44802514\n",
      " 0.54604924 0.65286738 0.45552218 0.8459298 ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1.]\n",
      "Train Epoch: 37 [0/43 (0%)]\tTrain Loss: 0.065785\n",
      "Train Epoch: 37 [10/43 (23%)]\tTrain Loss: 0.068842\n",
      "Train Epoch: 37 [20/43 (47%)]\tTrain Loss: 0.064580\n",
      "Train Epoch: 37 [30/43 (70%)]\tTrain Loss: 0.056478\n",
      "Train Epoch: 37 [40/43 (93%)]\tTrain Loss: 0.073739\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.24088874 0.87083799 0.80209726 0.2160826  0.17056809 0.15395172\n",
      " 0.736431   0.2391489  0.1377639  0.16276382 0.13835371 0.18405698\n",
      " 0.20562905 0.66233146 0.76720858 0.42246106 0.40706238 0.54359382\n",
      " 0.31495774 0.14414641 0.19490841 0.5531252  0.54082942 0.15564352\n",
      " 0.25645199 0.29887581 0.09586055 0.4688288  0.50778937 0.30273619\n",
      " 0.82635224 0.7069869  0.52351236 0.09547599 0.26302812 0.2336242\n",
      " 0.38837436 0.09432875 0.31500629 0.43585411 0.56346083 0.52790582\n",
      " 0.08993515 0.33258241 0.52351695 0.7521106  0.52601278 0.5315612\n",
      " 0.1107062  0.31198978 0.16705222 0.20496367 0.13247252 0.26838174\n",
      " 0.08195697 0.35304734 0.08450578 0.32518178 0.10338011 0.37529767\n",
      " 0.56600916 0.76162714 0.74848753 0.83856958 0.08653737 0.6550231\n",
      " 0.57407433 0.84282047 0.07565117 0.18249647 0.73298466 0.59068149\n",
      " 0.13079627 0.18339291 0.16547929 0.58378786 0.74148798 0.81066239\n",
      " 0.47665268 0.50327677 0.75528497 0.7695238  0.76632059 0.37546605\n",
      " 0.41346735 0.4484137  0.47891986 0.47454429 0.56657863 0.84860355\n",
      " 0.69359279 0.11665896 0.47854799 0.11022752 0.18398811 0.14089696\n",
      " 0.09366475 0.62226301 0.36570272 0.64584786 0.31044593 0.40301001\n",
      " 0.49339199 0.2763752  0.16234793 0.50854754 0.19274975 0.07981883\n",
      " 0.49381983 0.10851014 0.35863024 0.29796436 0.15552372 0.19803266\n",
      " 0.51063639 0.42086339 0.42932951 0.53546494]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "Train Epoch: 38 [0/43 (0%)]\tTrain Loss: 0.061222\n",
      "Train Epoch: 38 [10/43 (23%)]\tTrain Loss: 0.052523\n",
      "Train Epoch: 38 [20/43 (47%)]\tTrain Loss: 0.071500\n",
      "Train Epoch: 38 [30/43 (70%)]\tTrain Loss: 0.060944\n",
      "Train Epoch: 38 [40/43 (93%)]\tTrain Loss: 0.110566\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.97663748 0.89160126 0.96415704 0.97788793 0.97607076 0.94168401\n",
      " 0.84837103 0.96947867 0.93747133 0.96462959 0.96776342 0.9718821\n",
      " 0.74633735 0.91295129 0.72477329 0.52179915 0.51029426 0.60499936\n",
      " 0.50911957 0.57997906 0.71993512 0.58854574 0.73213083 0.525756\n",
      " 0.48438644 0.62046653 0.60440856 0.42124373 0.63967139 0.26303029\n",
      " 0.78958714 0.57015622 0.63254112 0.98911428 0.94299752 0.39977294\n",
      " 0.4214274  0.65501809 0.41212347 0.72249019 0.46466309 0.6673246\n",
      " 0.98370659 0.4086923  0.76181448 0.9447602  0.97103947 0.69188756\n",
      " 0.98670119 0.96927434 0.98464495 0.98723084 0.97500151 0.93566132\n",
      " 0.58866274 0.97330904 0.6234507  0.97506207 0.96355128 0.97417045\n",
      " 0.91868246 0.84094906 0.61408848 0.72616976 0.94585103 0.84787369\n",
      " 0.74761015 0.61035711 0.79358625 0.96203494 0.94323993 0.96213681\n",
      " 0.78503096 0.87490743 0.86273831 0.72915024 0.88668543 0.89421654\n",
      " 0.91590261 0.58890933 0.63735873 0.86371577 0.92461371 0.74342036\n",
      " 0.72986066 0.70336676 0.88186735 0.94488639 0.95993543 0.90470493\n",
      " 0.91076666 0.97520185 0.84578466 0.85918528 0.9825477  0.93928921\n",
      " 0.9491697  0.79799968 0.91222328 0.81956339 0.58769548 0.47757208\n",
      " 0.7927295  0.44140208 0.47565314 0.90782452 0.7853924  0.60109371\n",
      " 0.81428915 0.44386005 0.29679057 0.50637853 0.8177861  0.69782519\n",
      " 0.85727662 0.76165837 0.7910977  0.92297035]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 39 [0/43 (0%)]\tTrain Loss: 0.069252\n",
      "Train Epoch: 39 [10/43 (23%)]\tTrain Loss: 0.090457\n",
      "Train Epoch: 39 [20/43 (47%)]\tTrain Loss: 0.062691\n",
      "Train Epoch: 39 [30/43 (70%)]\tTrain Loss: 0.069297\n",
      "Train Epoch: 39 [40/43 (93%)]\tTrain Loss: 0.059664\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.28230032 0.90251255 0.8635391  0.20170464 0.11282353 0.19053347\n",
      " 0.86335522 0.27575716 0.22511636 0.15882018 0.17703724 0.16428581\n",
      " 0.22720054 0.83740884 0.68168586 0.70795572 0.32194272 0.67866492\n",
      " 0.64646053 0.5898295  0.55913758 0.56336677 0.50348943 0.47264752\n",
      " 0.31763169 0.63074535 0.80761313 0.42301026 0.38589558 0.53468561\n",
      " 0.66816938 0.51642042 0.65191746 0.18599574 0.14157604 0.17468126\n",
      " 0.58842039 0.19522786 0.52789986 0.64368302 0.57934558 0.70259452\n",
      " 0.19325715 0.45827821 0.42571011 0.21385743 0.3524217  0.21087675\n",
      " 0.13161801 0.14775649 0.1989882  0.28152943 0.19873239 0.23674278\n",
      " 0.46439284 0.19244857 0.47017446 0.22122087 0.13119431 0.16015664\n",
      " 0.78097433 0.66013181 0.8505156  0.78253949 0.61980689 0.70537221\n",
      " 0.65458709 0.88647395 0.77281266 0.16185401 0.85066634 0.84305704\n",
      " 0.81211978 0.82380867 0.81326097 0.71982652 0.88096112 0.81866652\n",
      " 0.85569775 0.56914312 0.85684294 0.78117812 0.66822618 0.32646513\n",
      " 0.670681   0.55987865 0.66123122 0.78848165 0.78160995 0.59580249\n",
      " 0.73955542 0.18847446 0.79082698 0.76630944 0.16383126 0.2521027\n",
      " 0.24956031 0.79330337 0.53910303 0.84319973 0.66585088 0.63758558\n",
      " 0.66315645 0.48306149 0.32855079 0.91523778 0.79052198 0.35185429\n",
      " 0.8879354  0.57499951 0.72553641 0.65299535 0.90301555 0.84122533\n",
      " 0.87427098 0.63780838 0.55588263 0.74198329]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 40 [0/43 (0%)]\tTrain Loss: 0.059017\n",
      "Train Epoch: 40 [10/43 (23%)]\tTrain Loss: 0.086177\n",
      "Train Epoch: 40 [20/43 (47%)]\tTrain Loss: 0.061968\n",
      "Train Epoch: 40 [30/43 (70%)]\tTrain Loss: 0.079886\n",
      "Train Epoch: 40 [40/43 (93%)]\tTrain Loss: 0.057107\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.98464531 0.81251574 0.87482202 0.97900599 0.9908793  0.9777348\n",
      " 0.68314314 0.99021739 0.98173457 0.97691649 0.99417627 0.97695905\n",
      " 0.99004984 0.82446247 0.90585095 0.2023347  0.23219968 0.6669054\n",
      " 0.62244028 0.48816729 0.48297411 0.39969525 0.60754639 0.4543547\n",
      " 0.53987402 0.45941284 0.5379256  0.43372235 0.38396767 0.53622216\n",
      " 0.822366   0.68114382 0.88988042 0.99063814 0.99465948 0.17361623\n",
      " 0.27308136 0.18897237 0.40588865 0.65050232 0.56842428 0.75234425\n",
      " 0.98605019 0.29829746 0.21301027 0.98668796 0.99231708 0.99379468\n",
      " 0.99320424 0.98836654 0.9854278  0.99236476 0.98587972 0.99086034\n",
      " 0.58097512 0.97519898 0.3988367  0.99553597 0.97526395 0.97476089\n",
      " 0.79940426 0.98617768 0.82268155 0.71730536 0.73006451 0.68367684\n",
      " 0.7931866  0.82703012 0.79324239 0.98380464 0.96169502 0.97037458\n",
      " 0.89635241 0.7257781  0.74197358 0.63616043 0.79246116 0.82061934\n",
      " 0.75615513 0.4212409  0.68851638 0.75481284 0.82367438 0.83963746\n",
      " 0.39721236 0.48423475 0.96015412 0.96091104 0.91568863 0.32542714\n",
      " 0.93258607 0.99445999 0.49834839 0.67115426 0.99239963 0.98862833\n",
      " 0.98631454 0.78675073 0.68836546 0.7778281  0.24409094 0.41598463\n",
      " 0.71836573 0.51883751 0.18844609 0.8604899  0.78605336 0.16913003\n",
      " 0.77108014 0.24848613 0.4161078  0.33813354 0.74016082 0.6842066\n",
      " 0.6905815  0.32959995 0.58433294 0.68651462]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 40 TN= 37 FN= 18 FP= 23\n",
      "TP+FP 63\n",
      "precision 0.6349206349206349\n",
      "recall 0.6896551724137931\n",
      "F1 0.6611570247933884\n",
      "acc 0.652542372881356\n",
      "AUCp 0.6531609195402299\n",
      "AUC 0.7568965517241378\n",
      "\n",
      " The epoch is 40, average recall: 0.6897, average precision: 0.6349,average F1: 0.6612, average accuracy: 0.6525, average AUC: 0.7569\n",
      "Train Epoch: 41 [0/43 (0%)]\tTrain Loss: 0.104119\n",
      "Train Epoch: 41 [10/43 (23%)]\tTrain Loss: 0.073677\n",
      "Train Epoch: 41 [20/43 (47%)]\tTrain Loss: 0.071810\n",
      "Train Epoch: 41 [30/43 (70%)]\tTrain Loss: 0.088304\n",
      "Train Epoch: 41 [40/43 (93%)]\tTrain Loss: 0.100892\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01494228 0.88208157 0.78605306 0.0420542  0.04664093 0.04626579\n",
      " 0.87108493 0.03509112 0.08881105 0.07490515 0.05428594 0.05205429\n",
      " 0.06757174 0.69053817 0.52897346 0.29661417 0.2579248  0.48391828\n",
      " 0.47599575 0.63766623 0.24746835 0.24454254 0.28054234 0.79993224\n",
      " 0.37074462 0.70194429 0.68657041 0.61041975 0.20906474 0.36880633\n",
      " 0.79055691 0.59617925 0.73174679 0.07926249 0.03221074 0.05720654\n",
      " 0.28023505 0.01825228 0.42493564 0.29357886 0.27068332 0.29844764\n",
      " 0.03221711 0.28786108 0.27900863 0.02211514 0.0263529  0.01887348\n",
      " 0.07179825 0.05309331 0.0258751  0.028553   0.02739287 0.03117132\n",
      " 0.35830563 0.05853182 0.6208871  0.03523586 0.06806123 0.06889594\n",
      " 0.78377432 0.36149812 0.72599041 0.70414752 0.78320527 0.74648404\n",
      " 0.77928686 0.66751111 0.65108979 0.11501537 0.80588078 0.51643437\n",
      " 0.69549024 0.82702994 0.63388199 0.72417808 0.86581981 0.90249401\n",
      " 0.86712492 0.58003235 0.81006801 0.952218   0.91343975 0.27035931\n",
      " 0.72427332 0.46411446 0.44483104 0.69656074 0.70214623 0.54547668\n",
      " 0.47481078 0.01832923 0.55979878 0.04474073 0.07188455 0.02529218\n",
      " 0.0557019  0.31175244 0.50043702 0.73557889 0.41340593 0.60089749\n",
      " 0.41659683 0.35028112 0.31133896 0.19423229 0.65372658 0.158683\n",
      " 0.73160845 0.20556927 0.52982807 0.48557568 0.59717721 0.43328232\n",
      " 0.49714044 0.52149385 0.73054385 0.79142374]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1.]\n",
      "Train Epoch: 42 [0/43 (0%)]\tTrain Loss: 0.091951\n",
      "Train Epoch: 42 [10/43 (23%)]\tTrain Loss: 0.098206\n",
      "Train Epoch: 42 [20/43 (47%)]\tTrain Loss: 0.085404\n",
      "Train Epoch: 42 [30/43 (70%)]\tTrain Loss: 0.062685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42 [40/43 (93%)]\tTrain Loss: 0.064483\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.56127203 0.73268777 0.89611512 0.64897156 0.67538649 0.64713073\n",
      " 0.82080185 0.67789775 0.40250072 0.57695973 0.34238422 0.54308587\n",
      " 0.39621851 0.62944895 0.65801972 0.56709558 0.31349224 0.36479616\n",
      " 0.56113029 0.52257097 0.60415971 0.39629838 0.69596636 0.70311219\n",
      " 0.48530751 0.69016707 0.74901068 0.72314751 0.31753474 0.46548352\n",
      " 0.66502887 0.38973749 0.6442396  0.38462257 0.58242524 0.51133758\n",
      " 0.41183043 0.34843096 0.54922169 0.47689921 0.59255087 0.78881514\n",
      " 0.56758827 0.25151533 0.36008811 0.56566817 0.69695139 0.461725\n",
      " 0.6346494  0.46662104 0.5201624  0.75830781 0.5229153  0.41286114\n",
      " 0.78110546 0.58403027 0.48180011 0.77877629 0.50526339 0.72210389\n",
      " 0.75197107 0.6821     0.75591868 0.82063651 0.73332196 0.7623446\n",
      " 0.66384143 0.77363825 0.81864709 0.7380926  0.88834113 0.85767853\n",
      " 0.65581238 0.76504207 0.72593343 0.57736516 0.75682831 0.8822341\n",
      " 0.70259279 0.58257186 0.69954371 0.44431597 0.65426064 0.62316304\n",
      " 0.22805397 0.48711529 0.79719061 0.75598979 0.6559242  0.83530724\n",
      " 0.72978616 0.58315307 0.65146476 0.64414293 0.55635542 0.72091609\n",
      " 0.53349477 0.74777454 0.52459615 0.70118314 0.28541765 0.57213688\n",
      " 0.46240473 0.41431415 0.5387547  0.69711095 0.49851018 0.44718215\n",
      " 0.80042726 0.47837371 0.7238363  0.57533169 0.73572165 0.80171281\n",
      " 0.56179613 0.47890133 0.59817135 0.68939888]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 43 [0/43 (0%)]\tTrain Loss: 0.059867\n",
      "Train Epoch: 43 [10/43 (23%)]\tTrain Loss: 0.070053\n",
      "Train Epoch: 43 [20/43 (47%)]\tTrain Loss: 0.067567\n",
      "Train Epoch: 43 [30/43 (70%)]\tTrain Loss: 0.084473\n",
      "Train Epoch: 43 [40/43 (93%)]\tTrain Loss: 0.075109\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.42473343 0.87254179 0.93807381 0.54065502 0.40157342 0.41777706\n",
      " 0.93120152 0.40500236 0.41454959 0.52615839 0.51217544 0.58802581\n",
      " 0.37774327 0.90478915 0.85058403 0.38704908 0.54833919 0.4239431\n",
      " 0.40146077 0.4173829  0.68785179 0.35862982 0.56877708 0.75902176\n",
      " 0.28672069 0.43252054 0.67956275 0.62848866 0.62512356 0.7793262\n",
      " 0.76797754 0.5273518  0.86760837 0.37843513 0.59417397 0.35131902\n",
      " 0.50703245 0.63777363 0.6627571  0.66314179 0.33175501 0.29704928\n",
      " 0.46161911 0.44412896 0.54215848 0.50579607 0.59081656 0.52299631\n",
      " 0.62042725 0.52501976 0.54406393 0.50651741 0.47058716 0.71644545\n",
      " 0.52929211 0.5342924  0.7013669  0.57839173 0.54651207 0.41496199\n",
      " 0.92928404 0.88315547 0.86790603 0.85208482 0.65447789 0.75617075\n",
      " 0.66198951 0.85041654 0.85930741 0.4279699  0.92618495 0.92625958\n",
      " 0.79331875 0.56160277 0.74797016 0.88733786 0.92777514 0.96554673\n",
      " 0.89852607 0.65934503 0.78527981 0.89136553 0.90809262 0.84701622\n",
      " 0.36881089 0.64888293 0.93192226 0.91550148 0.94368494 0.64398277\n",
      " 0.83309823 0.5383935  0.57677859 0.84108835 0.45373753 0.28944314\n",
      " 0.30604351 0.90343195 0.67017621 0.7231254  0.51955712 0.6780681\n",
      " 0.7376712  0.305628   0.74759567 0.87837863 0.82067555 0.42673898\n",
      " 0.89297336 0.23635435 0.74788219 0.39622766 0.73264062 0.63937587\n",
      " 0.88126737 0.79176968 0.74758661 0.72967243]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 44 [0/43 (0%)]\tTrain Loss: 0.079360\n",
      "Train Epoch: 44 [10/43 (23%)]\tTrain Loss: 0.086408\n",
      "Train Epoch: 44 [20/43 (47%)]\tTrain Loss: 0.066496\n",
      "Train Epoch: 44 [30/43 (70%)]\tTrain Loss: 0.079493\n",
      "Train Epoch: 44 [40/43 (93%)]\tTrain Loss: 0.095686\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.00948212 0.74779874 0.85899568 0.00843315 0.01213066 0.00645553\n",
      " 0.82517898 0.00704712 0.02542935 0.0090221  0.00701542 0.0084567\n",
      " 0.10213299 0.88306832 0.58039027 0.36033854 0.61051464 0.53389758\n",
      " 0.38827792 0.51109773 0.19540831 0.19695774 0.7889958  0.86141866\n",
      " 0.39410868 0.82825869 0.64378333 0.68634999 0.31760582 0.43146926\n",
      " 0.72347862 0.48877373 0.70734918 0.0032287  0.01031237 0.43861204\n",
      " 0.33340544 0.64728427 0.23241897 0.43771696 0.52722025 0.34367487\n",
      " 0.01091882 0.22640282 0.36309433 0.00445288 0.00885493 0.64461446\n",
      " 0.00311395 0.00672809 0.01240508 0.00838295 0.01768913 0.01207889\n",
      " 0.66254455 0.00622596 0.69709915 0.00623733 0.03205725 0.01114733\n",
      " 0.7234509  0.7017895  0.77034396 0.8720839  0.86561209 0.81527609\n",
      " 0.4703972  0.49896547 0.86317331 0.0086057  0.61047494 0.62429982\n",
      " 0.75491899 0.62055087 0.86944681 0.58378696 0.76815802 0.79795641\n",
      " 0.74060816 0.49672392 0.66607916 0.84089398 0.77820212 0.58386475\n",
      " 0.44035912 0.58391762 0.68879759 0.86973637 0.31242242 0.42864814\n",
      " 0.57469159 0.00385473 0.52850282 0.59430003 0.00862775 0.00378411\n",
      " 0.01180548 0.41490215 0.80919272 0.65979016 0.16610894 0.58296746\n",
      " 0.44047362 0.4160248  0.50342321 0.41056246 0.70399207 0.36836106\n",
      " 0.65024805 0.56577963 0.63880932 0.31379786 0.80866414 0.61882889\n",
      " 0.34619665 0.88115799 0.82850504 0.45689803]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      "Train Epoch: 45 [0/43 (0%)]\tTrain Loss: 0.073666\n",
      "Train Epoch: 45 [10/43 (23%)]\tTrain Loss: 0.052615\n",
      "Train Epoch: 45 [20/43 (47%)]\tTrain Loss: 0.080026\n",
      "Train Epoch: 45 [30/43 (70%)]\tTrain Loss: 0.076099\n",
      "Train Epoch: 45 [40/43 (93%)]\tTrain Loss: 0.063809\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.23181808 0.85864502 0.80058545 0.10940278 0.16064924 0.18580168\n",
      " 0.67370683 0.09640205 0.07897812 0.08921488 0.17498773 0.1476711\n",
      " 0.14582886 0.73356611 0.70162541 0.74241036 0.46455944 0.4269079\n",
      " 0.64805168 0.33243731 0.51945502 0.48946345 0.63527507 0.75947475\n",
      " 0.33948734 0.48727211 0.60824162 0.65337944 0.23876885 0.45334259\n",
      " 0.68840593 0.65895224 0.62348056 0.19983329 0.14445555 0.18669964\n",
      " 0.32928506 0.48018107 0.5136438  0.43778402 0.62940389 0.47712666\n",
      " 0.14733119 0.59167045 0.31530604 0.18623134 0.09764158 0.11552812\n",
      " 0.12182816 0.09943327 0.20595303 0.19521384 0.13434802 0.0686528\n",
      " 0.39515078 0.09606186 0.82793325 0.11749639 0.17185542 0.14813057\n",
      " 0.5569151  0.6536954  0.80629671 0.71756703 0.63087118 0.76406687\n",
      " 0.60709709 0.84382367 0.69417596 0.14445712 0.66305923 0.71103925\n",
      " 0.611817   0.58841842 0.65286922 0.48836678 0.77807504 0.90251207\n",
      " 0.77775341 0.5601449  0.65178984 0.67132163 0.72596568 0.42168441\n",
      " 0.50622487 0.34903365 0.8561815  0.72776246 0.74206811 0.7836929\n",
      " 0.495828   0.14193738 0.76632875 0.41794908 0.07087225 0.09839611\n",
      " 0.17526859 0.60252088 0.53573692 0.66061532 0.41175032 0.46484053\n",
      " 0.51630163 0.45177001 0.47340962 0.86187118 0.61380816 0.46263078\n",
      " 0.7203663  0.46236953 0.70113176 0.70455444 0.11793254 0.62014139\n",
      " 0.64812702 0.73040521 0.65812546 0.76595515]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [0/43 (0%)]\tTrain Loss: 0.074025\n",
      "Train Epoch: 46 [10/43 (23%)]\tTrain Loss: 0.088231\n",
      "Train Epoch: 46 [20/43 (47%)]\tTrain Loss: 0.052565\n",
      "Train Epoch: 46 [30/43 (70%)]\tTrain Loss: 0.072956\n",
      "Train Epoch: 46 [40/43 (93%)]\tTrain Loss: 0.092684\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.69550467 0.94231659 0.85392213 0.75852674 0.69589281 0.44938961\n",
      " 0.93937629 0.60839367 0.54663479 0.46993345 0.79124957 0.62402838\n",
      " 0.47048271 0.77977121 0.89267051 0.50218648 0.51733381 0.36583742\n",
      " 0.40903035 0.37117821 0.52395916 0.49559632 0.71563017 0.72512162\n",
      " 0.25446406 0.6595695  0.74802244 0.52751911 0.52453136 0.36841175\n",
      " 0.81221575 0.7493754  0.78526473 0.71921587 0.76951045 0.66386318\n",
      " 0.28802246 0.70097917 0.20304044 0.47803673 0.50330704 0.50103885\n",
      " 0.71221215 0.38339204 0.77038085 0.61526859 0.67636037 0.61691421\n",
      " 0.47770107 0.50365555 0.56876844 0.66312987 0.54098296 0.61176103\n",
      " 0.3135255  0.51330501 0.37866798 0.4580465  0.77318287 0.69615781\n",
      " 0.74152845 0.8346979  0.74747711 0.77041101 0.91701967 0.824301\n",
      " 0.69841754 0.85506654 0.87419832 0.6436978  0.80553919 0.81915939\n",
      " 0.76373279 0.81361485 0.93178016 0.74233049 0.94686294 0.96164244\n",
      " 0.85167867 0.53106254 0.6885401  0.90655351 0.79640937 0.32549593\n",
      " 0.53878784 0.56858063 0.77882439 0.81363028 0.92389542 0.82455134\n",
      " 0.74559546 0.66263819 0.34211046 0.95230782 0.48774463 0.72447127\n",
      " 0.4321447  0.55374753 0.39796212 0.75482517 0.48266873 0.68663996\n",
      " 0.69645399 0.16214827 0.2536065  0.80193597 0.64363676 0.12090661\n",
      " 0.49606267 0.24474955 0.73135233 0.44532683 0.77549559 0.54541034\n",
      " 0.44248936 0.55476171 0.63294363 0.71374905]\n",
      "predict [1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1.]\n",
      "Train Epoch: 47 [0/43 (0%)]\tTrain Loss: 0.080713\n",
      "Train Epoch: 47 [10/43 (23%)]\tTrain Loss: 0.062825\n",
      "Train Epoch: 47 [20/43 (47%)]\tTrain Loss: 0.088365\n",
      "Train Epoch: 47 [30/43 (70%)]\tTrain Loss: 0.068163\n",
      "Train Epoch: 47 [40/43 (93%)]\tTrain Loss: 0.094929\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.88910908 0.89648265 0.83706093 0.87689525 0.77767444 0.71753365\n",
      " 0.88976496 0.91669792 0.8545959  0.76581788 0.90478855 0.8709057\n",
      " 0.86319894 0.65118748 0.8638733  0.259489   0.23971561 0.38078955\n",
      " 0.10887715 0.35124147 0.11169965 0.25761682 0.53095609 0.70184726\n",
      " 0.28467751 0.33415443 0.55999744 0.31901559 0.27235213 0.27522638\n",
      " 0.78140354 0.60193509 0.60962111 0.74996173 0.81459802 0.08805411\n",
      " 0.09250581 0.33582595 0.20604824 0.44973382 0.47991502 0.41029984\n",
      " 0.95090264 0.24428165 0.26929784 0.85408914 0.7596001  0.52161765\n",
      " 0.94813657 0.84247994 0.84287846 0.92839855 0.9080528  0.94853228\n",
      " 0.23713113 0.86971229 0.29459313 0.88245827 0.7643258  0.79018277\n",
      " 0.79563677 0.37760603 0.76567972 0.60025734 0.83711624 0.56573457\n",
      " 0.71975428 0.73901075 0.88922954 0.85110402 0.95006895 0.87825054\n",
      " 0.66471899 0.6191209  0.45583498 0.52688575 0.92541528 0.90055621\n",
      " 0.82651579 0.70945358 0.24054198 0.73224658 0.81605017 0.66993582\n",
      " 0.20542099 0.38151646 0.87736219 0.85402459 0.82190347 0.90212667\n",
      " 0.69133294 0.93058598 0.5950101  0.88294935 0.89157861 0.92983019\n",
      " 0.91189688 0.7537241  0.6033383  0.62049735 0.29933655 0.46165511\n",
      " 0.77150077 0.37745303 0.40560853 0.55703717 0.61603028 0.10287739\n",
      " 0.57093465 0.25211868 0.07154059 0.6486212  0.51162034 0.21646395\n",
      " 0.71574813 0.46230039 0.73099059 0.8812058 ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1.]\n",
      "Train Epoch: 48 [0/43 (0%)]\tTrain Loss: 0.069784\n",
      "Train Epoch: 48 [10/43 (23%)]\tTrain Loss: 0.083982\n",
      "Train Epoch: 48 [20/43 (47%)]\tTrain Loss: 0.071058\n",
      "Train Epoch: 48 [30/43 (70%)]\tTrain Loss: 0.064359\n",
      "Train Epoch: 48 [40/43 (93%)]\tTrain Loss: 0.059562\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.72539705 0.88808674 0.77553862 0.78082633 0.62301159 0.63705868\n",
      " 0.86297631 0.89611691 0.7205106  0.51178175 0.49890888 0.60532576\n",
      " 0.51401997 0.71621799 0.64342684 0.70459914 0.6261425  0.64827919\n",
      " 0.70440269 0.65214586 0.60409689 0.48009863 0.55161333 0.61978453\n",
      " 0.40855876 0.64546996 0.8317669  0.65517944 0.48001045 0.57809556\n",
      " 0.77113372 0.72131503 0.57082701 0.43094373 0.40301713 0.55571032\n",
      " 0.30990449 0.49609959 0.40227127 0.56852585 0.42812839 0.62851393\n",
      " 0.46716908 0.52080995 0.576446   0.80324191 0.52065945 0.6775049\n",
      " 0.8559289  0.72831714 0.76405686 0.3583675  0.38252613 0.44395244\n",
      " 0.57907915 0.54594004 0.58487201 0.46515337 0.64549249 0.46912485\n",
      " 0.82547188 0.74662727 0.7701202  0.84978235 0.75225031 0.66512179\n",
      " 0.79933506 0.61194664 0.83361769 0.64494741 0.88536531 0.79598349\n",
      " 0.81474471 0.76988322 0.71118355 0.76606488 0.92986834 0.84006822\n",
      " 0.89043212 0.74095494 0.63385338 0.8849802  0.90183002 0.29659057\n",
      " 0.69721127 0.69065011 0.57159942 0.58837295 0.70900273 0.47154123\n",
      " 0.65210944 0.63850367 0.67625248 0.83583564 0.45749918 0.59639239\n",
      " 0.43471304 0.65903938 0.65818483 0.73392034 0.54155415 0.66804075\n",
      " 0.44499919 0.41750261 0.73823798 0.56674975 0.76887745 0.37971169\n",
      " 0.74356872 0.41792646 0.62273049 0.7507118  0.67395645 0.76498497\n",
      " 0.57821542 0.48641974 0.73447156 0.77426153]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 49 [0/43 (0%)]\tTrain Loss: 0.056029\n",
      "Train Epoch: 49 [10/43 (23%)]\tTrain Loss: 0.050233\n",
      "Train Epoch: 49 [20/43 (47%)]\tTrain Loss: 0.061505\n",
      "Train Epoch: 49 [30/43 (70%)]\tTrain Loss: 0.083632\n",
      "Train Epoch: 49 [40/43 (93%)]\tTrain Loss: 0.096519\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.49258792 0.76262635 0.86013246 0.17318736 0.12897772 0.22577929\n",
      " 0.91179043 0.78738922 0.13174303 0.19099265 0.24959221 0.18162577\n",
      " 0.14890696 0.71983373 0.61913705 0.33216685 0.1856503  0.48877996\n",
      " 0.53029722 0.45877585 0.58373988 0.53330624 0.66494    0.69970149\n",
      " 0.64244473 0.61306471 0.87502432 0.51998764 0.28631574 0.5128991\n",
      " 0.83203131 0.69734532 0.57396275 0.17630456 0.23319925 0.33509439\n",
      " 0.51203382 0.48060352 0.41910964 0.5586803  0.65149933 0.65300071\n",
      " 0.09973192 0.28496361 0.24669839 0.10511295 0.2463765  0.26256675\n",
      " 0.09589829 0.09240869 0.13313366 0.15647259 0.20348749 0.20671695\n",
      " 0.79752141 0.11112402 0.66342491 0.20049773 0.20846921 0.17022836\n",
      " 0.43217039 0.79186606 0.74127424 0.74140567 0.57975388 0.85681921\n",
      " 0.66203749 0.97988772 0.93038654 0.50647181 0.90067887 0.86291313\n",
      " 0.85626417 0.69819039 0.70308012 0.5074029  0.91499215 0.94415897\n",
      " 0.76183254 0.83192867 0.83326763 0.89515924 0.76399791 0.80715936\n",
      " 0.61427289 0.58807904 0.91762877 0.84719568 0.81329316 0.88831359\n",
      " 0.91181028 0.1546693  0.88314188 0.76888019 0.70646709 0.72464126\n",
      " 0.1987724  0.76133329 0.8456552  0.81175566 0.43315515 0.68673825\n",
      " 0.68300194 0.69594866 0.82584685 0.80340827 0.74767739 0.67510271\n",
      " 0.87601352 0.67208254 0.6813007  0.6033172  0.91892838 0.68827546\n",
      " 0.89806002 0.89200211 0.40610158 0.67771119]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [0/43 (0%)]\tTrain Loss: 0.054930\n",
      "Train Epoch: 50 [10/43 (23%)]\tTrain Loss: 0.073610\n",
      "Train Epoch: 50 [20/43 (47%)]\tTrain Loss: 0.100320\n",
      "Train Epoch: 50 [30/43 (70%)]\tTrain Loss: 0.065149\n",
      "Train Epoch: 50 [40/43 (93%)]\tTrain Loss: 0.079159\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.5559696  0.96227938 0.92959636 0.48811468 0.66219747 0.66237259\n",
      " 0.9391849  0.72643423 0.35751328 0.59601218 0.61192507 0.4843407\n",
      " 0.55702889 0.77324021 0.69419664 0.63563609 0.35446924 0.53654534\n",
      " 0.4321661  0.66004604 0.47986621 0.64787519 0.72318929 0.71025723\n",
      " 0.68913144 0.71468252 0.68590343 0.68216014 0.42420802 0.57232547\n",
      " 0.87650794 0.6640408  0.86177242 0.55843705 0.70870548 0.08106929\n",
      " 0.4529514  0.4073061  0.5609383  0.49955916 0.64441764 0.54865789\n",
      " 0.5644992  0.49287671 0.56938076 0.62388092 0.62984931 0.56851542\n",
      " 0.90250772 0.55517548 0.51583058 0.45765543 0.67734545 0.52070779\n",
      " 0.5154804  0.61394852 0.70439827 0.57040083 0.47710913 0.45963043\n",
      " 0.81874865 0.89158279 0.80343646 0.70854783 0.79736817 0.61435217\n",
      " 0.67257023 0.96421754 0.85273731 0.71223849 0.95351648 0.97515863\n",
      " 0.68580079 0.85699254 0.7432459  0.78594935 0.94729799 0.95647043\n",
      " 0.94301087 0.79738379 0.85976136 0.93478209 0.87409306 0.65780067\n",
      " 0.56610644 0.7384451  0.95649987 0.96083641 0.90623426 0.77326345\n",
      " 0.89107728 0.22115406 0.43930477 0.86142105 0.47260541 0.4124684\n",
      " 0.44405854 0.80744958 0.65210629 0.74256492 0.60039592 0.82021797\n",
      " 0.75365043 0.64091641 0.48706138 0.9582442  0.65550596 0.16329356\n",
      " 0.80397886 0.71507686 0.43230212 0.64995962 0.55931777 0.62794483\n",
      " 0.80848134 0.81829381 0.73299873 0.87810451]\n",
      "predict [1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 49 TN= 37 FN= 9 FP= 23\n",
      "TP+FP 72\n",
      "precision 0.6805555555555556\n",
      "recall 0.8448275862068966\n",
      "F1 0.7538461538461538\n",
      "acc 0.7288135593220338\n",
      "AUCp 0.7307471264367816\n",
      "AUC 0.7922413793103449\n",
      "\n",
      " The epoch is 50, average recall: 0.8448, average precision: 0.6806,average F1: 0.7538, average accuracy: 0.7288, average AUC: 0.7922\n",
      "Train Epoch: 51 [0/43 (0%)]\tTrain Loss: 0.096307\n",
      "Train Epoch: 51 [10/43 (23%)]\tTrain Loss: 0.064601\n",
      "Train Epoch: 51 [20/43 (47%)]\tTrain Loss: 0.048534\n",
      "Train Epoch: 51 [30/43 (70%)]\tTrain Loss: 0.060652\n",
      "Train Epoch: 51 [40/43 (93%)]\tTrain Loss: 0.064050\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.93761712 0.96572834 0.93763316 0.93229234 0.93787456 0.75745606\n",
      " 0.89502919 0.82657182 0.91883129 0.82144862 0.6687938  0.69625282\n",
      " 0.7485612  0.95394325 0.85002279 0.57743829 0.57681602 0.40644136\n",
      " 0.42400384 0.3201004  0.30150548 0.42692074 0.59523463 0.74836653\n",
      " 0.24852096 0.57277471 0.83105183 0.49252072 0.32471862 0.47412285\n",
      " 0.8146438  0.75253624 0.69620216 0.77463126 0.91102171 0.34015569\n",
      " 0.43303695 0.44400871 0.37143585 0.2419305  0.38275257 0.24985471\n",
      " 0.76640159 0.42148215 0.8819558  0.77769095 0.82869101 0.80184263\n",
      " 0.91426879 0.84243184 0.80228025 0.82472664 0.70638943 0.87438476\n",
      " 0.64000255 0.88542747 0.46330354 0.8441534  0.74099737 0.8445279\n",
      " 0.70863575 0.87055165 0.76987046 0.84945512 0.82411718 0.84415269\n",
      " 0.71798617 0.89326179 0.91325521 0.92470801 0.95704854 0.97398859\n",
      " 0.75700909 0.85541558 0.79274577 0.78279352 0.97531736 0.97130805\n",
      " 0.96036118 0.72490805 0.7788564  0.90797305 0.97399628 0.5066365\n",
      " 0.4747906  0.66124594 0.8733713  0.87469274 0.81105947 0.83030856\n",
      " 0.96680498 0.85487902 0.75081366 0.89936244 0.61860663 0.6256125\n",
      " 0.91971153 0.513457   0.69141942 0.84780949 0.28146157 0.52180517\n",
      " 0.51544291 0.40395415 0.58499503 0.90864706 0.88641691 0.42322624\n",
      " 0.84301639 0.31435472 0.72857773 0.30338502 0.76539135 0.62506437\n",
      " 0.63341588 0.7130124  0.80935717 0.94483405]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 52 [0/43 (0%)]\tTrain Loss: 0.050827\n",
      "Train Epoch: 52 [10/43 (23%)]\tTrain Loss: 0.056258\n",
      "Train Epoch: 52 [20/43 (47%)]\tTrain Loss: 0.066807\n",
      "Train Epoch: 52 [30/43 (70%)]\tTrain Loss: 0.079008\n",
      "Train Epoch: 52 [40/43 (93%)]\tTrain Loss: 0.051715\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.70231622 0.98616171 0.98318177 0.80878657 0.83700907 0.61857241\n",
      " 0.95191121 0.91994071 0.63294476 0.47087485 0.45819354 0.30328116\n",
      " 0.66939634 0.86549318 0.78639823 0.76026273 0.45609543 0.68648982\n",
      " 0.38663441 0.63438869 0.8231017  0.55045569 0.83199698 0.85815346\n",
      " 0.50296319 0.64867622 0.95192236 0.69767791 0.51598847 0.60232919\n",
      " 0.89853716 0.89843035 0.69238967 0.50157541 0.36568388 0.3272281\n",
      " 0.61278689 0.59188771 0.40722513 0.4411912  0.41382602 0.46100134\n",
      " 0.5861932  0.68052179 0.5922749  0.60974789 0.60456914 0.68998307\n",
      " 0.85481828 0.62947208 0.88317835 0.70084012 0.70293581 0.39367691\n",
      " 0.73145199 0.71534747 0.71968132 0.65068907 0.74029064 0.70488536\n",
      " 0.80113441 0.94980389 0.94286716 0.84065443 0.89838701 0.87662017\n",
      " 0.86986667 0.93039036 0.89176875 0.89228159 0.93523121 0.97861242\n",
      " 0.90766245 0.86356437 0.94878352 0.70168656 0.9871586  0.99288708\n",
      " 0.9401902  0.80002379 0.86286634 0.93000162 0.92404604 0.60815597\n",
      " 0.56497121 0.83165574 0.85368216 0.91681612 0.95901668 0.89684737\n",
      " 0.90565044 0.87288952 0.90968931 0.89992201 0.6297574  0.85148442\n",
      " 0.43253621 0.9006049  0.79362744 0.8924734  0.67268693 0.87129909\n",
      " 0.56861246 0.68654048 0.81022328 0.87119782 0.89837182 0.42351007\n",
      " 0.92835301 0.63500905 0.90576231 0.57314312 0.81730926 0.82555354\n",
      " 0.67125261 0.46268532 0.84800768 0.90509206]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 53 [0/43 (0%)]\tTrain Loss: 0.061296\n",
      "Train Epoch: 53 [10/43 (23%)]\tTrain Loss: 0.057826\n",
      "Train Epoch: 53 [20/43 (47%)]\tTrain Loss: 0.067868\n",
      "Train Epoch: 53 [30/43 (70%)]\tTrain Loss: 0.044762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 53 [40/43 (93%)]\tTrain Loss: 0.054409\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.60554016 0.96218956 0.96829671 0.43640071 0.20798358 0.41560224\n",
      " 0.86846393 0.74683839 0.27851477 0.1695174  0.14774761 0.17166123\n",
      " 0.27316388 0.7681672  0.64545888 0.13747914 0.25303578 0.68181211\n",
      " 0.72043431 0.29346365 0.4545005  0.37516323 0.67313194 0.93075532\n",
      " 0.34808594 0.52004427 0.81383348 0.50302267 0.24191587 0.35847709\n",
      " 0.55242938 0.7356708  0.48523441 0.03578138 0.0649426  0.04473973\n",
      " 0.11105534 0.13485126 0.33239648 0.39976695 0.38209721 0.29006714\n",
      " 0.2603294  0.32101467 0.09843868 0.25547901 0.27646774 0.35595092\n",
      " 0.88785887 0.23924465 0.30241743 0.11918435 0.19037279 0.09882236\n",
      " 0.71328884 0.18284735 0.60165375 0.44754848 0.12511063 0.1947221\n",
      " 0.81113654 0.95037669 0.85282642 0.82087058 0.54272515 0.84441817\n",
      " 0.57294357 0.84992683 0.92742479 0.70095247 0.94799966 0.88016808\n",
      " 0.8678326  0.74275774 0.83687329 0.80633748 0.93504149 0.94193667\n",
      " 0.86277628 0.86116141 0.7361387  0.82928044 0.87467545 0.38147074\n",
      " 0.61373651 0.7545628  0.93928289 0.77584118 0.7581504  0.76142961\n",
      " 0.81447226 0.28571668 0.64101011 0.82291144 0.40308011 0.48876956\n",
      " 0.23056538 0.41844994 0.51659524 0.80355555 0.35780385 0.60391444\n",
      " 0.58782899 0.56092846 0.42177173 0.53199017 0.90359092 0.09720702\n",
      " 0.39831281 0.37255442 0.53584957 0.49257275 0.4367933  0.57641876\n",
      " 0.62991488 0.79408127 0.74305898 0.64915299]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 54 [0/43 (0%)]\tTrain Loss: 0.051655\n",
      "Train Epoch: 54 [10/43 (23%)]\tTrain Loss: 0.063371\n",
      "Train Epoch: 54 [20/43 (47%)]\tTrain Loss: 0.076709\n",
      "Train Epoch: 54 [30/43 (70%)]\tTrain Loss: 0.100043\n",
      "Train Epoch: 54 [40/43 (93%)]\tTrain Loss: 0.082847\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.88125378 0.87947655 0.9455539  0.73332655 0.57164782 0.81325293\n",
      " 0.72134465 0.80333376 0.52222174 0.13131434 0.10925731 0.07152455\n",
      " 0.3918266  0.88560677 0.91397184 0.44106141 0.3595385  0.40481409\n",
      " 0.38014939 0.37333384 0.45738915 0.13218269 0.70237833 0.74236614\n",
      " 0.27903214 0.61783201 0.73079151 0.49195826 0.25547588 0.44740272\n",
      " 0.7777968  0.45810407 0.76259476 0.15775369 0.18663442 0.21668762\n",
      " 0.47709975 0.47803548 0.38433853 0.22366722 0.29361624 0.27046502\n",
      " 0.44330108 0.44230357 0.49475899 0.12293969 0.36351255 0.74303472\n",
      " 0.83881533 0.10008758 0.84417301 0.20219415 0.10377786 0.3314099\n",
      " 0.58739293 0.11126918 0.76937002 0.13061889 0.42110291 0.21465257\n",
      " 0.77514839 0.76859337 0.84157258 0.75749069 0.85440248 0.79029787\n",
      " 0.65047163 0.9483487  0.83697659 0.75946379 0.73790604 0.7819531\n",
      " 0.64115453 0.71864212 0.57950926 0.61986738 0.91052729 0.92951119\n",
      " 0.82569623 0.68945789 0.57617182 0.81973785 0.79103911 0.46060842\n",
      " 0.43191364 0.42961904 0.77765846 0.91958088 0.80093962 0.73425555\n",
      " 0.84534389 0.6192944  0.69336152 0.74455589 0.39064115 0.48834854\n",
      " 0.16710366 0.2890237  0.44163629 0.89011866 0.31420746 0.32116354\n",
      " 0.26107696 0.39638504 0.55497766 0.79641068 0.82518977 0.42814812\n",
      " 0.60382533 0.20448305 0.31540829 0.62681788 0.55512595 0.52012843\n",
      " 0.79687136 0.79891461 0.75186747 0.75607264]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 55 [0/43 (0%)]\tTrain Loss: 0.055988\n",
      "Train Epoch: 55 [10/43 (23%)]\tTrain Loss: 0.045099\n",
      "Train Epoch: 55 [20/43 (47%)]\tTrain Loss: 0.073699\n",
      "Train Epoch: 55 [30/43 (70%)]\tTrain Loss: 0.056659\n",
      "Train Epoch: 55 [40/43 (93%)]\tTrain Loss: 0.061380\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.81342137 0.95123643 0.97547442 0.84388494 0.78115022 0.83992404\n",
      " 0.94237685 0.95708472 0.85479259 0.26436308 0.61928141 0.30795604\n",
      " 0.46563035 0.97091818 0.97889608 0.55624115 0.8403405  0.78770339\n",
      " 0.82636422 0.91276234 0.84790266 0.75748616 0.94375461 0.92007339\n",
      " 0.65302247 0.90815717 0.8541981  0.86044168 0.79721808 0.80650473\n",
      " 0.89854813 0.89721668 0.89118344 0.16935024 0.4463219  0.33476156\n",
      " 0.51578546 0.72521108 0.60422534 0.72168916 0.56279039 0.39244425\n",
      " 0.77502167 0.69717842 0.80670202 0.42263097 0.42145643 0.37110913\n",
      " 0.95260251 0.4451116  0.61429811 0.60445559 0.35754707 0.52168173\n",
      " 0.73764187 0.2968334  0.81681573 0.54685551 0.3179431  0.47754064\n",
      " 0.9030031  0.92533982 0.89109075 0.95167989 0.89233297 0.9274413\n",
      " 0.85672486 0.97423154 0.92389572 0.93856198 0.95937717 0.86015224\n",
      " 0.92271155 0.91963106 0.84572506 0.89767218 0.98887026 0.98611152\n",
      " 0.98156238 0.90692848 0.84700626 0.92858404 0.90041476 0.65920049\n",
      " 0.79501653 0.82367259 0.97766179 0.94318223 0.98747033 0.95436871\n",
      " 0.96425956 0.70405948 0.90599865 0.89688677 0.72467417 0.77431244\n",
      " 0.25516036 0.83547729 0.73895299 0.8239789  0.60423547 0.94152266\n",
      " 0.65364605 0.83522797 0.76615512 0.88412064 0.92351288 0.49314395\n",
      " 0.86198086 0.78980106 0.66364366 0.71578544 0.64730972 0.71021295\n",
      " 0.84735405 0.89938146 0.83923268 0.97786558]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 56 [0/43 (0%)]\tTrain Loss: 0.118863\n",
      "Train Epoch: 56 [10/43 (23%)]\tTrain Loss: 0.031455\n",
      "Train Epoch: 56 [20/43 (47%)]\tTrain Loss: 0.091189\n",
      "Train Epoch: 56 [30/43 (70%)]\tTrain Loss: 0.070710\n",
      "Train Epoch: 56 [40/43 (93%)]\tTrain Loss: 0.080407\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.75084913 0.86235553 0.86304575 0.70183146 0.65512508 0.53408039\n",
      " 0.87324536 0.74455881 0.74397767 0.09110239 0.03478806 0.07159314\n",
      " 0.04520264 0.47827518 0.6538173  0.21711269 0.38681999 0.62618899\n",
      " 0.56725264 0.40462053 0.49619785 0.41203088 0.6007365  0.69332582\n",
      " 0.13710433 0.6235494  0.86421621 0.32312828 0.54891449 0.51952082\n",
      " 0.71635538 0.53581315 0.5191406  0.04571226 0.10060222 0.14719632\n",
      " 0.28601313 0.42793548 0.10655469 0.27635592 0.36196586 0.20891115\n",
      " 0.19510339 0.58086812 0.66701019 0.08315422 0.05914587 0.27889875\n",
      " 0.41370463 0.10755838 0.03039625 0.08005015 0.09676754 0.06883665\n",
      " 0.24419874 0.05095863 0.43378028 0.07645702 0.15810457 0.07052698\n",
      " 0.83395654 0.69481951 0.77781838 0.74124557 0.90921068 0.46747476\n",
      " 0.32826409 0.73495531 0.84316653 0.61649501 0.64774734 0.8428297\n",
      " 0.51531029 0.66757661 0.48170277 0.59808916 0.97221541 0.95994908\n",
      " 0.90853173 0.58455759 0.64613134 0.71309084 0.8344537  0.46712926\n",
      " 0.39624652 0.34865507 0.60023284 0.82927293 0.78227216 0.55020213\n",
      " 0.85453749 0.40843797 0.62968934 0.66734481 0.18714884 0.60214043\n",
      " 0.14049141 0.20288295 0.48673674 0.69126326 0.29007825 0.53079122\n",
      " 0.71913797 0.14946693 0.45131394 0.51231921 0.42602614 0.06284062\n",
      " 0.47704276 0.17853732 0.19243754 0.30702633 0.51677591 0.19236997\n",
      " 0.5792135  0.81365019 0.77440256 0.76168948]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 57 [0/43 (0%)]\tTrain Loss: 0.060895\n",
      "Train Epoch: 57 [10/43 (23%)]\tTrain Loss: 0.080998\n",
      "Train Epoch: 57 [20/43 (47%)]\tTrain Loss: 0.063685\n",
      "Train Epoch: 57 [30/43 (70%)]\tTrain Loss: 0.045174\n",
      "Train Epoch: 57 [40/43 (93%)]\tTrain Loss: 0.085801\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.60290641 0.93367606 0.97836065 0.94075733 0.88986301 0.82466507\n",
      " 0.85781384 0.92757726 0.75238848 0.88015568 0.94971842 0.91395611\n",
      " 0.9452433  0.80343324 0.81371969 0.67075402 0.51652586 0.62551719\n",
      " 0.32365686 0.48020506 0.30617818 0.32234642 0.84140193 0.95573038\n",
      " 0.7112239  0.60406941 0.93861115 0.80280823 0.56996846 0.39071268\n",
      " 0.84687418 0.78035188 0.62241459 0.4838106  0.14007452 0.30161846\n",
      " 0.69162136 0.47738922 0.68899423 0.60640723 0.60023326 0.28826416\n",
      " 0.62416285 0.60739344 0.69301265 0.94579619 0.95695662 0.48484856\n",
      " 0.7996546  0.94700873 0.91681892 0.95112979 0.94566888 0.98726839\n",
      " 0.60439533 0.95479614 0.39861423 0.95692211 0.85905999 0.95832449\n",
      " 0.70664603 0.94727713 0.87067175 0.92439383 0.79239434 0.87592626\n",
      " 0.83142561 0.97957224 0.92895329 0.86171174 0.96518534 0.98537701\n",
      " 0.92390168 0.69309872 0.93445688 0.78572768 0.96251225 0.99087256\n",
      " 0.93740451 0.7964282  0.88059503 0.97199911 0.93986982 0.37837145\n",
      " 0.63720918 0.58638173 0.91304326 0.96983069 0.94413972 0.94403756\n",
      " 0.9921481  0.85571092 0.94059098 0.96995354 0.82730323 0.70281285\n",
      " 0.94038039 0.83932912 0.47295558 0.9355337  0.46113527 0.88779366\n",
      " 0.78694791 0.59414059 0.79079372 0.91147155 0.879273   0.71379411\n",
      " 0.96377432 0.65319413 0.94959629 0.78788054 0.89585012 0.87414688\n",
      " 0.89282179 0.55378097 0.84415388 0.87422723]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 58 [0/43 (0%)]\tTrain Loss: 0.082042\n",
      "Train Epoch: 58 [10/43 (23%)]\tTrain Loss: 0.076173\n",
      "Train Epoch: 58 [20/43 (47%)]\tTrain Loss: 0.052406\n",
      "Train Epoch: 58 [30/43 (70%)]\tTrain Loss: 0.068758\n",
      "Train Epoch: 58 [40/43 (93%)]\tTrain Loss: 0.044790\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.84006375 0.98508781 0.9356901  0.81677347 0.86150479 0.63402009\n",
      " 0.95415878 0.87380534 0.91872329 0.01261396 0.01704802 0.04634644\n",
      " 0.0289335  0.77789581 0.8977986  0.56328237 0.86462367 0.71119213\n",
      " 0.7221576  0.6991868  0.43536225 0.80928981 0.66273761 0.91698301\n",
      " 0.68418247 0.84151077 0.89219314 0.87540656 0.57007134 0.76082927\n",
      " 0.92684901 0.91178155 0.70674354 0.11715907 0.25470245 0.19342895\n",
      " 0.61851943 0.43843168 0.36416912 0.42212003 0.52509683 0.41975808\n",
      " 0.4604111  0.48523176 0.86728376 0.01483629 0.14071456 0.83124042\n",
      " 0.81261939 0.02857113 0.93078351 0.29406509 0.05821225 0.6630156\n",
      " 0.65981352 0.06297021 0.58232641 0.03959768 0.12202203 0.25208905\n",
      " 0.88567662 0.82330465 0.92630225 0.97144824 0.93902993 0.87245268\n",
      " 0.84845585 0.7856304  0.93824893 0.80528641 0.9331724  0.96827912\n",
      " 0.9037748  0.88140839 0.76746565 0.89269525 0.9725495  0.97130513\n",
      " 0.9889552  0.93300211 0.92671055 0.95974785 0.93984467 0.49471468\n",
      " 0.51495993 0.86870795 0.70661581 0.88753313 0.96261638 0.9425683\n",
      " 0.94620734 0.82343554 0.8317548  0.87778157 0.79036444 0.8861897\n",
      " 0.04146639 0.69912124 0.81351036 0.96700615 0.47751722 0.92691702\n",
      " 0.63580292 0.72299725 0.8107428  0.81956118 0.91478705 0.40507182\n",
      " 0.91727239 0.43312177 0.84368956 0.67776704 0.84764117 0.78591305\n",
      " 0.89491093 0.76449096 0.93068486 0.96424037]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 59 [0/43 (0%)]\tTrain Loss: 0.085944\n",
      "Train Epoch: 59 [10/43 (23%)]\tTrain Loss: 0.059602\n",
      "Train Epoch: 59 [20/43 (47%)]\tTrain Loss: 0.053766\n",
      "Train Epoch: 59 [30/43 (70%)]\tTrain Loss: 0.062119\n",
      "Train Epoch: 59 [40/43 (93%)]\tTrain Loss: 0.058795\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.68698251 0.94296306 0.90229267 0.84427428 0.80066353 0.72311527\n",
      " 0.89460677 0.80537564 0.71225387 0.01871633 0.01736813 0.01273054\n",
      " 0.03306739 0.91518182 0.88549411 0.41827416 0.7293998  0.28761822\n",
      " 0.42316213 0.46182191 0.80940819 0.50581604 0.72668868 0.85115331\n",
      " 0.40754196 0.69035339 0.8466599  0.77223772 0.54237574 0.6074599\n",
      " 0.84635365 0.60959017 0.55528367 0.30998966 0.71109498 0.2633405\n",
      " 0.59168959 0.7781713  0.35240152 0.34804741 0.37937364 0.14050534\n",
      " 0.75338888 0.87759429 0.29060954 0.02309464 0.01125121 0.01428529\n",
      " 0.61164701 0.02785717 0.01056277 0.01071578 0.01496383 0.01783144\n",
      " 0.45963231 0.01673857 0.6474793  0.01373379 0.00994915 0.00609653\n",
      " 0.91764003 0.24365698 0.80048966 0.92728984 0.85906672 0.82566696\n",
      " 0.58914119 0.89948589 0.935974   0.5717386  0.76247519 0.67954081\n",
      " 0.62140727 0.83659762 0.7561658  0.79708648 0.92927015 0.94390261\n",
      " 0.92533183 0.83764052 0.85745227 0.90522617 0.92136925 0.45353818\n",
      " 0.57377732 0.66530973 0.70684421 0.75593334 0.57418835 0.82033086\n",
      " 0.70348436 0.85826737 0.90905052 0.8896544  0.82049531 0.83079445\n",
      " 0.01018442 0.69877142 0.42487934 0.840653   0.18057218 0.85048598\n",
      " 0.69612575 0.28108457 0.34572083 0.44401726 0.5923295  0.49796289\n",
      " 0.85780752 0.37610233 0.87086147 0.47481462 0.81393796 0.72222441\n",
      " 0.87114036 0.59815711 0.83730257 0.69113868]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 60 [0/43 (0%)]\tTrain Loss: 0.082716\n",
      "Train Epoch: 60 [10/43 (23%)]\tTrain Loss: 0.055144\n",
      "Train Epoch: 60 [20/43 (47%)]\tTrain Loss: 0.051591\n",
      "Train Epoch: 60 [30/43 (70%)]\tTrain Loss: 0.065204\n",
      "Train Epoch: 60 [40/43 (93%)]\tTrain Loss: 0.072472\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.28058538 0.8935523  0.91771281 0.36582199 0.4868291  0.39924371\n",
      " 0.77888    0.57953387 0.47213835 0.09789678 0.18869327 0.09498829\n",
      " 0.06130464 0.65597659 0.55752969 0.13700129 0.51967221 0.40999731\n",
      " 0.30282089 0.48663449 0.5835591  0.58736163 0.7200976  0.76224554\n",
      " 0.17840855 0.64287663 0.75370151 0.62444741 0.46967655 0.55487585\n",
      " 0.79620993 0.76781857 0.66510385 0.10052932 0.06061752 0.09558289\n",
      " 0.16778702 0.40483615 0.3595137  0.30711317 0.32987577 0.32207182\n",
      " 0.45417109 0.40966624 0.37709326 0.10064142 0.25119805 0.21531931\n",
      " 0.56974924 0.14936493 0.57237756 0.13247716 0.19361567 0.15692961\n",
      " 0.57775724 0.11574311 0.55911022 0.10715045 0.09758047 0.02749426\n",
      " 0.71702421 0.5568012  0.74407601 0.72563964 0.74653989 0.74223262\n",
      " 0.49371326 0.92843139 0.893987   0.54753351 0.55792814 0.61702889\n",
      " 0.52054954 0.76285738 0.68559521 0.58224958 0.96306169 0.96357971\n",
      " 0.92438924 0.71363777 0.82692415 0.86328    0.73660356 0.27394021\n",
      " 0.4706789  0.51383883 0.79131788 0.57795662 0.78179544 0.83170623\n",
      " 0.72452515 0.38593343 0.50945872 0.78640604 0.56010538 0.64314538\n",
      " 0.16463549 0.52280283 0.58751857 0.90351713 0.33990917 0.74422157\n",
      " 0.29937524 0.45800799 0.45331305 0.69177681 0.47754791 0.5324735\n",
      " 0.64859343 0.40322906 0.72947854 0.2811448  0.74396384 0.56930059\n",
      " 0.70369732 0.57507014 0.80924457 0.81605721]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 51 TN= 32 FN= 7 FP= 28\n",
      "TP+FP 79\n",
      "precision 0.6455696202531646\n",
      "recall 0.8793103448275862\n",
      "F1 0.7445255474452555\n",
      "acc 0.7033898305084746\n",
      "AUCp 0.7063218390804598\n",
      "AUC 0.7804597701149425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 60, average recall: 0.8793, average precision: 0.6456,average F1: 0.7445, average accuracy: 0.7034, average AUC: 0.7805\n",
      "Train Epoch: 61 [0/43 (0%)]\tTrain Loss: 0.049630\n",
      "Train Epoch: 61 [10/43 (23%)]\tTrain Loss: 0.084878\n",
      "Train Epoch: 61 [20/43 (47%)]\tTrain Loss: 0.116439\n",
      "Train Epoch: 61 [30/43 (70%)]\tTrain Loss: 0.059173\n",
      "Train Epoch: 61 [40/43 (93%)]\tTrain Loss: 0.072116\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.53363866 0.86044323 0.96980661 0.49832529 0.8019734  0.72177368\n",
      " 0.85541356 0.89570403 0.70861387 0.63409156 0.42125487 0.51311243\n",
      " 0.59635437 0.90981174 0.94679379 0.59156466 0.66495293 0.62159514\n",
      " 0.51543099 0.34612843 0.25350401 0.57693809 0.61553967 0.674299\n",
      " 0.17774987 0.57130772 0.91827494 0.65278709 0.46345666 0.67920345\n",
      " 0.66796851 0.84508979 0.61800915 0.12672499 0.42375949 0.37398002\n",
      " 0.40938699 0.58108598 0.38777989 0.70367277 0.43282533 0.31590137\n",
      " 0.55664921 0.48770866 0.57201809 0.51679534 0.42758507 0.56150424\n",
      " 0.60061711 0.33735991 0.67392695 0.652583   0.70739394 0.41364521\n",
      " 0.40426514 0.67278379 0.3850888  0.54525167 0.66379684 0.45482969\n",
      " 0.64069152 0.82838118 0.89400679 0.94848651 0.73176426 0.73461217\n",
      " 0.69249499 0.97716856 0.92716449 0.80282605 0.91765147 0.86054647\n",
      " 0.68636596 0.69120026 0.7528742  0.69246852 0.98354727 0.985084\n",
      " 0.96032345 0.72586673 0.95158887 0.92128587 0.95323443 0.83733261\n",
      " 0.57852626 0.47640961 0.90360129 0.96078706 0.92352152 0.96035093\n",
      " 0.97577763 0.72023052 0.82845426 0.93791306 0.81678766 0.76985234\n",
      " 0.78263152 0.77897549 0.55914265 0.91715223 0.76950502 0.60300642\n",
      " 0.52839029 0.35823843 0.66845965 0.69222462 0.63407892 0.30964917\n",
      " 0.91074479 0.36022788 0.7594614  0.74010479 0.84837765 0.43323123\n",
      " 0.85267794 0.78663212 0.81117082 0.94819498]\n",
      "predict [1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 62 [0/43 (0%)]\tTrain Loss: 0.042978\n",
      "Train Epoch: 62 [10/43 (23%)]\tTrain Loss: 0.065119\n",
      "Train Epoch: 62 [20/43 (47%)]\tTrain Loss: 0.074379\n",
      "Train Epoch: 62 [30/43 (70%)]\tTrain Loss: 0.057539\n",
      "Train Epoch: 62 [40/43 (93%)]\tTrain Loss: 0.051693\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.34358823 0.95151329 0.9463138  0.39865091 0.40412834 0.36537474\n",
      " 0.8741715  0.5086329  0.31697863 0.78435761 0.68024915 0.84362376\n",
      " 0.71628267 0.66717428 0.50988716 0.27438715 0.32267678 0.44482788\n",
      " 0.49489054 0.56384534 0.52177131 0.43562397 0.76903629 0.74191952\n",
      " 0.57623094 0.74016947 0.83158213 0.7176106  0.47278905 0.54128671\n",
      " 0.71651697 0.77969474 0.59985739 0.24496751 0.15382455 0.08720474\n",
      " 0.20225535 0.31010175 0.3625409  0.4049947  0.56387752 0.4707489\n",
      " 0.16390215 0.32400981 0.28615838 0.8606981  0.89601326 0.8492685\n",
      " 0.46363106 0.74815178 0.82100266 0.76660997 0.845375   0.79302102\n",
      " 0.43126482 0.8854773  0.69065762 0.89960521 0.80675626 0.91759372\n",
      " 0.75427783 0.7427091  0.72193265 0.84695452 0.515172   0.5276233\n",
      " 0.48478484 0.87961322 0.78851366 0.66232145 0.83069038 0.88865894\n",
      " 0.8520059  0.68457454 0.46054041 0.79108876 0.98429465 0.98630893\n",
      " 0.94909376 0.59773499 0.80999649 0.95275539 0.92755121 0.25624934\n",
      " 0.62313151 0.38212836 0.92382741 0.81181103 0.5124355  0.83428192\n",
      " 0.91196859 0.1678455  0.54232907 0.93525094 0.71981269 0.76020986\n",
      " 0.80463815 0.56928742 0.47385755 0.85251546 0.24033847 0.4487057\n",
      " 0.58349949 0.44994852 0.54335189 0.8018831  0.82849157 0.43020472\n",
      " 0.78217947 0.26813349 0.58823311 0.58028924 0.84902412 0.47547996\n",
      " 0.46543115 0.71463531 0.82712007 0.6876359 ]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "Train Epoch: 63 [0/43 (0%)]\tTrain Loss: 0.046444\n",
      "Train Epoch: 63 [10/43 (23%)]\tTrain Loss: 0.069529\n",
      "Train Epoch: 63 [20/43 (47%)]\tTrain Loss: 0.046553\n",
      "Train Epoch: 63 [30/43 (70%)]\tTrain Loss: 0.061098\n",
      "Train Epoch: 63 [40/43 (93%)]\tTrain Loss: 0.035247\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.68178582 0.93483436 0.96579295 0.7626344  0.81614929 0.80880731\n",
      " 0.8906455  0.7991094  0.70587838 0.52958173 0.66234505 0.49870929\n",
      " 0.47861832 0.94264358 0.9820857  0.14610113 0.45526677 0.5321849\n",
      " 0.60016185 0.70425844 0.55705619 0.70850569 0.96365088 0.91477084\n",
      " 0.51195472 0.82729816 0.93199992 0.76859027 0.66615123 0.71328914\n",
      " 0.87367135 0.76109529 0.34697887 0.20145185 0.06226689 0.19233531\n",
      " 0.26579827 0.42579588 0.7715801  0.45174286 0.3208597  0.34949052\n",
      " 0.4663156  0.62780094 0.50738037 0.42377749 0.37166202 0.39406136\n",
      " 0.85219431 0.12388217 0.41379315 0.13476034 0.45248902 0.47967044\n",
      " 0.5903827  0.76940417 0.49617618 0.6934799  0.13623063 0.13809115\n",
      " 0.97433215 0.726363   0.97405994 0.97720683 0.6747666  0.49814254\n",
      " 0.40381402 0.96671093 0.90552586 0.83895677 0.83261836 0.91600782\n",
      " 0.54452085 0.72356862 0.28524944 0.93770653 0.98203206 0.98688382\n",
      " 0.98692429 0.81850749 0.93723214 0.94126898 0.96826953 0.78808236\n",
      " 0.59386086 0.68407822 0.91789669 0.92815208 0.98294026 0.98011231\n",
      " 0.98988348 0.74082106 0.77470273 0.90444911 0.70725119 0.8947069\n",
      " 0.37600291 0.92517543 0.8789826  0.94984376 0.58618754 0.87995434\n",
      " 0.61173677 0.46104804 0.67315316 0.82328701 0.8995043  0.44643554\n",
      " 0.75299567 0.68702543 0.56871885 0.38152987 0.90600091 0.35256416\n",
      " 0.95102489 0.94826627 0.88834262 0.92678171]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 64 [0/43 (0%)]\tTrain Loss: 0.055544\n",
      "Train Epoch: 64 [10/43 (23%)]\tTrain Loss: 0.063125\n",
      "Train Epoch: 64 [20/43 (47%)]\tTrain Loss: 0.064614\n",
      "Train Epoch: 64 [30/43 (70%)]\tTrain Loss: 0.049977\n",
      "Train Epoch: 64 [40/43 (93%)]\tTrain Loss: 0.076850\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.49303338 0.97283554 0.95957112 0.93292898 0.82749432 0.88539541\n",
      " 0.91413587 0.95804322 0.84957808 0.88567555 0.86869258 0.83703834\n",
      " 0.8052718  0.97503984 0.97696745 0.43723848 0.73794556 0.54377657\n",
      " 0.76053387 0.33689699 0.76692927 0.58734131 0.93729168 0.88716567\n",
      " 0.50391465 0.90475833 0.79868776 0.70738447 0.66768748 0.65097004\n",
      " 0.78946948 0.80451733 0.5686667  0.8790009  0.49076483 0.40701863\n",
      " 0.55956668 0.72576892 0.74767214 0.71766341 0.67458963 0.38378128\n",
      " 0.75498098 0.73913801 0.97838247 0.78161615 0.93785262 0.94609392\n",
      " 0.63300425 0.90065432 0.94194913 0.90386367 0.8677398  0.91467702\n",
      " 0.62241626 0.90481025 0.63459581 0.90081722 0.74257475 0.89523757\n",
      " 0.92467386 0.97934681 0.94928676 0.97544962 0.92610991 0.63827401\n",
      " 0.55575395 0.99529344 0.99214786 0.70336324 0.93451536 0.98098952\n",
      " 0.65534216 0.79754996 0.82882106 0.9150306  0.98109668 0.96226859\n",
      " 0.99203449 0.9068985  0.79333556 0.97488379 0.9396013  0.81739378\n",
      " 0.54024476 0.701011   0.93790793 0.96273232 0.9787764  0.97494161\n",
      " 0.9818123  0.93819481 0.91193151 0.97420532 0.75865972 0.95029551\n",
      " 0.91382617 0.79871064 0.8256318  0.97365957 0.88098234 0.79668272\n",
      " 0.77309996 0.34814936 0.61077327 0.90192211 0.89141983 0.66284144\n",
      " 0.79692525 0.40181726 0.6253702  0.56045067 0.89264917 0.76968646\n",
      " 0.9437151  0.87277573 0.9460122  0.95725155]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 65 [0/43 (0%)]\tTrain Loss: 0.073361\n",
      "Train Epoch: 65 [10/43 (23%)]\tTrain Loss: 0.057009\n",
      "Train Epoch: 65 [20/43 (47%)]\tTrain Loss: 0.055845\n",
      "Train Epoch: 65 [30/43 (70%)]\tTrain Loss: 0.059295\n",
      "Train Epoch: 65 [40/43 (93%)]\tTrain Loss: 0.100738\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.43100944 0.96936774 0.92148387 0.17156301 0.49184224 0.36028346\n",
      " 0.88196969 0.32981688 0.48283666 0.5636484  0.60633254 0.3024599\n",
      " 0.47273216 0.70610702 0.74485517 0.2014349  0.17566909 0.60402089\n",
      " 0.62169099 0.44825706 0.36711422 0.42117971 0.78559524 0.6612308\n",
      " 0.23724596 0.67657644 0.93420815 0.39183515 0.3596743  0.38774687\n",
      " 0.79317731 0.5099293  0.31274882 0.13041173 0.21263386 0.19350046\n",
      " 0.06704962 0.34384796 0.50435996 0.37578738 0.26258105 0.24376932\n",
      " 0.3156437  0.19983682 0.24419081 0.49680749 0.61050129 0.65669328\n",
      " 0.61575979 0.49392405 0.32922584 0.72748429 0.5439952  0.56259847\n",
      " 0.2721253  0.24542929 0.48775911 0.47181553 0.63068938 0.7617783\n",
      " 0.78123844 0.89972502 0.91321373 0.94706488 0.6445303  0.13037519\n",
      " 0.3945103  0.94812167 0.63788056 0.5800041  0.92992252 0.92181635\n",
      " 0.69927502 0.83888292 0.2770068  0.39518884 0.98463792 0.9801293\n",
      " 0.97404093 0.70076168 0.78797317 0.94428056 0.94408011 0.40379852\n",
      " 0.40897089 0.73079407 0.91642237 0.74088472 0.79077035 0.96352404\n",
      " 0.93294621 0.11121425 0.35803741 0.92054456 0.37635234 0.59978831\n",
      " 0.47042376 0.92424238 0.63141072 0.87776583 0.45382872 0.75872916\n",
      " 0.57511711 0.23037827 0.2526885  0.93332404 0.7531684  0.27754864\n",
      " 0.88042784 0.25903565 0.54236007 0.47894517 0.79398394 0.73514855\n",
      " 0.84702033 0.54483634 0.75488627 0.90764791]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 66 [0/43 (0%)]\tTrain Loss: 0.076325\n",
      "Train Epoch: 66 [10/43 (23%)]\tTrain Loss: 0.064941\n",
      "Train Epoch: 66 [20/43 (47%)]\tTrain Loss: 0.042130\n",
      "Train Epoch: 66 [30/43 (70%)]\tTrain Loss: 0.032310\n",
      "Train Epoch: 66 [40/43 (93%)]\tTrain Loss: 0.054869\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.58301878 0.92498606 0.93363929 0.68020409 0.62567711 0.82714063\n",
      " 0.86499816 0.63578588 0.67475206 0.69185138 0.44027522 0.52143633\n",
      " 0.60082036 0.96735752 0.80627966 0.57694256 0.69860286 0.91182393\n",
      " 0.69369107 0.73270285 0.81542498 0.7447843  0.88259852 0.94975787\n",
      " 0.65465581 0.98613375 0.96788132 0.8853761  0.73028719 0.55762684\n",
      " 0.92167336 0.80996138 0.73308319 0.40572006 0.33891493 0.23585702\n",
      " 0.29843748 0.67997086 0.7300033  0.50199062 0.77579558 0.69095868\n",
      " 0.62100667 0.45448068 0.55552304 0.68747652 0.89450109 0.6258527\n",
      " 0.77552426 0.6621148  0.87366807 0.42221037 0.57650995 0.39167824\n",
      " 0.76688635 0.57167822 0.88144451 0.63746107 0.39685488 0.56847352\n",
      " 0.92745954 0.95376658 0.95271498 0.88591045 0.91522157 0.83518863\n",
      " 0.80320269 0.9898178  0.98428506 0.92645752 0.96242577 0.96765733\n",
      " 0.88253027 0.86848038 0.87523878 0.87775481 0.98132139 0.98123473\n",
      " 0.98199248 0.94667518 0.9243567  0.92922843 0.95835078 0.49321631\n",
      " 0.74640244 0.93022197 0.94675702 0.93653423 0.97916871 0.99373442\n",
      " 0.97139525 0.64153916 0.58667088 0.97389936 0.89866894 0.97043419\n",
      " 0.85078055 0.51030803 0.56910837 0.98315746 0.62322813 0.91208696\n",
      " 0.54668874 0.54078901 0.70921516 0.84528589 0.8994289  0.4710018\n",
      " 0.96173406 0.63355559 0.93599522 0.65545434 0.76887071 0.86107606\n",
      " 0.90289873 0.75462174 0.91980612 0.82589865]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 67 [0/43 (0%)]\tTrain Loss: 0.057646\n",
      "Train Epoch: 67 [10/43 (23%)]\tTrain Loss: 0.030794\n",
      "Train Epoch: 67 [20/43 (47%)]\tTrain Loss: 0.090299\n",
      "Train Epoch: 67 [30/43 (70%)]\tTrain Loss: 0.051519\n",
      "Train Epoch: 67 [40/43 (93%)]\tTrain Loss: 0.062674\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.16013697 0.90674138 0.93356055 0.22023374 0.17142838 0.24250878\n",
      " 0.75619173 0.18974127 0.07571473 0.07475438 0.18004206 0.11657798\n",
      " 0.18083514 0.69318372 0.41522548 0.06199428 0.11701386 0.45123076\n",
      " 0.29776162 0.59046817 0.18505174 0.43871942 0.80804497 0.7197051\n",
      " 0.30205533 0.78128392 0.92795569 0.51560318 0.142719   0.09882031\n",
      " 0.74488306 0.18448766 0.20412078 0.04573584 0.0510949  0.07426506\n",
      " 0.08335096 0.19488807 0.22467197 0.14006257 0.23323667 0.27253374\n",
      " 0.12978365 0.10135631 0.14911123 0.09194551 0.1608474  0.31631768\n",
      " 0.7835989  0.12236772 0.58153641 0.22217502 0.15549223 0.37214392\n",
      " 0.45211992 0.21823344 0.52115595 0.12786913 0.13312893 0.13309091\n",
      " 0.93729818 0.94772297 0.95360851 0.9799279  0.49598616 0.22494809\n",
      " 0.12764412 0.97810936 0.45754683 0.81486744 0.92449021 0.97554761\n",
      " 0.35635883 0.79406357 0.26568639 0.79888391 0.98838019 0.99065512\n",
      " 0.98390239 0.81633478 0.89553368 0.9663884  0.93639058 0.21120641\n",
      " 0.40390599 0.89018673 0.91724306 0.88805431 0.57077849 0.85615391\n",
      " 0.9933036  0.26341638 0.2952593  0.88617003 0.33446264 0.63292772\n",
      " 0.32850525 0.41814619 0.46065423 0.92526621 0.43240833 0.82807076\n",
      " 0.43074706 0.13635744 0.32696396 0.838337   0.85987407 0.22225177\n",
      " 0.6801579  0.37941989 0.62289566 0.6696806  0.84650934 0.40681568\n",
      " 0.96691906 0.93145496 0.84040958 0.93283504]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 68 [0/43 (0%)]\tTrain Loss: 0.029821\n",
      "Train Epoch: 68 [10/43 (23%)]\tTrain Loss: 0.095154\n",
      "Train Epoch: 68 [20/43 (47%)]\tTrain Loss: 0.128321\n",
      "Train Epoch: 68 [30/43 (70%)]\tTrain Loss: 0.049260\n",
      "Train Epoch: 68 [40/43 (93%)]\tTrain Loss: 0.049558\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.28478023 0.61413735 0.72584587 0.40459824 0.35166475 0.27663431\n",
      " 0.66036296 0.41300115 0.18069138 0.01603037 0.05771711 0.01091853\n",
      " 0.01243286 0.15927593 0.16505089 0.0813821  0.06213211 0.41581333\n",
      " 0.2186088  0.53080904 0.29549098 0.51941687 0.63046283 0.85484844\n",
      " 0.57490462 0.59778893 0.82650733 0.74668711 0.30605069 0.28026223\n",
      " 0.8880586  0.34696883 0.42722341 0.00968441 0.02114016 0.05182125\n",
      " 0.11068369 0.52393883 0.52088481 0.43654236 0.49140498 0.28177124\n",
      " 0.43460301 0.18233146 0.03056714 0.01812856 0.02033801 0.027046\n",
      " 0.29456127 0.01672971 0.02648526 0.01829451 0.01771071 0.02174492\n",
      " 0.42561907 0.02914307 0.41154879 0.01697475 0.01466247 0.01750774\n",
      " 0.7962501  0.52565968 0.51933217 0.7340557  0.66952342 0.39001411\n",
      " 0.67485952 0.8380689  0.63406146 0.74529326 0.68927443 0.43449765\n",
      " 0.61655098 0.76107776 0.45655626 0.81164688 0.95239097 0.96949762\n",
      " 0.85839486 0.7937327  0.76009077 0.78277957 0.9549067  0.38142824\n",
      " 0.43213439 0.71337461 0.62589324 0.76824933 0.17941017 0.76720959\n",
      " 0.81042856 0.01400284 0.49619886 0.87648213 0.47763926 0.61641562\n",
      " 0.0102718  0.24542093 0.20758866 0.93551922 0.07184353 0.73750991\n",
      " 0.07968167 0.50426054 0.55923116 0.53889102 0.69483614 0.58013028\n",
      " 0.78702819 0.49619439 0.48239368 0.5523234  0.86980128 0.71592897\n",
      " 0.51997137 0.80506974 0.74787307 0.80043626]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 69 [0/43 (0%)]\tTrain Loss: 0.055749\n",
      "Train Epoch: 69 [10/43 (23%)]\tTrain Loss: 0.072227\n",
      "Train Epoch: 69 [20/43 (47%)]\tTrain Loss: 0.061573\n",
      "Train Epoch: 69 [30/43 (70%)]\tTrain Loss: 0.050620\n",
      "Train Epoch: 69 [40/43 (93%)]\tTrain Loss: 0.022934\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.56771195 0.85056621 0.91910994 0.61813033 0.41251627 0.30981684\n",
      " 0.84487438 0.5258708  0.37594324 0.17155518 0.17848992 0.18905224\n",
      " 0.12904949 0.61900628 0.54432917 0.13418336 0.21936359 0.65715772\n",
      " 0.68916821 0.40217453 0.32735667 0.45314169 0.87712532 0.85170996\n",
      " 0.48520839 0.79488814 0.94760352 0.33440635 0.2761108  0.35054848\n",
      " 0.75588381 0.48411387 0.48457667 0.17247935 0.06156422 0.10731227\n",
      " 0.124234   0.39980114 0.28188279 0.19470645 0.2653062  0.20989269\n",
      " 0.18718447 0.11516088 0.27185917 0.18880349 0.26844001 0.16668381\n",
      " 0.15545216 0.19625294 0.09361335 0.16839647 0.15970846 0.17574881\n",
      " 0.47086188 0.21115725 0.71494955 0.20137449 0.10786473 0.21184465\n",
      " 0.89719152 0.80640262 0.94247782 0.89137423 0.36966059 0.2711004\n",
      " 0.49265209 0.91086537 0.76668096 0.66409111 0.7736035  0.83308434\n",
      " 0.53851718 0.56633681 0.27831593 0.85292441 0.9648639  0.97819877\n",
      " 0.95461124 0.88366675 0.84102178 0.87538189 0.94473922 0.33830985\n",
      " 0.54592371 0.73746884 0.73342597 0.89862597 0.52226079 0.89464563\n",
      " 0.93033612 0.2565721  0.49104631 0.94409257 0.62978315 0.525868\n",
      " 0.16468149 0.41649318 0.45069313 0.76990968 0.49136746 0.8517673\n",
      " 0.41236466 0.32153919 0.50146896 0.60933656 0.78009087 0.38419938\n",
      " 0.52560848 0.41127354 0.66342181 0.43479702 0.79396337 0.5530802\n",
      " 0.67127943 0.8746227  0.82758266 0.91616279]\n",
      "predict [1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 70 [0/43 (0%)]\tTrain Loss: 0.044339\n",
      "Train Epoch: 70 [10/43 (23%)]\tTrain Loss: 0.077323\n",
      "Train Epoch: 70 [20/43 (47%)]\tTrain Loss: 0.082283\n",
      "Train Epoch: 70 [30/43 (70%)]\tTrain Loss: 0.032519\n",
      "Train Epoch: 70 [40/43 (93%)]\tTrain Loss: 0.053461\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.1877965  0.94737768 0.96414697 0.50195694 0.6103304  0.53944278\n",
      " 0.92987967 0.37928048 0.27424783 0.54040539 0.74104726 0.67640555\n",
      " 0.51870048 0.99424678 0.9446184  0.16363987 0.27130505 0.6256789\n",
      " 0.56994498 0.38171867 0.3959786  0.61665314 0.90941221 0.90008724\n",
      " 0.30267468 0.83212584 0.91070485 0.53938723 0.18373483 0.53334981\n",
      " 0.88577646 0.57035726 0.62461042 0.74703103 0.10927427 0.0927731\n",
      " 0.26825202 0.26700976 0.65123492 0.60964757 0.59656483 0.35303193\n",
      " 0.37543023 0.17557655 0.17052972 0.73291641 0.65506351 0.66198313\n",
      " 0.67539769 0.80202568 0.70716983 0.54733914 0.78213131 0.83762228\n",
      " 0.19108652 0.73306215 0.47572398 0.70503837 0.70986938 0.70816118\n",
      " 0.78900439 0.99691284 0.94500029 0.93903571 0.63155276 0.75374186\n",
      " 0.55116105 0.99502844 0.93840837 0.67983723 0.96026695 0.95105428\n",
      " 0.37949347 0.86277431 0.61707205 0.80448824 0.96911937 0.98356712\n",
      " 0.94494081 0.88924962 0.84326804 0.93539453 0.95872587 0.88030094\n",
      " 0.43194869 0.79738545 0.99747473 0.98467535 0.9860481  0.9788841\n",
      " 0.98841196 0.7397458  0.47461447 0.94631338 0.37700167 0.82108432\n",
      " 0.73554409 0.82252163 0.92512983 0.97680104 0.73802251 0.85494286\n",
      " 0.86326194 0.23859406 0.23668991 0.88810444 0.7509501  0.09845129\n",
      " 0.91407669 0.43878767 0.64745837 0.49439529 0.83818901 0.64148283\n",
      " 0.97874075 0.9898113  0.78684336 0.92574757]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 35 FN= 11 FP= 25\n",
      "TP+FP 72\n",
      "precision 0.6527777777777778\n",
      "recall 0.8103448275862069\n",
      "F1 0.7230769230769231\n",
      "acc 0.6949152542372882\n",
      "AUCp 0.69683908045977\n",
      "AUC 0.8117816091954022\n",
      "\n",
      " The epoch is 70, average recall: 0.8103, average precision: 0.6528,average F1: 0.7231, average accuracy: 0.6949, average AUC: 0.8118\n",
      "Train Epoch: 71 [0/43 (0%)]\tTrain Loss: 0.067247\n",
      "Train Epoch: 71 [10/43 (23%)]\tTrain Loss: 0.056669\n",
      "Train Epoch: 71 [20/43 (47%)]\tTrain Loss: 0.115962\n",
      "Train Epoch: 71 [30/43 (70%)]\tTrain Loss: 0.064661\n",
      "Train Epoch: 71 [40/43 (93%)]\tTrain Loss: 0.052475\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.28204244 0.9401353  0.71194887 0.47701401 0.46770883 0.10883615\n",
      " 0.86207145 0.37979653 0.16960034 0.19799931 0.30905476 0.3155511\n",
      " 0.09183344 0.94151759 0.93415153 0.10901739 0.53901643 0.85956591\n",
      " 0.8674801  0.44727892 0.53120124 0.77655256 0.96750176 0.87170899\n",
      " 0.56156671 0.95149505 0.86324835 0.48301095 0.45862669 0.2759831\n",
      " 0.91078907 0.52540034 0.64521462 0.05163626 0.06251627 0.08524279\n",
      " 0.11774484 0.64207852 0.66910529 0.37800059 0.57565767 0.15935978\n",
      " 0.58004242 0.22045349 0.33659884 0.22829579 0.19043569 0.29217562\n",
      " 0.4262118  0.25312063 0.28192008 0.37510559 0.21272926 0.13296603\n",
      " 0.11092221 0.21078667 0.69024372 0.24083005 0.15000924 0.41430423\n",
      " 0.84379464 0.86003029 0.90696114 0.9293915  0.93259281 0.51492643\n",
      " 0.73723137 0.94203544 0.86043423 0.72989696 0.90768296 0.84971219\n",
      " 0.82161081 0.8369512  0.48615691 0.91357827 0.98240119 0.98786527\n",
      " 0.95180655 0.92743337 0.88322484 0.9498325  0.91995883 0.40868542\n",
      " 0.70534891 0.87149233 0.93716574 0.80760306 0.74028778 0.97327811\n",
      " 0.79853272 0.27036372 0.42807823 0.86496776 0.5052619  0.95341009\n",
      " 0.25901395 0.64054781 0.85105246 0.94695079 0.17046413 0.83249044\n",
      " 0.70069981 0.39760247 0.27008504 0.29043895 0.82947046 0.09185505\n",
      " 0.67201489 0.40287328 0.50882101 0.3420265  0.89686537 0.42995402\n",
      " 0.78916502 0.6966207  0.93230683 0.95190537]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 72 [0/43 (0%)]\tTrain Loss: 0.083359\n",
      "Train Epoch: 72 [10/43 (23%)]\tTrain Loss: 0.071408\n",
      "Train Epoch: 72 [20/43 (47%)]\tTrain Loss: 0.060590\n",
      "Train Epoch: 72 [30/43 (70%)]\tTrain Loss: 0.070243\n",
      "Train Epoch: 72 [40/43 (93%)]\tTrain Loss: 0.061053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.4258253  0.80136609 0.86265641 0.7784242  0.79601687 0.00482642\n",
      " 0.89422429 0.62211365 0.46002731 0.00595107 0.0061607  0.00310572\n",
      " 0.00519352 0.57396692 0.93779856 0.22708645 0.61219591 0.24582724\n",
      " 0.13389955 0.4556587  0.19848959 0.37844303 0.74556875 0.78898227\n",
      " 0.32036382 0.80872661 0.76816207 0.63374305 0.17053482 0.28706029\n",
      " 0.79274768 0.46832108 0.30678913 0.00370639 0.16695225 0.11592736\n",
      " 0.41051069 0.50118715 0.12477671 0.20381667 0.14968593 0.05628554\n",
      " 0.07010433 0.26755911 0.44376886 0.00268736 0.00740898 0.00213453\n",
      " 0.0023821  0.00137978 0.00295724 0.00501801 0.00359012 0.00298616\n",
      " 0.35755637 0.0022932  0.3215774  0.00293584 0.00673726 0.00168478\n",
      " 0.81865484 0.86198473 0.8015458  0.88512236 0.74946547 0.47389767\n",
      " 0.41340658 0.83277076 0.89492643 0.46704867 0.7423225  0.70662266\n",
      " 0.6233964  0.58150542 0.2246054  0.70320719 0.92393398 0.93875879\n",
      " 0.94261223 0.82177734 0.70798504 0.82513583 0.88499367 0.26397541\n",
      " 0.31144214 0.6955145  0.80761725 0.52506298 0.71640903 0.94773167\n",
      " 0.76924068 0.00191212 0.60843557 0.84841275 0.75958908 0.78934431\n",
      " 0.00233952 0.48521224 0.53879124 0.69030201 0.08263323 0.55082005\n",
      " 0.56950659 0.07209836 0.33164516 0.31263155 0.6619755  0.09871299\n",
      " 0.4072465  0.30870619 0.63441455 0.1588884  0.60089463 0.60070348\n",
      " 0.77234179 0.76678622 0.67456418 0.83559108]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 73 [0/43 (0%)]\tTrain Loss: 0.045057\n",
      "Train Epoch: 73 [10/43 (23%)]\tTrain Loss: 0.063155\n",
      "Train Epoch: 73 [20/43 (47%)]\tTrain Loss: 0.072219\n",
      "Train Epoch: 73 [30/43 (70%)]\tTrain Loss: 0.059182\n",
      "Train Epoch: 73 [40/43 (93%)]\tTrain Loss: 0.056596\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.9293173  0.92682284 0.85180891 0.82297766 0.80461633 0.76285255\n",
      " 0.72525561 0.85351288 0.76310325 0.7787081  0.83157766 0.74740469\n",
      " 0.81877041 0.9880653  0.97213972 0.5726189  0.67065346 0.70332497\n",
      " 0.57005292 0.47331476 0.56091082 0.76032859 0.94633758 0.81564444\n",
      " 0.48273876 0.81908631 0.89236629 0.77707928 0.72965419 0.43486843\n",
      " 0.81513578 0.75186735 0.37838602 0.87543994 0.68562442 0.13919477\n",
      " 0.47518861 0.6732887  0.47618228 0.62404495 0.39660421 0.58911222\n",
      " 0.85033524 0.66208148 0.59829891 0.77828169 0.90822589 0.9309963\n",
      " 0.67096251 0.87053549 0.86234719 0.84777951 0.90400618 0.56419724\n",
      " 0.29501826 0.83525383 0.6571666  0.87719059 0.69321752 0.85840672\n",
      " 0.82933348 0.93311816 0.88246906 0.89821631 0.9154253  0.80736876\n",
      " 0.74352801 0.98906088 0.98154449 0.89865845 0.75951046 0.85300171\n",
      " 0.70750636 0.80248171 0.46661666 0.64155477 0.98106891 0.99320364\n",
      " 0.97332913 0.90721518 0.8334257  0.86501044 0.92804241 0.66051763\n",
      " 0.60318559 0.60741609 0.93215579 0.97154266 0.99190021 0.98673207\n",
      " 0.99050444 0.86280352 0.78430152 0.88669115 0.69242984 0.68600917\n",
      " 0.79572523 0.53490418 0.90890908 0.96845901 0.60342818 0.81123006\n",
      " 0.7810955  0.31742638 0.64573616 0.83655733 0.81248486 0.25309601\n",
      " 0.93889165 0.29463208 0.77266282 0.64164245 0.6532045  0.82814699\n",
      " 0.97233969 0.95414561 0.79914224 0.93690419]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 74 [0/43 (0%)]\tTrain Loss: 0.059871\n",
      "Train Epoch: 74 [10/43 (23%)]\tTrain Loss: 0.054652\n",
      "Train Epoch: 74 [20/43 (47%)]\tTrain Loss: 0.053623\n",
      "Train Epoch: 74 [30/43 (70%)]\tTrain Loss: 0.060722\n",
      "Train Epoch: 74 [40/43 (93%)]\tTrain Loss: 0.043396\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.46975833 0.84508848 0.88016844 0.85594076 0.65264982 0.31347361\n",
      " 0.69401175 0.65479177 0.62235397 0.25380924 0.52088058 0.39039969\n",
      " 0.57554489 0.99089277 0.68048567 0.21964167 0.47188458 0.740089\n",
      " 0.61409187 0.4273451  0.64697993 0.90400666 0.89863294 0.90780061\n",
      " 0.8374176  0.91031724 0.91593671 0.82481694 0.43560854 0.7207945\n",
      " 0.87382507 0.78334624 0.40819892 0.4789851  0.43351158 0.23284563\n",
      " 0.5570119  0.71142703 0.87741107 0.7886827  0.54946011 0.42974317\n",
      " 0.52699584 0.70040375 0.69727212 0.43750414 0.54371762 0.77815002\n",
      " 0.48987675 0.28150687 0.43034407 0.59529305 0.54958856 0.67556512\n",
      " 0.33696759 0.47971788 0.69784808 0.61316216 0.31351444 0.50060362\n",
      " 0.78633177 0.88099372 0.83381557 0.97579145 0.78097421 0.73765224\n",
      " 0.600694   0.99005085 0.96890867 0.65595669 0.56578338 0.72580498\n",
      " 0.88178539 0.79818428 0.59902376 0.77465022 0.97586459 0.98673117\n",
      " 0.94437122 0.77235955 0.9418996  0.95903265 0.92057633 0.62549335\n",
      " 0.38618129 0.67212951 0.92615777 0.95048082 0.99447483 0.99770433\n",
      " 0.97512984 0.56139421 0.49127057 0.95153284 0.60805976 0.85083508\n",
      " 0.5905869  0.89311159 0.7145347  0.97432739 0.52533817 0.78566355\n",
      " 0.71708143 0.41887668 0.47670794 0.59906936 0.86342704 0.36334363\n",
      " 0.96532029 0.67523515 0.79221928 0.77470392 0.89301485 0.84592074\n",
      " 0.94611079 0.92493439 0.9270972  0.94013494]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 75 [0/43 (0%)]\tTrain Loss: 0.044820\n",
      "Train Epoch: 75 [10/43 (23%)]\tTrain Loss: 0.085864\n",
      "Train Epoch: 75 [20/43 (47%)]\tTrain Loss: 0.048954\n",
      "Train Epoch: 75 [30/43 (70%)]\tTrain Loss: 0.051166\n",
      "Train Epoch: 75 [40/43 (93%)]\tTrain Loss: 0.056490\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.19396231 0.71903914 0.72597623 0.30794051 0.16136262 0.16752863\n",
      " 0.59705299 0.40851763 0.35482275 0.4128606  0.27864379 0.33765596\n",
      " 0.3811917  0.91506076 0.47114334 0.22178912 0.15037602 0.90042531\n",
      " 0.60702568 0.61048526 0.69318813 0.68245673 0.88238531 0.77997094\n",
      " 0.53137702 0.94046593 0.92716032 0.59454513 0.27706146 0.40851039\n",
      " 0.73506719 0.31730619 0.25675797 0.13724008 0.13351615 0.2636047\n",
      " 0.15013234 0.24347425 0.78079265 0.45533523 0.43943024 0.44143277\n",
      " 0.26608229 0.21144447 0.12894459 0.26835915 0.39590302 0.25284111\n",
      " 0.244057   0.2536127  0.32396695 0.50015712 0.29464009 0.11504018\n",
      " 0.29514363 0.3918668  0.39256909 0.50364608 0.29092926 0.39666954\n",
      " 0.79193431 0.79079783 0.88514954 0.84377718 0.58766955 0.47383544\n",
      " 0.29882383 0.96491939 0.9204641  0.82659739 0.86797923 0.89631474\n",
      " 0.78795564 0.87823141 0.58835882 0.6080035  0.94006592 0.9066363\n",
      " 0.87728858 0.8139348  0.80356699 0.86786312 0.87975645 0.45224175\n",
      " 0.36865893 0.7121067  0.91625214 0.89050293 0.91163051 0.96274477\n",
      " 0.94137949 0.23936768 0.34591848 0.92152148 0.29343748 0.84209806\n",
      " 0.44037205 0.79957747 0.74111211 0.97579342 0.32310405 0.58950031\n",
      " 0.46342796 0.23154154 0.24349557 0.79227984 0.527753   0.26513398\n",
      " 0.96040595 0.32341322 0.72855604 0.60121131 0.94736326 0.86061811\n",
      " 0.79370755 0.81068397 0.74964654 0.91654164]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 76 [0/43 (0%)]\tTrain Loss: 0.045432\n",
      "Train Epoch: 76 [10/43 (23%)]\tTrain Loss: 0.094449\n",
      "Train Epoch: 76 [20/43 (47%)]\tTrain Loss: 0.041026\n",
      "Train Epoch: 76 [30/43 (70%)]\tTrain Loss: 0.065838\n",
      "Train Epoch: 76 [40/43 (93%)]\tTrain Loss: 0.060075\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.274331   0.87740225 0.73517233 0.64101481 0.49712116 0.33678669\n",
      " 0.75330466 0.54893714 0.3866919  0.35871583 0.48791632 0.19979712\n",
      " 0.49755633 0.9934749  0.77894819 0.28412661 0.49522397 0.95793742\n",
      " 0.8422026  0.67865205 0.52958888 0.88744909 0.99392128 0.95543808\n",
      " 0.59248143 0.97526324 0.99586546 0.98064947 0.85572916 0.96552491\n",
      " 0.94300115 0.83173209 0.30625823 0.17387393 0.15137222 0.29602045\n",
      " 0.26833421 0.63530904 0.92946422 0.92058331 0.91353548 0.79088551\n",
      " 0.40852356 0.5113799  0.38344994 0.2791217  0.46748608 0.55919105\n",
      " 0.74080086 0.2540231  0.71016794 0.22758952 0.43138713 0.36833918\n",
      " 0.42845705 0.65141326 0.66650879 0.74062204 0.59482688 0.50993955\n",
      " 0.86264569 0.96597099 0.9623338  0.9736008  0.9316361  0.703412\n",
      " 0.76560748 0.99835002 0.99891376 0.74203092 0.9483816  0.9243812\n",
      " 0.89951909 0.94937748 0.73315549 0.86423302 0.99506783 0.99352211\n",
      " 0.94806844 0.91209829 0.88361317 0.94204634 0.96320677 0.82744855\n",
      " 0.8482132  0.88871467 0.97503948 0.97761017 0.97789383 0.99617648\n",
      " 0.99186158 0.37678576 0.50477493 0.99543011 0.86772776 0.98881501\n",
      " 0.48956224 0.92564654 0.96492904 0.99497306 0.78316653 0.8346113\n",
      " 0.88946778 0.37587219 0.38876927 0.93568742 0.91725558 0.27666938\n",
      " 0.97518116 0.65546936 0.93860483 0.91089374 0.98287749 0.94422126\n",
      " 0.95548379 0.97773755 0.90208215 0.96512401]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 77 [0/43 (0%)]\tTrain Loss: 0.044471\n",
      "Train Epoch: 77 [10/43 (23%)]\tTrain Loss: 0.101547\n",
      "Train Epoch: 77 [20/43 (47%)]\tTrain Loss: 0.057454\n",
      "Train Epoch: 77 [30/43 (70%)]\tTrain Loss: 0.091606\n",
      "Train Epoch: 77 [40/43 (93%)]\tTrain Loss: 0.060333\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.08671077 0.82296723 0.77196246 0.20459828 0.44253656 0.31168872\n",
      " 0.4441289  0.41171914 0.418749   0.05318488 0.11729735 0.14406751\n",
      " 0.09277699 0.85039014 0.26274765 0.17328656 0.09416027 0.41049147\n",
      " 0.23862891 0.41385329 0.08230583 0.43438584 0.80909741 0.78540373\n",
      " 0.32328513 0.78764248 0.87087965 0.60530674 0.3905842  0.38355878\n",
      " 0.7518127  0.37792042 0.4542419  0.01896831 0.01371411 0.14520547\n",
      " 0.08181848 0.34739771 0.46350783 0.10924596 0.1973294  0.18952624\n",
      " 0.28889495 0.14221792 0.12441023 0.16995785 0.34008196 0.3671338\n",
      " 0.62094182 0.07512253 0.22139984 0.13868441 0.08962583 0.06731847\n",
      " 0.21949995 0.16906467 0.51310283 0.16217463 0.15448931 0.05951432\n",
      " 0.73963368 0.94431585 0.91291678 0.84157836 0.66278702 0.14802554\n",
      " 0.45076495 0.91895962 0.94128579 0.38124847 0.73121762 0.64296764\n",
      " 0.51009268 0.48611343 0.34215745 0.59288663 0.94867313 0.98619485\n",
      " 0.94634867 0.80469066 0.6925869  0.83502054 0.8445161  0.54615515\n",
      " 0.38904715 0.86068332 0.89508486 0.86458659 0.79827678 0.94418269\n",
      " 0.85194552 0.14845987 0.27809697 0.81044728 0.28266227 0.90826023\n",
      " 0.10583576 0.79990524 0.77298075 0.66898751 0.22438836 0.40679514\n",
      " 0.36486617 0.07665221 0.10580587 0.22984026 0.50847298 0.07850472\n",
      " 0.36040375 0.14068392 0.50588208 0.4274171  0.78199214 0.46165767\n",
      " 0.79857677 0.714194   0.75599355 0.72802562]\n",
      "predict [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 78 [0/43 (0%)]\tTrain Loss: 0.054115\n",
      "Train Epoch: 78 [10/43 (23%)]\tTrain Loss: 0.055793\n",
      "Train Epoch: 78 [20/43 (47%)]\tTrain Loss: 0.052242\n",
      "Train Epoch: 78 [30/43 (70%)]\tTrain Loss: 0.067315\n",
      "Train Epoch: 78 [40/43 (93%)]\tTrain Loss: 0.044304\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.34713736 0.55207992 0.49861169 0.4112218  0.59211534 0.25455514\n",
      " 0.60371417 0.35907313 0.58465677 0.23146912 0.58565646 0.34703726\n",
      " 0.71569216 0.5539887  0.27643332 0.14286713 0.13049583 0.7329492\n",
      " 0.64794588 0.58521122 0.35190248 0.78987914 0.84076077 0.6966967\n",
      " 0.64916956 0.59379011 0.56327862 0.53198189 0.37356567 0.6246224\n",
      " 0.78611863 0.68598336 0.33704296 0.65351123 0.55259675 0.15221368\n",
      " 0.19877394 0.64304906 0.56585902 0.40160561 0.47096369 0.52692759\n",
      " 0.51149619 0.26871216 0.50557464 0.6452747  0.5988512  0.46414959\n",
      " 0.39174771 0.44701108 0.33921319 0.56547707 0.69732744 0.26649427\n",
      " 0.45420426 0.54938877 0.64140737 0.61343908 0.45881924 0.50393647\n",
      " 0.85470688 0.9211787  0.92207646 0.9474799  0.4554145  0.1982756\n",
      " 0.21480776 0.97556156 0.8327201  0.30436528 0.68276751 0.80598921\n",
      " 0.43561259 0.50608981 0.16499844 0.89451551 0.96171212 0.97498161\n",
      " 0.93377614 0.7222268  0.65744525 0.92175668 0.89187425 0.58918732\n",
      " 0.42182568 0.47616085 0.81520778 0.92717373 0.4767735  0.92939639\n",
      " 0.84494448 0.58999884 0.48873821 0.84989661 0.70102537 0.61540747\n",
      " 0.59229302 0.73226517 0.68205935 0.69345701 0.55678952 0.78517783\n",
      " 0.78599834 0.15551981 0.37581679 0.85108268 0.39579409 0.17950888\n",
      " 0.38828477 0.55088073 0.52158672 0.21086031 0.81455654 0.45949122\n",
      " 0.80556834 0.90991753 0.87474364 0.90885675]\n",
      "predict [0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "Train Epoch: 79 [0/43 (0%)]\tTrain Loss: 0.051825\n",
      "Train Epoch: 79 [10/43 (23%)]\tTrain Loss: 0.039693\n",
      "Train Epoch: 79 [20/43 (47%)]\tTrain Loss: 0.034427\n",
      "Train Epoch: 79 [30/43 (70%)]\tTrain Loss: 0.072987\n",
      "Train Epoch: 79 [40/43 (93%)]\tTrain Loss: 0.092346\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.14963867 0.91983062 0.89498216 0.88722736 0.61739057 0.85978925\n",
      " 0.77678907 0.76617599 0.58811945 0.75610667 0.80711955 0.67406648\n",
      " 0.65809542 0.99212569 0.91430485 0.33033636 0.54780263 0.7297765\n",
      " 0.72694868 0.47418129 0.43452716 0.91511053 0.98497307 0.89866048\n",
      " 0.72766536 0.88319743 0.90721852 0.72639686 0.5527789  0.89251137\n",
      " 0.91128719 0.69654286 0.31556818 0.66827035 0.24669878 0.15353352\n",
      " 0.4181616  0.78326994 0.60722518 0.3485685  0.68099141 0.53721172\n",
      " 0.6574263  0.56695342 0.58849859 0.93274325 0.67582405 0.2313022\n",
      " 0.68920666 0.61423343 0.78617579 0.24933229 0.81107563 0.30638802\n",
      " 0.26377091 0.81016695 0.75685143 0.89040512 0.81543022 0.42387283\n",
      " 0.94006866 0.97434539 0.94825447 0.96639311 0.94568563 0.82421517\n",
      " 0.5924089  0.99670702 0.99582738 0.94856089 0.89673913 0.9329083\n",
      " 0.80710757 0.92230082 0.79699212 0.96122801 0.9788987  0.98374003\n",
      " 0.97647333 0.96740079 0.95677227 0.97790819 0.97641844 0.28075004\n",
      " 0.58742875 0.8880744  0.67064506 0.93255651 0.95492822 0.95320171\n",
      " 0.86831951 0.6966899  0.74002206 0.99142027 0.54841548 0.93871248\n",
      " 0.65264386 0.77581578 0.48430371 0.98107713 0.40001741 0.92552596\n",
      " 0.56240714 0.21539588 0.29757929 0.57265902 0.88987964 0.10746057\n",
      " 0.98589224 0.43331021 0.90637308 0.35917932 0.89167386 0.89686453\n",
      " 0.92811626 0.74548161 0.91567963 0.92886031]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 80 [0/43 (0%)]\tTrain Loss: 0.047170\n",
      "Train Epoch: 80 [10/43 (23%)]\tTrain Loss: 0.099753\n",
      "Train Epoch: 80 [20/43 (47%)]\tTrain Loss: 0.050282\n",
      "Train Epoch: 80 [30/43 (70%)]\tTrain Loss: 0.023139\n",
      "Train Epoch: 80 [40/43 (93%)]\tTrain Loss: 0.077050\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.06483033 0.65338987 0.56165564 0.30787107 0.40056953 0.00252818\n",
      " 0.4399505  0.55306739 0.38045105 0.00463414 0.10175361 0.00149096\n",
      " 0.26358864 0.76473844 0.8898297  0.22108379 0.61766958 0.36234343\n",
      " 0.11119802 0.38636726 0.58878541 0.71835536 0.21920963 0.74108636\n",
      " 0.38445613 0.46527222 0.75240493 0.66097021 0.24719259 0.30863431\n",
      " 0.74076426 0.58014292 0.45384026 0.02813078 0.08967881 0.01987525\n",
      " 0.22449766 0.5568555  0.37682161 0.7740199  0.28913385 0.29611525\n",
      " 0.49908075 0.30691063 0.54686397 0.54258049 0.11159149 0.5104211\n",
      " 0.76464331 0.00272391 0.00119965 0.07064625 0.00325416 0.10865616\n",
      " 0.00195234 0.2645857  0.86444414 0.07228209 0.00308129 0.32114437\n",
      " 0.70505255 0.39358029 0.89668822 0.64849919 0.59139192 0.54189402\n",
      " 0.54300833 0.74459517 0.32001889 0.1548827  0.79091579 0.54269719\n",
      " 0.73247242 0.58686537 0.48649991 0.87147713 0.86645186 0.95316964\n",
      " 0.8701548  0.67029536 0.85327172 0.83821088 0.91818362 0.08707624\n",
      " 0.43130603 0.64526957 0.69371647 0.62853187 0.72825748 0.82948691\n",
      " 0.78218216 0.56503516 0.35552305 0.85438097 0.38912311 0.77954274\n",
      " 0.00471614 0.56173217 0.59852123 0.50083494 0.3140623  0.68549716\n",
      " 0.22061124 0.27133894 0.78657663 0.07755152 0.60455763 0.2540082\n",
      " 0.63139594 0.46782002 0.68810624 0.46852118 0.72670001 0.73958856\n",
      " 0.36061424 0.48742113 0.80955958 0.90734494]\n",
      "predict [0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1.]\n",
      "vote_pred [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 46 TN= 38 FN= 12 FP= 22\n",
      "TP+FP 68\n",
      "precision 0.6764705882352942\n",
      "recall 0.7931034482758621\n",
      "F1 0.7301587301587301\n",
      "acc 0.711864406779661\n",
      "AUCp 0.7132183908045977\n",
      "AUC 0.796264367816092\n",
      "\n",
      " The epoch is 80, average recall: 0.7931, average precision: 0.6765,average F1: 0.7302, average accuracy: 0.7119, average AUC: 0.7963\n",
      "Train Epoch: 81 [0/43 (0%)]\tTrain Loss: 0.059519\n",
      "Train Epoch: 81 [10/43 (23%)]\tTrain Loss: 0.059802\n",
      "Train Epoch: 81 [20/43 (47%)]\tTrain Loss: 0.039449\n",
      "Train Epoch: 81 [30/43 (70%)]\tTrain Loss: 0.044462\n",
      "Train Epoch: 81 [40/43 (93%)]\tTrain Loss: 0.065577\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.14813283 0.87353891 0.78644925 0.93834203 0.61794686 0.26602569\n",
      " 0.87037599 0.83346546 0.21848759 0.57467866 0.16425751 0.59849721\n",
      " 0.58053821 0.99095988 0.79834235 0.41827989 0.85137087 0.57468605\n",
      " 0.4082422  0.4883     0.5060634  0.66509157 0.85194409 0.87419516\n",
      " 0.54067868 0.84454679 0.79237336 0.70955396 0.12096217 0.80774343\n",
      " 0.87657428 0.79821026 0.51068783 0.13733011 0.08468543 0.09866569\n",
      " 0.42277336 0.74177206 0.28096357 0.37572667 0.49510354 0.26262316\n",
      " 0.62431443 0.52175164 0.69789803 0.7670626  0.58986109 0.30661264\n",
      " 0.76307446 0.66998464 0.616844   0.20972073 0.62315518 0.18616728\n",
      " 0.87804824 0.87988418 0.73810273 0.45082694 0.69959891 0.57484335\n",
      " 0.9379679  0.91689044 0.9358393  0.91467738 0.80696046 0.70680374\n",
      " 0.69506586 0.98010808 0.94065922 0.42407626 0.81774092 0.84308213\n",
      " 0.72689015 0.91292644 0.58904302 0.80240667 0.98143148 0.97488111\n",
      " 0.9796021  0.84824353 0.8818987  0.94700807 0.9790622  0.64360064\n",
      " 0.59392083 0.83990175 0.83667582 0.93832463 0.81207657 0.96452188\n",
      " 0.94385695 0.58428043 0.85207117 0.97876304 0.86927229 0.95226109\n",
      " 0.57066619 0.61848605 0.36151412 0.95113969 0.15395704 0.81030077\n",
      " 0.23095937 0.39307126 0.19380267 0.36604953 0.66661465 0.72990292\n",
      " 0.69459617 0.27555734 0.76556301 0.77204466 0.71807182 0.85590041\n",
      " 0.76163304 0.33063304 0.90831208 0.87194264]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 82 [0/43 (0%)]\tTrain Loss: 0.082746\n",
      "Train Epoch: 82 [10/43 (23%)]\tTrain Loss: 0.051346\n",
      "Train Epoch: 82 [20/43 (47%)]\tTrain Loss: 0.058578\n",
      "Train Epoch: 82 [30/43 (70%)]\tTrain Loss: 0.049224\n",
      "Train Epoch: 82 [40/43 (93%)]\tTrain Loss: 0.054470\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.15060154 0.89386344 0.89168823 0.56287128 0.4153845  0.15930964\n",
      " 0.87342095 0.59859353 0.33311704 0.10922249 0.15880634 0.09097464\n",
      " 0.13294245 0.94821107 0.73887479 0.21240994 0.59888995 0.57574707\n",
      " 0.72961915 0.50598282 0.35012293 0.8727383  0.85009289 0.85037076\n",
      " 0.43972412 0.79014021 0.87398624 0.58570099 0.22657709 0.46067831\n",
      " 0.84952813 0.74098688 0.37420458 0.16903332 0.0348554  0.05812837\n",
      " 0.23128498 0.61678708 0.44516879 0.25645846 0.35367706 0.25737756\n",
      " 0.21440138 0.16586308 0.67107105 0.21969232 0.04212118 0.29465941\n",
      " 0.08153246 0.11757098 0.16862436 0.07166757 0.15571134 0.04618646\n",
      " 0.03063809 0.46614182 0.69002849 0.14946127 0.0524277  0.11510047\n",
      " 0.91963428 0.95929986 0.87586772 0.92731458 0.71222687 0.59250563\n",
      " 0.84553331 0.94786352 0.91093946 0.07542296 0.94000971 0.88319302\n",
      " 0.70859534 0.85874349 0.61343527 0.81284833 0.98133677 0.9760887\n",
      " 0.96617806 0.95507371 0.82211739 0.92942858 0.94856477 0.38881016\n",
      " 0.6505205  0.8143369  0.83316553 0.94958627 0.82718271 0.90253967\n",
      " 0.96363372 0.20717598 0.57387114 0.97375172 0.68784761 0.80043048\n",
      " 0.07839476 0.82611215 0.65134549 0.8781392  0.70258301 0.77230376\n",
      " 0.68833607 0.33141801 0.45805925 0.90221602 0.04487213 0.52556902\n",
      " 0.93653858 0.23529978 0.76885253 0.77363539 0.72825789 0.78426301\n",
      " 0.90847564 0.88202041 0.93098366 0.89275289]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 83 [0/43 (0%)]\tTrain Loss: 0.051083\n",
      "Train Epoch: 83 [10/43 (23%)]\tTrain Loss: 0.027563\n",
      "Train Epoch: 83 [20/43 (47%)]\tTrain Loss: 0.046995\n",
      "Train Epoch: 83 [30/43 (70%)]\tTrain Loss: 0.053077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 83 [40/43 (93%)]\tTrain Loss: 0.057655\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.48946285 0.83004951 0.45893687 0.78996706 0.32233304 0.4968242\n",
      " 0.42004448 0.44986182 0.36138454 0.0717189  0.53458297 0.0706436\n",
      " 0.44077426 0.83122325 0.52880681 0.06939396 0.32447693 0.61076707\n",
      " 0.45286041 0.44418621 0.48953801 0.41469091 0.85824478 0.69248885\n",
      " 0.64172566 0.85616535 0.76701659 0.46731368 0.211438   0.66762924\n",
      " 0.87846553 0.58150506 0.72205299 0.04177466 0.02493249 0.09331036\n",
      " 0.22201656 0.60680097 0.41146117 0.31439829 0.3586494  0.25967458\n",
      " 0.73599076 0.41796076 0.40966457 0.81941175 0.06041742 0.5000577\n",
      " 0.03510698 0.03849575 0.03460221 0.05541916 0.0719792  0.08678276\n",
      " 0.343151   0.28940538 0.76111412 0.14136758 0.02217531 0.09001176\n",
      " 0.92030996 0.82409781 0.90787119 0.96272862 0.70250231 0.27738547\n",
      " 0.52436632 0.93731987 0.95680678 0.63967538 0.80889779 0.84574378\n",
      " 0.6301319  0.91154009 0.24088715 0.8732608  0.96139324 0.98421013\n",
      " 0.89944732 0.77196079 0.86262065 0.84621996 0.88899457 0.58825511\n",
      " 0.39095679 0.73656213 0.89361519 0.90276724 0.49191886 0.827869\n",
      " 0.8200953  0.48606631 0.6946106  0.93733537 0.6330598  0.86633569\n",
      " 0.04421328 0.71372801 0.80689406 0.94871682 0.28155154 0.81378096\n",
      " 0.43676338 0.34379727 0.60692799 0.45890668 0.67794925 0.64870059\n",
      " 0.82067668 0.44931877 0.68499666 0.58769089 0.53935724 0.8945322\n",
      " 0.78491694 0.832784   0.74041069 0.95540732]\n",
      "predict [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 84 [0/43 (0%)]\tTrain Loss: 0.058488\n",
      "Train Epoch: 84 [10/43 (23%)]\tTrain Loss: 0.031081\n",
      "Train Epoch: 84 [20/43 (47%)]\tTrain Loss: 0.095645\n",
      "Train Epoch: 84 [30/43 (70%)]\tTrain Loss: 0.032345\n",
      "Train Epoch: 84 [40/43 (93%)]\tTrain Loss: 0.062760\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.18595973 0.70768732 0.63873345 0.56880695 0.49271396 0.19725122\n",
      " 0.73717582 0.67115498 0.42433876 0.16979477 0.19573016 0.1549383\n",
      " 0.33700141 0.80225641 0.79808277 0.30691344 0.49465609 0.75018138\n",
      " 0.66266876 0.44450545 0.54355007 0.79503858 0.78437972 0.73990959\n",
      " 0.67586106 0.86249143 0.85118043 0.83039474 0.58395696 0.62271613\n",
      " 0.81280112 0.51260197 0.47655386 0.20972782 0.16844301 0.18871289\n",
      " 0.22492357 0.39674419 0.64653254 0.62654996 0.52803737 0.58001554\n",
      " 0.49255913 0.21570738 0.33319297 0.37881255 0.22148585 0.23670729\n",
      " 0.34810564 0.15614411 0.08435619 0.24126254 0.15274182 0.09690025\n",
      " 0.28897884 0.26796263 0.57067639 0.21724738 0.22876246 0.24061789\n",
      " 0.76413149 0.71036214 0.83412737 0.91387534 0.67511451 0.75540942\n",
      " 0.84326482 0.95995212 0.89052373 0.68010086 0.74364239 0.75415504\n",
      " 0.69905877 0.86021537 0.61532861 0.77086318 0.93557662 0.93399698\n",
      " 0.95457006 0.8928923  0.87833416 0.93106675 0.85422587 0.38356939\n",
      " 0.46055174 0.74281061 0.58294982 0.86234879 0.55401635 0.95532668\n",
      " 0.91264129 0.59804124 0.42750311 0.93319488 0.50280327 0.89192837\n",
      " 0.12979482 0.52527869 0.5534597  0.89909244 0.25498307 0.80253553\n",
      " 0.42632297 0.38226575 0.53705287 0.59034204 0.6041168  0.41331822\n",
      " 0.77922976 0.62769055 0.86039519 0.73657119 0.7142942  0.80089033\n",
      " 0.65295541 0.71493119 0.64795709 0.84140009]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 85 [0/43 (0%)]\tTrain Loss: 0.044559\n",
      "Train Epoch: 85 [10/43 (23%)]\tTrain Loss: 0.066139\n",
      "Train Epoch: 85 [20/43 (47%)]\tTrain Loss: 0.043451\n",
      "Train Epoch: 85 [30/43 (70%)]\tTrain Loss: 0.055639\n",
      "Train Epoch: 85 [40/43 (93%)]\tTrain Loss: 0.049247\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.17007406 0.6707983  0.30831614 0.46502641 0.30455399 0.25037709\n",
      " 0.31685364 0.49219161 0.23181002 0.37179491 0.25337917 0.18143824\n",
      " 0.53384602 0.92085767 0.43968669 0.11794442 0.66674542 0.72601438\n",
      " 0.28979173 0.49378315 0.34543759 0.86511093 0.90133446 0.84867179\n",
      " 0.37647378 0.88733435 0.8663528  0.77328914 0.20141955 0.51594317\n",
      " 0.69658095 0.79861289 0.36740986 0.02381839 0.03003432 0.06264564\n",
      " 0.09123605 0.50289065 0.26197669 0.30064064 0.32401738 0.10097986\n",
      " 0.2383045  0.24989524 0.19675808 0.25730079 0.58486152 0.13869734\n",
      " 0.31257182 0.22150347 0.1592243  0.36675927 0.26583657 0.01374943\n",
      " 0.227669   0.26991346 0.69693798 0.56633711 0.22935259 0.07114308\n",
      " 0.89683163 0.93228787 0.89492679 0.92798883 0.74063915 0.79413211\n",
      " 0.78787136 0.92697436 0.97459996 0.94077158 0.76108181 0.44942439\n",
      " 0.74218422 0.81235391 0.62664402 0.75518072 0.98327214 0.98344791\n",
      " 0.95945245 0.87979633 0.95300305 0.95172101 0.88678527 0.2792415\n",
      " 0.47212684 0.63145369 0.82327956 0.81678236 0.62291759 0.98134804\n",
      " 0.91856337 0.2966716  0.21504644 0.98939836 0.86649543 0.97984719\n",
      " 0.33391812 0.80332494 0.37566149 0.99113631 0.12686044 0.62383372\n",
      " 0.1208263  0.1010575  0.29035684 0.43557537 0.76324451 0.68825698\n",
      " 0.78737926 0.40608537 0.68482143 0.90223217 0.88179791 0.83376682\n",
      " 0.78092206 0.50037944 0.9385711  0.92256647]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 86 [0/43 (0%)]\tTrain Loss: 0.049884\n",
      "Train Epoch: 86 [10/43 (23%)]\tTrain Loss: 0.044223\n",
      "Train Epoch: 86 [20/43 (47%)]\tTrain Loss: 0.044243\n",
      "Train Epoch: 86 [30/43 (70%)]\tTrain Loss: 0.087452\n",
      "Train Epoch: 86 [40/43 (93%)]\tTrain Loss: 0.058286\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.05099852 0.75408113 0.45922613 0.06496473 0.15875614 0.20049047\n",
      " 0.58256716 0.07192025 0.04383932 0.62902808 0.26000673 0.46913287\n",
      " 0.06484149 0.58711821 0.06826    0.11738033 0.18563303 0.44363052\n",
      " 0.33124918 0.28300437 0.09484758 0.29607925 0.87737143 0.88288832\n",
      " 0.53260618 0.6807515  0.80703062 0.51098901 0.15082864 0.27722353\n",
      " 0.86572653 0.61682999 0.37485412 0.01717497 0.01743045 0.02753619\n",
      " 0.06083307 0.10487655 0.45861301 0.22997667 0.06598298 0.1162743\n",
      " 0.31759697 0.15031563 0.34121737 0.09920728 0.65151364 0.15743198\n",
      " 0.73078501 0.56695753 0.64018846 0.53145754 0.67174691 0.04131471\n",
      " 0.76271415 0.07391202 0.3570849  0.01119632 0.62487602 0.02064312\n",
      " 0.87960684 0.82704353 0.89361566 0.88369918 0.50054353 0.23498568\n",
      " 0.55032009 0.98776662 0.96372044 0.28306073 0.54945964 0.74068409\n",
      " 0.79963672 0.85856909 0.11507794 0.7904405  0.98399734 0.99054551\n",
      " 0.95024735 0.72846997 0.80462867 0.88671768 0.96233249 0.2378681\n",
      " 0.55653495 0.55707562 0.67627358 0.91565585 0.4669469  0.97762817\n",
      " 0.95345658 0.20205647 0.22985645 0.9422875  0.42536974 0.89710146\n",
      " 0.53380179 0.72360194 0.78091031 0.94913203 0.02379225 0.75692892\n",
      " 0.13909715 0.18632399 0.22996467 0.53674436 0.36618355 0.1359802\n",
      " 0.58978277 0.19912353 0.42815778 0.66156715 0.9538244  0.86404246\n",
      " 0.75857353 0.67668277 0.76065451 0.78597897]\n",
      "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 87 [0/43 (0%)]\tTrain Loss: 0.043477\n",
      "Train Epoch: 87 [10/43 (23%)]\tTrain Loss: 0.061605\n",
      "Train Epoch: 87 [20/43 (47%)]\tTrain Loss: 0.081843\n",
      "Train Epoch: 87 [30/43 (70%)]\tTrain Loss: 0.071430\n",
      "Train Epoch: 87 [40/43 (93%)]\tTrain Loss: 0.066467\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.12284185 0.88391781 0.86963266 0.38646805 0.22435308 0.2150695\n",
      " 0.77267337 0.31580126 0.3244262  0.77214044 0.48393673 0.63275486\n",
      " 0.14871986 0.96096033 0.90173006 0.21966913 0.35451272 0.89425081\n",
      " 0.73955214 0.45331529 0.3608512  0.70235264 0.98153138 0.91841197\n",
      " 0.8898555  0.9681648  0.98386985 0.82628793 0.74855262 0.73696262\n",
      " 0.817011   0.87472385 0.46692222 0.83335781 0.08036864 0.11555659\n",
      " 0.08673657 0.39711416 0.69450074 0.68171519 0.56131822 0.71972835\n",
      " 0.82806832 0.28686762 0.42839897 0.83186483 0.68704265 0.44511074\n",
      " 0.75295037 0.56090271 0.67320269 0.88446915 0.82642764 0.1326016\n",
      " 0.77885711 0.80116189 0.6264925  0.86490142 0.80146295 0.42766005\n",
      " 0.93062717 0.98577696 0.96682972 0.94492239 0.83530921 0.89508599\n",
      " 0.43310866 0.99615616 0.98762518 0.8558892  0.58842278 0.63604569\n",
      " 0.68023813 0.90639275 0.4978151  0.84835029 0.9934243  0.96888685\n",
      " 0.98149419 0.91267735 0.96863323 0.9453361  0.96214306 0.50656503\n",
      " 0.31665525 0.76673281 0.99040705 0.98121822 0.57211268 0.99498183\n",
      " 0.93551975 0.39097062 0.40973243 0.99867225 0.46249104 0.99311382\n",
      " 0.70533264 0.87063116 0.77886361 0.97620219 0.37543988 0.91087788\n",
      " 0.63896722 0.22168912 0.42578697 0.65831256 0.79567307 0.4856312\n",
      " 0.88070959 0.38133818 0.98623925 0.73914421 0.97458923 0.97044927\n",
      " 0.97221327 0.98881382 0.90880853 0.87234759]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 88 [0/43 (0%)]\tTrain Loss: 0.047750\n",
      "Train Epoch: 88 [10/43 (23%)]\tTrain Loss: 0.078648\n",
      "Train Epoch: 88 [20/43 (47%)]\tTrain Loss: 0.052612\n",
      "Train Epoch: 88 [30/43 (70%)]\tTrain Loss: 0.056458\n",
      "Train Epoch: 88 [40/43 (93%)]\tTrain Loss: 0.030321\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.76519793 0.85330921 0.87546772 0.17160459 0.22939809 0.77693784\n",
      " 0.65951276 0.19128205 0.17704742 0.72426504 0.12716351 0.71519864\n",
      " 0.44771791 0.47855872 0.23464534 0.1008797  0.26740542 0.57361943\n",
      " 0.31013125 0.15441199 0.20960833 0.52809483 0.97567296 0.80811369\n",
      " 0.74778241 0.84708714 0.80645776 0.6113131  0.40225288 0.33178276\n",
      " 0.85665184 0.82254744 0.40528506 0.90976208 0.19430356 0.05741898\n",
      " 0.07249108 0.15877147 0.79499811 0.58202738 0.74571276 0.83334607\n",
      " 0.86996764 0.19485693 0.13254197 0.58687711 0.80885112 0.07658989\n",
      " 0.90987867 0.74418396 0.82158041 0.91962051 0.81115842 0.08745854\n",
      " 0.88882869 0.79023272 0.35564983 0.91706818 0.69886279 0.91835952\n",
      " 0.9093892  0.92187339 0.94073445 0.96073151 0.38520879 0.36162114\n",
      " 0.66366756 0.99644309 0.96681255 0.75884789 0.75014377 0.69680536\n",
      " 0.64234728 0.85521996 0.34819686 0.82186574 0.98678583 0.98458993\n",
      " 0.98974109 0.9114148  0.94271511 0.95421076 0.94662327 0.77352756\n",
      " 0.64139909 0.61667603 0.99139673 0.966398   0.80231172 0.99399865\n",
      " 0.97274017 0.36750162 0.33158472 0.97859544 0.50185651 0.387761\n",
      " 0.8160938  0.91189909 0.89584303 0.94869363 0.53434283 0.91502023\n",
      " 0.56672859 0.33731484 0.23141211 0.88175982 0.65999293 0.1631304\n",
      " 0.62207508 0.21896471 0.75557333 0.62726671 0.95542181 0.91082698\n",
      " 0.98529226 0.94849271 0.86320347 0.92547661]\n",
      "predict [1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 89 [0/43 (0%)]\tTrain Loss: 0.076697\n",
      "Train Epoch: 89 [10/43 (23%)]\tTrain Loss: 0.103623\n",
      "Train Epoch: 89 [20/43 (47%)]\tTrain Loss: 0.027082\n",
      "Train Epoch: 89 [30/43 (70%)]\tTrain Loss: 0.060801\n",
      "Train Epoch: 89 [40/43 (93%)]\tTrain Loss: 0.052250\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.07396486 0.89011073 0.8370989  0.11223498 0.10515843 0.16703355\n",
      " 0.72396725 0.13765904 0.07066931 0.03174372 0.02685288 0.16355632\n",
      " 0.0436547  0.87528658 0.3275786  0.09560584 0.21978083 0.47668219\n",
      " 0.054049   0.46174961 0.27265123 0.55597007 0.77038741 0.83181089\n",
      " 0.27814889 0.63401961 0.83845466 0.58994323 0.07494695 0.30495393\n",
      " 0.8194266  0.46585467 0.14179027 0.08855158 0.02388453 0.04307092\n",
      " 0.04009279 0.12905283 0.48527163 0.13297637 0.14938889 0.13359824\n",
      " 0.01413899 0.12806654 0.05376635 0.12102199 0.12323989 0.04810324\n",
      " 0.11763025 0.07227524 0.07878478 0.23760888 0.10513094 0.00666001\n",
      " 0.15622151 0.26760635 0.3448765  0.08692835 0.0880583  0.01355318\n",
      " 0.82298911 0.8355006  0.81548178 0.7651751  0.2218163  0.58625853\n",
      " 0.40653992 0.89219373 0.82384628 0.13604689 0.56856132 0.77588671\n",
      " 0.52398533 0.73266494 0.16981873 0.67921889 0.97043097 0.97647297\n",
      " 0.96768618 0.7473405  0.63983548 0.91734928 0.93996716 0.28485155\n",
      " 0.42577082 0.39605716 0.67741364 0.63443565 0.38676345 0.71792591\n",
      " 0.69810498 0.15986222 0.20436117 0.95594931 0.55648601 0.17365561\n",
      " 0.193552   0.379354   0.06646199 0.89082444 0.06806225 0.71094656\n",
      " 0.05082721 0.12441885 0.04811294 0.09096561 0.17318495 0.26417592\n",
      " 0.45169404 0.3786028  0.15893185 0.48188946 0.36488017 0.87661594\n",
      " 0.57480097 0.5247528  0.66910911 0.93939209]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 90 [0/43 (0%)]\tTrain Loss: 0.072619\n",
      "Train Epoch: 90 [10/43 (23%)]\tTrain Loss: 0.040189\n",
      "Train Epoch: 90 [20/43 (47%)]\tTrain Loss: 0.060304\n",
      "Train Epoch: 90 [30/43 (70%)]\tTrain Loss: 0.043576\n",
      "Train Epoch: 90 [40/43 (93%)]\tTrain Loss: 0.057877\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.04323695 0.41099185 0.3309997  0.28736579 0.66415155 0.02157701\n",
      " 0.55520833 0.24656607 0.50269753 0.0520293  0.4373804  0.03891516\n",
      " 0.28423288 0.58582681 0.35623118 0.05821167 0.2494095  0.42494243\n",
      " 0.07950485 0.07401392 0.0991733  0.4732179  0.8253485  0.74941468\n",
      " 0.13018855 0.64337796 0.74212086 0.53717124 0.05481764 0.44394588\n",
      " 0.6979745  0.29937801 0.07129585 0.02597411 0.07140251 0.059106\n",
      " 0.048936   0.37504706 0.42553702 0.25761706 0.21157871 0.16550706\n",
      " 0.17218894 0.18235703 0.14662735 0.33556995 0.02465317 0.19660252\n",
      " 0.04061317 0.04024671 0.043444   0.04610977 0.09243281 0.07173023\n",
      " 0.02962189 0.00888242 0.17839423 0.00930445 0.05120623 0.04032506\n",
      " 0.34365153 0.43964255 0.69692653 0.81200927 0.37130949 0.66660488\n",
      " 0.69713223 0.89687085 0.7385273  0.83891308 0.44847664 0.62888598\n",
      " 0.58881855 0.67200226 0.51487797 0.69706494 0.90269506 0.83614868\n",
      " 0.92658186 0.79776245 0.87077683 0.90922028 0.90865427 0.251185\n",
      " 0.27502614 0.35447487 0.51280797 0.84080207 0.26278186 0.95283806\n",
      " 0.97102427 0.28301716 0.44406465 0.96660066 0.71103603 0.85325289\n",
      " 0.02431841 0.52122551 0.53239471 0.78406215 0.06483333 0.63546348\n",
      " 0.08774194 0.15904495 0.2018728  0.06184659 0.0126847  0.35392916\n",
      " 0.67601669 0.38121733 0.52871895 0.55688262 0.86923391 0.89086705\n",
      " 0.70246762 0.50938058 0.57665288 0.70354116]\n",
      "predict [0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 45 TN= 45 FN= 13 FP= 15\n",
      "TP+FP 60\n",
      "precision 0.75\n",
      "recall 0.7758620689655172\n",
      "F1 0.7627118644067795\n",
      "acc 0.7627118644067796\n",
      "AUCp 0.7629310344827586\n",
      "AUC 0.8186781609195403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 90, average recall: 0.7759, average precision: 0.7500,average F1: 0.7627, average accuracy: 0.7627, average AUC: 0.8187\n",
      "Train Epoch: 91 [0/43 (0%)]\tTrain Loss: 0.078357\n",
      "Train Epoch: 91 [10/43 (23%)]\tTrain Loss: 0.048364\n",
      "Train Epoch: 91 [20/43 (47%)]\tTrain Loss: 0.075513\n",
      "Train Epoch: 91 [30/43 (70%)]\tTrain Loss: 0.030412\n",
      "Train Epoch: 91 [40/43 (93%)]\tTrain Loss: 0.067033\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.06611716 0.57341582 0.4792904  0.49232998 0.72038454 0.6804018\n",
      " 0.43390286 0.50797242 0.19131517 0.4932873  0.28262389 0.48667902\n",
      " 0.26376992 0.91785246 0.35532454 0.31388217 0.67449272 0.8265205\n",
      " 0.88273495 0.59448606 0.65547556 0.62661481 0.9635216  0.8799547\n",
      " 0.63002795 0.92750365 0.94730985 0.93247575 0.65082395 0.90351528\n",
      " 0.85205758 0.69877583 0.2509771  0.07448731 0.20727959 0.22961156\n",
      " 0.21691383 0.44606891 0.60975695 0.59002513 0.73008043 0.38413996\n",
      " 0.42102981 0.12531912 0.37930664 0.19306439 0.13553621 0.37508744\n",
      " 0.55541277 0.31431097 0.55470723 0.25233325 0.48822275 0.12072454\n",
      " 0.52738523 0.07508828 0.63207525 0.0536078  0.38497764 0.09519209\n",
      " 0.80756772 0.85441619 0.81656408 0.87613338 0.83056909 0.92132407\n",
      " 0.76838571 0.99212766 0.98574579 0.47453046 0.7820456  0.67484921\n",
      " 0.83056855 0.95422149 0.83855808 0.79382682 0.8998009  0.89147794\n",
      " 0.94773167 0.88593376 0.88638908 0.93716639 0.95607817 0.49824065\n",
      " 0.58317083 0.73963225 0.96822596 0.97050202 0.92664176 0.97480041\n",
      " 0.958965   0.64189893 0.54809666 0.99667394 0.66437924 0.9199273\n",
      " 0.61405253 0.80636215 0.71223962 0.89855486 0.46071923 0.55262017\n",
      " 0.45098191 0.2925747  0.4295812  0.53148299 0.38149771 0.4420892\n",
      " 0.93120527 0.47778237 0.88612109 0.83494854 0.94025928 0.93585569\n",
      " 0.92914993 0.90091646 0.74703711 0.96958935]\n",
      "predict [0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 92 [0/43 (0%)]\tTrain Loss: 0.079888\n",
      "Train Epoch: 92 [10/43 (23%)]\tTrain Loss: 0.064403\n",
      "Train Epoch: 92 [20/43 (47%)]\tTrain Loss: 0.104694\n",
      "Train Epoch: 92 [30/43 (70%)]\tTrain Loss: 0.055155\n",
      "Train Epoch: 92 [40/43 (93%)]\tTrain Loss: 0.058292\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.57795167 0.94796646 0.91842127 0.87203872 0.70535362 0.79613292\n",
      " 0.89815992 0.45121858 0.85028911 0.71537507 0.68952656 0.53709304\n",
      " 0.86265534 0.87077713 0.57084543 0.09250284 0.49179518 0.29085428\n",
      " 0.76629674 0.82441574 0.75460017 0.77865857 0.98820823 0.93931466\n",
      " 0.55434608 0.840918   0.6748606  0.71770746 0.67375505 0.80170888\n",
      " 0.89189124 0.64988393 0.50257331 0.64114225 0.05466469 0.35435018\n",
      " 0.18671148 0.51599514 0.43293309 0.4254328  0.31143576 0.08520389\n",
      " 0.85147291 0.3009094  0.63113415 0.39104444 0.76273102 0.31237364\n",
      " 0.84732062 0.67937535 0.69868165 0.54272223 0.81289238 0.12643296\n",
      " 0.66220856 0.84889424 0.85419899 0.149229   0.66996008 0.33257002\n",
      " 0.88923699 0.87479436 0.92649692 0.96668428 0.84169251 0.75186914\n",
      " 0.27529585 0.99523586 0.99454224 0.65581864 0.84679741 0.3598572\n",
      " 0.59425408 0.77085942 0.63711983 0.81170386 0.98798078 0.98663121\n",
      " 0.95663011 0.83582902 0.9285835  0.92643452 0.94437039 0.27876195\n",
      " 0.53164613 0.79539764 0.93823045 0.97826684 0.96841919 0.97967118\n",
      " 0.98558182 0.58631039 0.48957857 0.98311275 0.98134547 0.64188516\n",
      " 0.56552982 0.85540712 0.84700644 0.97026104 0.18118547 0.83713222\n",
      " 0.22990938 0.22283299 0.38866192 0.26964214 0.65823925 0.72660822\n",
      " 0.65675676 0.74283206 0.79151976 0.51556444 0.82227516 0.81061453\n",
      " 0.80807167 0.78665978 0.75996131 0.94349039]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 93 [0/43 (0%)]\tTrain Loss: 0.048751\n",
      "Train Epoch: 93 [10/43 (23%)]\tTrain Loss: 0.037783\n",
      "Train Epoch: 93 [20/43 (47%)]\tTrain Loss: 0.071161\n",
      "Train Epoch: 93 [30/43 (70%)]\tTrain Loss: 0.039557\n",
      "Train Epoch: 93 [40/43 (93%)]\tTrain Loss: 0.048945\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.77762544 0.9158662  0.93115896 0.85414034 0.86393851 0.8673172\n",
      " 0.92785406 0.88445508 0.78198195 0.87143314 0.79851794 0.85781121\n",
      " 0.84594047 0.98118442 0.61301851 0.43700615 0.8202219  0.28164551\n",
      " 0.46104318 0.45580202 0.49603516 0.40447342 0.92654371 0.91466057\n",
      " 0.38550708 0.93418056 0.9515292  0.72381973 0.35559836 0.80973583\n",
      " 0.90234578 0.70604765 0.26722029 0.17798291 0.36686343 0.48852739\n",
      " 0.38798562 0.7059164  0.5038572  0.3546094  0.3447938  0.45880634\n",
      " 0.68119806 0.4844375  0.58812273 0.77115673 0.77272207 0.53895372\n",
      " 0.82392782 0.60533631 0.62786132 0.8413325  0.73681766 0.48949173\n",
      " 0.76422113 0.64329535 0.45204017 0.36099088 0.78188705 0.57409555\n",
      " 0.6812942  0.57126647 0.82086295 0.73212135 0.89466107 0.886397\n",
      " 0.87200236 0.98244518 0.98238599 0.8339467  0.57566148 0.55062282\n",
      " 0.8442933  0.97517097 0.90330046 0.56470132 0.9265359  0.96762288\n",
      " 0.98010522 0.94487095 0.92157936 0.88624287 0.94638681 0.1573576\n",
      " 0.43645161 0.46243188 0.8598693  0.86135721 0.82380241 0.9132728\n",
      " 0.98385435 0.87713981 0.81466585 0.99888259 0.96109313 0.99666947\n",
      " 0.78226823 0.55520058 0.37209192 0.95919192 0.22045745 0.60528922\n",
      " 0.40115681 0.2386144  0.43755811 0.58635092 0.79734534 0.73763269\n",
      " 0.79694492 0.70582014 0.93232042 0.89698029 0.9805665  0.95009011\n",
      " 0.83539999 0.26583993 0.81112933 0.9066844 ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 94 [0/43 (0%)]\tTrain Loss: 0.042385\n",
      "Train Epoch: 94 [10/43 (23%)]\tTrain Loss: 0.062160\n",
      "Train Epoch: 94 [20/43 (47%)]\tTrain Loss: 0.037853\n",
      "Train Epoch: 94 [30/43 (70%)]\tTrain Loss: 0.048380\n",
      "Train Epoch: 94 [40/43 (93%)]\tTrain Loss: 0.052313\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.67544019 0.93493658 0.94302231 0.69327277 0.64411664 0.61378169\n",
      " 0.83862633 0.3692266  0.65723985 0.54966402 0.75964797 0.69917363\n",
      " 0.86443174 0.99397212 0.8100003  0.14219473 0.27280113 0.46895069\n",
      " 0.67882276 0.76769876 0.44198853 0.92041993 0.98789805 0.39588374\n",
      " 0.84095818 0.92001367 0.68413436 0.91908455 0.57301849 0.74707949\n",
      " 0.96615535 0.79017776 0.48246279 0.79942441 0.21403214 0.15381725\n",
      " 0.29491633 0.84986776 0.68434358 0.27742425 0.60948545 0.28886434\n",
      " 0.57764304 0.272185   0.43982986 0.62670553 0.44283661 0.22769628\n",
      " 0.79421782 0.73115176 0.49397498 0.77492583 0.58470446 0.33515576\n",
      " 0.5226174  0.45519534 0.75854254 0.36602205 0.49117318 0.43754029\n",
      " 0.95393872 0.99427313 0.96711463 0.98892599 0.92371225 0.89044386\n",
      " 0.65662855 0.99967885 0.99862289 0.66158921 0.90308505 0.82764798\n",
      " 0.87863868 0.93034649 0.78898424 0.97229439 0.98715848 0.99683869\n",
      " 0.98928738 0.93861985 0.9723354  0.98344111 0.98390847 0.89699507\n",
      " 0.6649375  0.69131517 0.98837602 0.99808717 0.98908895 0.99838591\n",
      " 0.99653745 0.62119228 0.83824915 0.99690717 0.70225662 0.76320934\n",
      " 0.67922682 0.972785   0.89843965 0.99806148 0.26846543 0.83713663\n",
      " 0.57430059 0.40258509 0.6866855  0.93855035 0.51404023 0.4519811\n",
      " 0.93782759 0.85842508 0.92193192 0.69199681 0.98359066 0.92371774\n",
      " 0.99489957 0.98505276 0.93901956 0.96601993]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [0/43 (0%)]\tTrain Loss: 0.031354\n",
      "Train Epoch: 95 [10/43 (23%)]\tTrain Loss: 0.055909\n",
      "Train Epoch: 95 [20/43 (47%)]\tTrain Loss: 0.052812\n",
      "Train Epoch: 95 [30/43 (70%)]\tTrain Loss: 0.071707\n",
      "Train Epoch: 95 [40/43 (93%)]\tTrain Loss: 0.047039\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.56366295 0.4952687  0.47146404 0.36025229 0.38563418 0.30765226\n",
      " 0.4296051  0.1841405  0.17132656 0.66524053 0.41879636 0.29252762\n",
      " 0.16955346 0.48923671 0.10495447 0.10044221 0.23465858 0.61451566\n",
      " 0.33135048 0.26947609 0.40234837 0.50173163 0.9629584  0.68552834\n",
      " 0.3444964  0.95993608 0.38352334 0.29424152 0.21518537 0.35351348\n",
      " 0.72507405 0.54342228 0.25393152 0.15570116 0.08529425 0.07619869\n",
      " 0.05302753 0.10038871 0.48864749 0.60556883 0.45303065 0.39061278\n",
      " 0.23723486 0.08480363 0.19541752 0.29693753 0.49234074 0.4176394\n",
      " 0.62367404 0.56257898 0.64882082 0.04960356 0.09348144 0.06169822\n",
      " 0.31479555 0.06216161 0.55253452 0.03275058 0.33466327 0.09146529\n",
      " 0.87355083 0.79613674 0.89694971 0.8915422  0.37854755 0.46776408\n",
      " 0.67102426 0.99416828 0.96075779 0.52729636 0.72923225 0.77378631\n",
      " 0.67144305 0.83699316 0.3928861  0.80871958 0.97905248 0.98126924\n",
      " 0.91845363 0.91964924 0.88728625 0.95800525 0.95103186 0.77546871\n",
      " 0.59007215 0.80458462 0.92614454 0.97081238 0.6192916  0.96524197\n",
      " 0.9664818  0.40808085 0.13244723 0.98211664 0.73825288 0.96619451\n",
      " 0.26526618 0.92218113 0.92962396 0.97622323 0.47837603 0.75587529\n",
      " 0.55880606 0.09845056 0.58773988 0.87943631 0.62733096 0.15950969\n",
      " 0.79526955 0.29700419 0.72751111 0.72036433 0.92436248 0.66527569\n",
      " 0.90250152 0.78542686 0.52513069 0.79595888]\n",
      "predict [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 96 [0/43 (0%)]\tTrain Loss: 0.053136\n",
      "Train Epoch: 96 [10/43 (23%)]\tTrain Loss: 0.093764\n",
      "Train Epoch: 96 [20/43 (47%)]\tTrain Loss: 0.045013\n",
      "Train Epoch: 96 [30/43 (70%)]\tTrain Loss: 0.068818\n",
      "Train Epoch: 96 [40/43 (93%)]\tTrain Loss: 0.055442\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.43991289 0.74495387 0.67484671 0.53421438 0.49856436 0.55764204\n",
      " 0.5224449  0.27142012 0.58809698 0.45107201 0.26800296 0.3223455\n",
      " 0.33930218 0.58904874 0.23043433 0.14475538 0.1243333  0.47954053\n",
      " 0.46319887 0.35534081 0.71383041 0.35704103 0.95615458 0.53742462\n",
      " 0.43896464 0.77083254 0.49158937 0.34099939 0.2625559  0.53237504\n",
      " 0.83226657 0.76454258 0.72792929 0.18482898 0.42224985 0.15238836\n",
      " 0.05706111 0.33568406 0.48916823 0.40206969 0.42311165 0.19317041\n",
      " 0.5061391  0.13264526 0.17057233 0.18922786 0.35770297 0.58137286\n",
      " 0.66770273 0.49009171 0.36930794 0.10972235 0.09476196 0.09037703\n",
      " 0.48545855 0.05186261 0.28002885 0.04031022 0.52992576 0.08660596\n",
      " 0.81570935 0.77028507 0.87287354 0.94328606 0.78380454 0.85301149\n",
      " 0.84775579 0.99764037 0.98905575 0.58413768 0.5156579  0.69671237\n",
      " 0.71407211 0.81868494 0.51060516 0.89568216 0.97846597 0.97913647\n",
      " 0.98825538 0.97639614 0.92468101 0.95339835 0.97620732 0.63471836\n",
      " 0.57168305 0.59012663 0.80356014 0.93524867 0.55549699 0.87827516\n",
      " 0.95953345 0.32975978 0.28894186 0.87191445 0.41420543 0.93142867\n",
      " 0.32158214 0.40167582 0.53610724 0.93903911 0.1945862  0.86407948\n",
      " 0.17018555 0.1097874  0.35128191 0.51162344 0.55807996 0.43101153\n",
      " 0.53491968 0.53958398 0.33636856 0.40649194 0.95455343 0.85709977\n",
      " 0.74173445 0.78914493 0.73459685 0.95546913]\n",
      "predict [0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 97 [0/43 (0%)]\tTrain Loss: 0.062256\n",
      "Train Epoch: 97 [10/43 (23%)]\tTrain Loss: 0.047697\n",
      "Train Epoch: 97 [20/43 (47%)]\tTrain Loss: 0.065597\n",
      "Train Epoch: 97 [30/43 (70%)]\tTrain Loss: 0.040300\n",
      "Train Epoch: 97 [40/43 (93%)]\tTrain Loss: 0.048360\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.15319854 0.86482155 0.86090672 0.66528672 0.55870312 0.41014981\n",
      " 0.92670441 0.41672152 0.3652868  0.47261009 0.55703688 0.41478023\n",
      " 0.4508394  0.90944886 0.6892646  0.08148848 0.3417674  0.37369066\n",
      " 0.46202129 0.5453732  0.55943799 0.73949277 0.77485859 0.05480664\n",
      " 0.78744698 0.91204679 0.11379439 0.74266356 0.23061493 0.80891222\n",
      " 0.88586706 0.67134953 0.58002609 0.14725044 0.12971857 0.23462425\n",
      " 0.21620063 0.69652086 0.36740211 0.21550101 0.4168686  0.19370539\n",
      " 0.42696419 0.29708838 0.41552195 0.30122653 0.4189623  0.44418937\n",
      " 0.67105621 0.15893498 0.45653385 0.09424929 0.25797907 0.10990845\n",
      " 0.08374582 0.09598323 0.51570195 0.0821774  0.34722668 0.15913767\n",
      " 0.14029506 0.08386824 0.12634403 0.10653885 0.75153577 0.5917747\n",
      " 0.57241195 0.97208101 0.92627811 0.53460288 0.17903371 0.1694859\n",
      " 0.57489127 0.84466219 0.56931978 0.86925608 0.97623289 0.96443862\n",
      " 0.9684692  0.92406577 0.82616591 0.89014775 0.26515782 0.13675997\n",
      " 0.10411289 0.21012466 0.0493294  0.11845454 0.14011247 0.96998918\n",
      " 0.1286293  0.38794005 0.40265122 0.79295945 0.65785789 0.91644132\n",
      " 0.50695252 0.75913525 0.68587333 0.93585718 0.18526003 0.67280281\n",
      " 0.32832578 0.13851726 0.48708865 0.53697139 0.14153518 0.16013327\n",
      " 0.6820662  0.17718135 0.45355883 0.27286446 0.89065212 0.81850451\n",
      " 0.37950188 0.91274655 0.80441558 0.80620033]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
      "Train Epoch: 98 [0/43 (0%)]\tTrain Loss: 0.048191\n",
      "Train Epoch: 98 [10/43 (23%)]\tTrain Loss: 0.039905\n",
      "Train Epoch: 98 [20/43 (47%)]\tTrain Loss: 0.034628\n",
      "Train Epoch: 98 [30/43 (70%)]\tTrain Loss: 0.043175\n",
      "Train Epoch: 98 [40/43 (93%)]\tTrain Loss: 0.027933\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.31988391 0.87050146 0.93279761 0.81499165 0.75675035 0.71801901\n",
      " 0.92874891 0.81140095 0.61069828 0.5497824  0.52231681 0.35928407\n",
      " 0.73060745 0.88464898 0.94814503 0.28694394 0.64951152 0.60786539\n",
      " 0.19357027 0.13280684 0.22039102 0.93197829 0.92890751 0.14179508\n",
      " 0.77932358 0.21201225 0.10092589 0.83720601 0.47718623 0.7193076\n",
      " 0.87936056 0.92132974 0.18524085 0.12537394 0.15684779 0.18503848\n",
      " 0.47407061 0.87908638 0.48735011 0.33269745 0.52507198 0.25484359\n",
      " 0.73424894 0.3868435  0.7052961  0.52570057 0.36555117 0.30235305\n",
      " 0.85356492 0.07029727 0.76569378 0.12928408 0.43587881 0.41227663\n",
      " 0.12335207 0.59184939 0.63907593 0.36761901 0.47068539 0.65513211\n",
      " 0.93772525 0.16610815 0.91693425 0.9339838  0.89214486 0.9160623\n",
      " 0.83285683 0.97938079 0.9665435  0.09744417 0.78005135 0.50872445\n",
      " 0.67742985 0.95402759 0.85778159 0.90781009 0.97698319 0.949341\n",
      " 0.96711391 0.9110347  0.88273478 0.92788291 0.91545492 0.12623538\n",
      " 0.76735377 0.75872582 0.12821469 0.84386533 0.93611103 0.92751491\n",
      " 0.33633545 0.69633079 0.748734   0.97285992 0.87672275 0.91074795\n",
      " 0.7556029  0.70922554 0.54151422 0.96276289 0.18367717 0.76439518\n",
      " 0.50259018 0.28905338 0.21449183 0.63564688 0.14504904 0.09043569\n",
      " 0.9037959  0.58347398 0.93345892 0.54786831 0.89664155 0.85652465\n",
      " 0.56882602 0.82561439 0.87045711 0.86687601]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99 [0/43 (0%)]\tTrain Loss: 0.039843\n",
      "Train Epoch: 99 [10/43 (23%)]\tTrain Loss: 0.032758\n",
      "Train Epoch: 99 [20/43 (47%)]\tTrain Loss: 0.048547\n",
      "Train Epoch: 99 [30/43 (70%)]\tTrain Loss: 0.046163\n",
      "Train Epoch: 99 [40/43 (93%)]\tTrain Loss: 0.053237\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.80540848 0.57467812 0.55203372 0.95650214 0.82177335 0.92032027\n",
      " 0.39457589 0.21222876 0.9096694  0.54889536 0.40212288 0.68547785\n",
      " 0.33526105 0.97319818 0.57936877 0.9542262  0.15374672 0.35880074\n",
      " 0.90977681 0.85536683 0.89374685 0.53790879 0.97975004 0.88555348\n",
      " 0.86665279 0.9003849  0.90704542 0.91195542 0.29229215 0.39980555\n",
      " 0.84368366 0.55833352 0.3924202  0.70593429 0.94686568 0.92774737\n",
      " 0.07780087 0.88383466 0.55954665 0.26025414 0.21971959 0.13659616\n",
      " 0.89267564 0.23247848 0.14305447 0.28251016 0.64326692 0.31435323\n",
      " 0.76597756 0.65296251 0.8532089  0.46694204 0.10680191 0.8968119\n",
      " 0.86566108 0.10530938 0.32675231 0.04922976 0.78934258 0.13041632\n",
      " 0.98750371 0.97818094 0.98496938 0.99146783 0.39222759 0.67418492\n",
      " 0.47952935 0.99960047 0.94616938 0.92628926 0.84592104 0.9058333\n",
      " 0.75339288 0.83930969 0.50216049 0.79364061 0.97226667 0.92925417\n",
      " 0.99118489 0.95283449 0.95314777 0.98447651 0.98003483 0.46936724\n",
      " 0.38966578 0.79902828 0.93985885 0.99235755 0.81966138 0.99634856\n",
      " 0.99375772 0.42053959 0.3525019  0.94400012 0.9380154  0.89510787\n",
      " 0.94292182 0.90541446 0.87647474 0.97581249 0.95358258 0.9757278\n",
      " 0.1745159  0.11192179 0.22783436 0.9157452  0.78581393 0.86238354\n",
      " 0.87579691 0.90461051 0.7021814  0.93675798 0.95114166 0.92218745\n",
      " 0.98194087 0.97497493 0.7784217  0.95881015]\n",
      "predict [1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 100 [0/43 (0%)]\tTrain Loss: 0.073975\n",
      "Train Epoch: 100 [10/43 (23%)]\tTrain Loss: 0.067038\n",
      "Train Epoch: 100 [20/43 (47%)]\tTrain Loss: 0.032031\n",
      "Train Epoch: 100 [30/43 (70%)]\tTrain Loss: 0.072456\n",
      "Train Epoch: 100 [40/43 (93%)]\tTrain Loss: 0.025667\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.10103908 0.71358675 0.59220403 0.01683187 0.03121496 0.07600677\n",
      " 0.52658612 0.05309193 0.03715071 0.04128614 0.04753239 0.05223612\n",
      " 0.68343991 0.11386093 0.12933092 0.02404834 0.22648221 0.26573968\n",
      " 0.15061013 0.33532566 0.21054029 0.41273361 0.75642121 0.92412907\n",
      " 0.53149909 0.87204796 0.78740084 0.3607797  0.1224332  0.18387513\n",
      " 0.84379631 0.64394951 0.0627328  0.03563312 0.08318994 0.10457528\n",
      " 0.13384888 0.15850356 0.50526631 0.08065967 0.08803361 0.28965798\n",
      " 0.03154501 0.20153756 0.34987035 0.52124143 0.05730663 0.36806193\n",
      " 0.02500295 0.08265554 0.02347321 0.02785816 0.09961476 0.05362641\n",
      " 0.10215216 0.04745567 0.44597283 0.04306926 0.07755323 0.15196185\n",
      " 0.60720831 0.78697568 0.91167897 0.94736242 0.73377115 0.4874548\n",
      " 0.41794699 0.94541675 0.91167164 0.06144531 0.61919028 0.40484667\n",
      " 0.47801694 0.6021564  0.23684405 0.69367015 0.97318268 0.95233363\n",
      " 0.93522406 0.81788284 0.84308642 0.89705384 0.97007769 0.24132493\n",
      " 0.32232931 0.67046374 0.58880597 0.76248783 0.44054767 0.88412833\n",
      " 0.64660251 0.09686533 0.520208   0.92219067 0.78352624 0.82480884\n",
      " 0.05400082 0.43576732 0.40520081 0.80480111 0.09488615 0.75681651\n",
      " 0.18481392 0.07499024 0.36015743 0.11759344 0.22547235 0.19059862\n",
      " 0.39739886 0.58376759 0.73899978 0.32847762 0.59737068 0.75008053\n",
      " 0.60213685 0.63545412 0.57981795 0.89065081]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 50 TN= 35 FN= 8 FP= 25\n",
      "TP+FP 75\n",
      "precision 0.6666666666666666\n",
      "recall 0.8620689655172413\n",
      "F1 0.7518796992481203\n",
      "acc 0.7203389830508474\n",
      "AUCp 0.7227011494252873\n",
      "AUC 0.8135057471264368\n",
      "\n",
      " The epoch is 100, average recall: 0.8621, average precision: 0.6667,average F1: 0.7519, average accuracy: 0.7203, average AUC: 0.8135\n",
      "Train Epoch: 101 [0/43 (0%)]\tTrain Loss: 0.069778\n",
      "Train Epoch: 101 [10/43 (23%)]\tTrain Loss: 0.036397\n",
      "Train Epoch: 101 [20/43 (47%)]\tTrain Loss: 0.045101\n",
      "Train Epoch: 101 [30/43 (70%)]\tTrain Loss: 0.053124\n",
      "Train Epoch: 101 [40/43 (93%)]\tTrain Loss: 0.035974\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.00351105 0.45288017 0.32916841 0.00947408 0.00641203 0.00489451\n",
      " 0.20981802 0.00443022 0.00374015 0.01161121 0.00823854 0.01322445\n",
      " 0.06810036 0.18443121 0.01968525 0.00547557 0.10940541 0.41374889\n",
      " 0.25398162 0.466607   0.32785088 0.75383127 0.69610047 0.55164969\n",
      " 0.56744057 0.83773118 0.76369202 0.6533649  0.14482038 0.33413553\n",
      " 0.74076211 0.55193621 0.01626687 0.00823181 0.00833842 0.00271103\n",
      " 0.02000537 0.27476412 0.77574658 0.20981568 0.26254943 0.4340283\n",
      " 0.00580581 0.13775167 0.08858263 0.16521925 0.00574297 0.0445028\n",
      " 0.0027694  0.01236295 0.01016038 0.01045309 0.00444298 0.00236066\n",
      " 0.00278831 0.00122349 0.40597752 0.00319413 0.01506861 0.01159974\n",
      " 0.62489575 0.36015338 0.82518739 0.7412591  0.55703962 0.40431058\n",
      " 0.16939047 0.59620351 0.74419445 0.00751737 0.64800394 0.38779575\n",
      " 0.21181038 0.44386032 0.16360803 0.85199726 0.91406453 0.91766703\n",
      " 0.83989137 0.79745841 0.59121108 0.89299762 0.83452874 0.1475679\n",
      " 0.51875699 0.670075   0.37824562 0.68650734 0.17674752 0.48750851\n",
      " 0.77396905 0.00999225 0.21528308 0.58932704 0.2284586  0.86483258\n",
      " 0.00803671 0.19038355 0.50809723 0.81227559 0.12160406 0.49608034\n",
      " 0.05254823 0.14508712 0.06734155 0.20280398 0.00705841 0.19493932\n",
      " 0.44038129 0.21343851 0.5998351  0.11457343 0.3384015  0.7176131\n",
      " 0.71867007 0.69824553 0.71041244 0.87190658]\n",
      "predict [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 102 [0/43 (0%)]\tTrain Loss: 0.029476\n",
      "Train Epoch: 102 [10/43 (23%)]\tTrain Loss: 0.047212\n",
      "Train Epoch: 102 [20/43 (47%)]\tTrain Loss: 0.026629\n",
      "Train Epoch: 102 [30/43 (70%)]\tTrain Loss: 0.067601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 102 [40/43 (93%)]\tTrain Loss: 0.089310\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.28132665 0.70657766 0.65196812 0.663634   0.5446164  0.32044426\n",
      " 0.61727095 0.72887123 0.59830165 0.19670132 0.18663611 0.21667606\n",
      " 0.73956436 0.62893754 0.44793794 0.10188171 0.37671342 0.37785923\n",
      " 0.15888153 0.51667881 0.3670018  0.77067524 0.91320103 0.8819499\n",
      " 0.43258715 0.89371914 0.84015852 0.68406725 0.29816583 0.50790679\n",
      " 0.92314517 0.51072615 0.06720945 0.10148292 0.0140509  0.02661539\n",
      " 0.13637114 0.62972873 0.66714334 0.08913884 0.34123826 0.3185057\n",
      " 0.13596712 0.22463077 0.33622712 0.26584014 0.1829536  0.12805405\n",
      " 0.29922491 0.26938781 0.21254946 0.07268979 0.10364293 0.0438759\n",
      " 0.12321199 0.04523542 0.48327795 0.01335227 0.29366469 0.18492919\n",
      " 0.77286279 0.60207975 0.89828354 0.94760215 0.85261589 0.90350848\n",
      " 0.63743311 0.98554021 0.97823501 0.70781243 0.59483069 0.61601233\n",
      " 0.69016308 0.92478806 0.57083172 0.91708833 0.98451149 0.98258406\n",
      " 0.96507818 0.96264964 0.79104489 0.97787356 0.96125269 0.17922679\n",
      " 0.41998082 0.76688933 0.52736026 0.66326642 0.60784894 0.94080198\n",
      " 0.99551845 0.70404917 0.69891095 0.97354579 0.69985747 0.96309084\n",
      " 0.17482738 0.32381022 0.53887182 0.92221147 0.04958346 0.75395167\n",
      " 0.14497645 0.25357696 0.18406011 0.23787934 0.67303795 0.20168646\n",
      " 0.81838113 0.48738015 0.68819577 0.21181528 0.86828488 0.93732166\n",
      " 0.94788533 0.64136398 0.86926317 0.9027797 ]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 103 [0/43 (0%)]\tTrain Loss: 0.043032\n",
      "Train Epoch: 103 [10/43 (23%)]\tTrain Loss: 0.038135\n",
      "Train Epoch: 103 [20/43 (47%)]\tTrain Loss: 0.030388\n",
      "Train Epoch: 103 [30/43 (70%)]\tTrain Loss: 0.040313\n",
      "Train Epoch: 103 [40/43 (93%)]\tTrain Loss: 0.025786\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.13020384 0.8577463  0.93073732 0.72077847 0.355057   0.18708043\n",
      " 0.89046729 0.44117516 0.13829245 0.10360406 0.71403015 0.10231404\n",
      " 0.70232421 0.59352028 0.406863   0.02824314 0.06428237 0.44194534\n",
      " 0.2373914  0.50154996 0.30868092 0.5864737  0.9739576  0.88882005\n",
      " 0.61525786 0.95279348 0.80239087 0.50752163 0.36957246 0.61477649\n",
      " 0.89529616 0.52577829 0.28693014 0.05189138 0.02645157 0.08024539\n",
      " 0.20741192 0.62953442 0.56422693 0.28094161 0.22778837 0.23880889\n",
      " 0.52848691 0.32227835 0.42181188 0.28910348 0.04945736 0.4341051\n",
      " 0.86571681 0.06393653 0.05699053 0.0702601  0.04033596 0.05864472\n",
      " 0.20545721 0.0668081  0.57167327 0.03911107 0.04785673 0.07213978\n",
      " 0.92021722 0.77376443 0.89021611 0.95433986 0.83287132 0.51491958\n",
      " 0.21949728 0.99095309 0.95683193 0.56277096 0.76948625 0.84659153\n",
      " 0.40191621 0.70479584 0.16391338 0.94666088 0.987818   0.98153895\n",
      " 0.97362238 0.95104146 0.91729522 0.95960218 0.97029459 0.78824759\n",
      " 0.44746235 0.84514368 0.9579652  0.87913501 0.64121312 0.88493633\n",
      " 0.86126012 0.60824019 0.52866393 0.94750571 0.77795607 0.85846102\n",
      " 0.08994444 0.8425433  0.78652143 0.87601846 0.56610572 0.89487535\n",
      " 0.53770506 0.09087589 0.16015393 0.54032242 0.80404735 0.23969509\n",
      " 0.84034938 0.40584952 0.67691928 0.38906616 0.95820284 0.8395015\n",
      " 0.92095077 0.87857455 0.87312955 0.91779554]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 104 [0/43 (0%)]\tTrain Loss: 0.038865\n",
      "Train Epoch: 104 [10/43 (23%)]\tTrain Loss: 0.053704\n",
      "Train Epoch: 104 [20/43 (47%)]\tTrain Loss: 0.051155\n",
      "Train Epoch: 104 [30/43 (70%)]\tTrain Loss: 0.062665\n",
      "Train Epoch: 104 [40/43 (93%)]\tTrain Loss: 0.068773\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.09695127 0.90335339 0.92236984 0.72718257 0.18496515 0.18106574\n",
      " 0.83805794 0.80120152 0.67821091 0.10687207 0.39789635 0.12013774\n",
      " 0.78989267 0.9688983  0.94657177 0.81870699 0.81346941 0.53408128\n",
      " 0.39054468 0.38883656 0.20012319 0.70940781 0.89485198 0.64481962\n",
      " 0.59050876 0.76465243 0.23725624 0.64150423 0.57484168 0.59923661\n",
      " 0.78010005 0.60066766 0.52506566 0.20113496 0.42415816 0.14213715\n",
      " 0.25672927 0.37530059 0.15359788 0.40194836 0.23333342 0.46431702\n",
      " 0.1324098  0.45544639 0.28850633 0.59499836 0.18253504 0.39755455\n",
      " 0.60013765 0.18674944 0.3096852  0.37593377 0.42803246 0.48009181\n",
      " 0.13360611 0.41789529 0.62008554 0.43741664 0.15194492 0.6532585\n",
      " 0.72512996 0.89433223 0.79613066 0.85087591 0.65128005 0.86892545\n",
      " 0.67976213 0.94621909 0.95043278 0.47229561 0.70319825 0.75418401\n",
      " 0.80522865 0.81950843 0.79842597 0.81092471 0.9791311  0.98139924\n",
      " 0.95459974 0.82069159 0.86667466 0.87094504 0.84935623 0.58905965\n",
      " 0.5751527  0.65063184 0.82351387 0.90813172 0.66530299 0.74569064\n",
      " 0.94196212 0.76203269 0.62582749 0.91013253 0.21676701 0.20139083\n",
      " 0.15084983 0.74136937 0.55134606 0.83704817 0.4359071  0.62321329\n",
      " 0.54119623 0.34825176 0.47296295 0.60601103 0.22348674 0.30363619\n",
      " 0.58163929 0.46713752 0.48271549 0.09869838 0.46509203 0.66539568\n",
      " 0.89815533 0.82685935 0.8913855  0.84299815]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 105 [0/43 (0%)]\tTrain Loss: 0.065424\n",
      "Train Epoch: 105 [10/43 (23%)]\tTrain Loss: 0.061357\n",
      "Train Epoch: 105 [20/43 (47%)]\tTrain Loss: 0.091359\n",
      "Train Epoch: 105 [30/43 (70%)]\tTrain Loss: 0.074137\n",
      "Train Epoch: 105 [40/43 (93%)]\tTrain Loss: 0.032572\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.36346343 0.88910729 0.93235058 0.67731655 0.35231951 0.30957609\n",
      " 0.81819582 0.62882668 0.36830744 0.28208667 0.50936794 0.2568689\n",
      " 0.73294932 0.97503847 0.90238082 0.44571349 0.43662003 0.51821202\n",
      " 0.49149975 0.30027494 0.40737891 0.83157182 0.91434902 0.3611097\n",
      " 0.75487536 0.97760326 0.44964325 0.68313289 0.24244729 0.44126719\n",
      " 0.88645363 0.74513769 0.36037976 0.36867234 0.07926139 0.10843171\n",
      " 0.27391714 0.58158076 0.26199698 0.26876557 0.51557511 0.2046795\n",
      " 0.38105947 0.24888135 0.27410758 0.49783558 0.30918398 0.50559521\n",
      " 0.68983608 0.38149029 0.5206297  0.21782994 0.12933429 0.18950288\n",
      " 0.29866341 0.18478398 0.35099301 0.25321975 0.34747669 0.23256396\n",
      " 0.85764676 0.7704587  0.8964563  0.95894015 0.54335672 0.49571311\n",
      " 0.49878794 0.99405503 0.97411942 0.87427515 0.66361713 0.70610893\n",
      " 0.62919801 0.82479846 0.71297181 0.87541187 0.98681575 0.98724174\n",
      " 0.97296745 0.88171792 0.93382192 0.96512192 0.94294506 0.63518155\n",
      " 0.53010255 0.76870698 0.98139518 0.95031303 0.79330462 0.99075341\n",
      " 0.98289686 0.64914954 0.54706424 0.97928029 0.34562311 0.36630902\n",
      " 0.28657886 0.78269792 0.81025255 0.9565872  0.29989064 0.61375004\n",
      " 0.54022962 0.15670718 0.50373608 0.27175298 0.27441436 0.62330759\n",
      " 0.94675982 0.40448436 0.88399053 0.341344   0.89940447 0.88207257\n",
      " 0.97687769 0.90397853 0.88114905 0.89978325]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 106 [0/43 (0%)]\tTrain Loss: 0.029084\n",
      "Train Epoch: 106 [10/43 (23%)]\tTrain Loss: 0.058230\n",
      "Train Epoch: 106 [20/43 (47%)]\tTrain Loss: 0.050735\n",
      "Train Epoch: 106 [30/43 (70%)]\tTrain Loss: 0.024961\n",
      "Train Epoch: 106 [40/43 (93%)]\tTrain Loss: 0.062231\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.0561747  0.9129768  0.85790318 0.60662979 0.16665858 0.13267061\n",
      " 0.86772883 0.66279918 0.54896456 0.10240562 0.43429109 0.17218356\n",
      " 0.72871923 0.91613013 0.94699633 0.38518697 0.58785611 0.48549312\n",
      " 0.94055998 0.62303197 0.45801148 0.87040961 0.99049395 0.87965113\n",
      " 0.83126467 0.92346597 0.15396266 0.84386009 0.7419427  0.80238187\n",
      " 0.9111293  0.80269933 0.5357753  0.13258654 0.16663794 0.31706467\n",
      " 0.3415353  0.38251367 0.50957137 0.31310549 0.62737668 0.73397362\n",
      " 0.09652946 0.38460523 0.76562935 0.76309848 0.24457821 0.24536321\n",
      " 0.62550145 0.17747609 0.52800047 0.2144587  0.29457989 0.16286792\n",
      " 0.13475914 0.28036597 0.30828333 0.25174838 0.06880495 0.41083512\n",
      " 0.88818508 0.71041632 0.93186772 0.9664551  0.84153295 0.84977102\n",
      " 0.74017251 0.98839372 0.97177482 0.80792201 0.55720258 0.80281061\n",
      " 0.7120592  0.84904957 0.87826616 0.85753524 0.98735422 0.98045415\n",
      " 0.98711824 0.94902223 0.94323665 0.93958277 0.96050984 0.67060119\n",
      " 0.63595462 0.85191488 0.92836761 0.92467755 0.81573355 0.97541898\n",
      " 0.92925358 0.62306219 0.82570851 0.98899359 0.50152916 0.76980752\n",
      " 0.15123501 0.87242627 0.81923616 0.93484038 0.27787933 0.80980307\n",
      " 0.60110843 0.43657166 0.31690022 0.70186681 0.15418018 0.50187749\n",
      " 0.91291857 0.72620517 0.89291197 0.60631025 0.90591854 0.95547116\n",
      " 0.93221027 0.91148758 0.86353743 0.96419948]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 107 [0/43 (0%)]\tTrain Loss: 0.038537\n",
      "Train Epoch: 107 [10/43 (23%)]\tTrain Loss: 0.060831\n",
      "Train Epoch: 107 [20/43 (47%)]\tTrain Loss: 0.025075\n",
      "Train Epoch: 107 [30/43 (70%)]\tTrain Loss: 0.036720\n",
      "Train Epoch: 107 [40/43 (93%)]\tTrain Loss: 0.063655\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.16907078 0.29245555 0.37107456 0.69732928 0.23230802 0.31273347\n",
      " 0.39096233 0.59489548 0.22878745 0.24844366 0.49375013 0.10362235\n",
      " 0.69045329 0.93828589 0.70101166 0.21236725 0.35718992 0.24251112\n",
      " 0.13434382 0.13433877 0.15939556 0.70354688 0.95902359 0.29175398\n",
      " 0.6530239  0.45656258 0.1952592  0.6144188  0.1083646  0.33702919\n",
      " 0.95470273 0.3024461  0.09561067 0.06725916 0.04857887 0.0513014\n",
      " 0.07578801 0.23202842 0.24270497 0.25787494 0.24599291 0.2269789\n",
      " 0.13815036 0.13150562 0.26333481 0.56780171 0.15994555 0.21800984\n",
      " 0.81907368 0.12134724 0.27506337 0.09717799 0.06609178 0.18366359\n",
      " 0.0953398  0.05535375 0.37667987 0.02985062 0.35865131 0.16760331\n",
      " 0.84747314 0.76178855 0.93335581 0.96322733 0.63765532 0.40551072\n",
      " 0.45521957 0.99487609 0.98755592 0.27652934 0.55403078 0.81363386\n",
      " 0.52220666 0.89357597 0.28944984 0.7348004  0.98750556 0.99400854\n",
      " 0.96320808 0.92343378 0.97241229 0.97392774 0.93476725 0.16296501\n",
      " 0.4327234  0.54163307 0.78105652 0.98176044 0.30876288 0.98868972\n",
      " 0.98739827 0.70704639 0.5947488  0.9532795  0.1302837  0.94071048\n",
      " 0.15876311 0.71270102 0.71810019 0.9439196  0.16708505 0.32350788\n",
      " 0.21664585 0.26002842 0.6717332  0.33500716 0.16787653 0.06183206\n",
      " 0.88029504 0.30879009 0.80065703 0.4783392  0.90486139 0.9402793\n",
      " 0.92641836 0.74978954 0.93259305 0.95572567]\n",
      "predict [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 108 [0/43 (0%)]\tTrain Loss: 0.030658\n",
      "Train Epoch: 108 [10/43 (23%)]\tTrain Loss: 0.035627\n",
      "Train Epoch: 108 [20/43 (47%)]\tTrain Loss: 0.051209\n",
      "Train Epoch: 108 [30/43 (70%)]\tTrain Loss: 0.022176\n",
      "Train Epoch: 108 [40/43 (93%)]\tTrain Loss: 0.110051\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.15976276 0.31928375 0.5129444  0.71823055 0.42773804 0.55186832\n",
      " 0.36201364 0.7779566  0.6512785  0.66247809 0.36344692 0.36581844\n",
      " 0.58271188 0.9901017  0.77013767 0.14690217 0.25766787 0.16776618\n",
      " 0.05007539 0.67988414 0.85737318 0.45139584 0.8961485  0.58650374\n",
      " 0.33650258 0.83411193 0.66493958 0.37350619 0.08380376 0.14335391\n",
      " 0.90574282 0.41265228 0.07740107 0.57735014 0.05100034 0.07695064\n",
      " 0.03998759 0.79075336 0.09529486 0.16764358 0.18195815 0.14180869\n",
      " 0.65134466 0.06731967 0.14880267 0.3226617  0.09438004 0.12864846\n",
      " 0.69490063 0.67759913 0.64626038 0.22040288 0.09358879 0.08005187\n",
      " 0.58015722 0.07793181 0.72080731 0.06654342 0.68754125 0.20401798\n",
      " 0.43604341 0.89579916 0.97202861 0.87621433 0.44557559 0.55696964\n",
      " 0.27704373 0.99634993 0.99386543 0.77999359 0.48095629 0.39999878\n",
      " 0.38148576 0.80266327 0.24549162 0.73583001 0.99673933 0.9963904\n",
      " 0.98428577 0.92796808 0.72462982 0.92661834 0.93415529 0.21098822\n",
      " 0.15636227 0.41863757 0.96373147 0.98865604 0.81585467 0.99197042\n",
      " 0.99471563 0.70847917 0.37452421 0.82859844 0.5398711  0.83498424\n",
      " 0.66180921 0.7694909  0.36121529 0.92561114 0.05609709 0.39088047\n",
      " 0.31445974 0.09865788 0.27921548 0.57761842 0.54227424 0.69622391\n",
      " 0.80832672 0.76803237 0.15356179 0.33142039 0.94921041 0.92859995\n",
      " 0.96426767 0.90738136 0.85604101 0.93010938]\n",
      "predict [0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 109 [0/43 (0%)]\tTrain Loss: 0.049592\n",
      "Train Epoch: 109 [10/43 (23%)]\tTrain Loss: 0.089124\n",
      "Train Epoch: 109 [20/43 (47%)]\tTrain Loss: 0.043181\n",
      "Train Epoch: 109 [30/43 (70%)]\tTrain Loss: 0.028336\n",
      "Train Epoch: 109 [40/43 (93%)]\tTrain Loss: 0.038380\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.28091341 0.93995595 0.95626879 0.88973951 0.39372656 0.24776557\n",
      " 0.89595795 0.85804784 0.21663272 0.55578727 0.39601126 0.40477559\n",
      " 0.90285742 0.98004651 0.98645812 0.18987733 0.4068507  0.51298046\n",
      " 0.685799   0.53282219 0.23435919 0.85741735 0.98987764 0.34688282\n",
      " 0.70421529 0.97330236 0.34274051 0.81567919 0.36247656 0.69794005\n",
      " 0.92557156 0.72187805 0.13039649 0.26875842 0.10794049 0.15495811\n",
      " 0.27469653 0.43051478 0.54033536 0.3207022  0.38575417 0.52242893\n",
      " 0.11875349 0.47625515 0.60164893 0.5016647  0.27213371 0.42876619\n",
      " 0.7372784  0.30554163 0.32158497 0.25711066 0.18636343 0.16328029\n",
      " 0.20122373 0.31291395 0.34010151 0.07620841 0.49520007 0.36812571\n",
      " 0.89934981 0.71757543 0.93101621 0.95031184 0.90197921 0.87503701\n",
      " 0.49835119 0.99709857 0.99153781 0.35072714 0.78037673 0.75083554\n",
      " 0.76293772 0.714508   0.63754356 0.881576   0.99171597 0.99898559\n",
      " 0.98762518 0.94437325 0.86888397 0.93473476 0.98043227 0.7232731\n",
      " 0.41629934 0.77071863 0.91115075 0.94368452 0.93601942 0.93876112\n",
      " 0.95396745 0.57148343 0.69001514 0.58537209 0.28986156 0.2887629\n",
      " 0.25173086 0.77551079 0.70399022 0.9541375  0.2635614  0.8622089\n",
      " 0.34248498 0.19185826 0.42450181 0.73757911 0.18664737 0.20747298\n",
      " 0.715406   0.4082005  0.69520283 0.28243071 0.93489528 0.83039594\n",
      " 0.93424141 0.676938   0.86177063 0.97393405]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 110 [0/43 (0%)]\tTrain Loss: 0.098134\n",
      "Train Epoch: 110 [10/43 (23%)]\tTrain Loss: 0.027944\n",
      "Train Epoch: 110 [20/43 (47%)]\tTrain Loss: 0.037150\n",
      "Train Epoch: 110 [30/43 (70%)]\tTrain Loss: 0.073792\n",
      "Train Epoch: 110 [40/43 (93%)]\tTrain Loss: 0.047275\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.08627332 0.73923063 0.7948581  0.43499205 0.38581839 0.04757083\n",
      " 0.87410587 0.38163352 0.08892917 0.05245269 0.10626069 0.10926403\n",
      " 0.36377159 0.89850652 0.88454825 0.34603298 0.37647459 0.22312924\n",
      " 0.34604406 0.54645109 0.63518542 0.72942966 0.90962511 0.76985067\n",
      " 0.61394417 0.89128113 0.80890781 0.74545252 0.30579805 0.17308243\n",
      " 0.79140455 0.50464702 0.17270815 0.06085815 0.01967448 0.04867963\n",
      " 0.08257173 0.13772319 0.37138516 0.14103113 0.23862265 0.17234425\n",
      " 0.0441587  0.05272126 0.36130607 0.07262848 0.08207405 0.11418621\n",
      " 0.04775336 0.06700294 0.11196554 0.06465236 0.05806117 0.09648\n",
      " 0.16369361 0.08848401 0.36734563 0.01688269 0.09432741 0.10410878\n",
      " 0.9071849  0.45413238 0.94249445 0.93128568 0.8527602  0.60488683\n",
      " 0.63558149 0.97842818 0.99159074 0.10513676 0.64689815 0.41868073\n",
      " 0.63051713 0.77687383 0.49469945 0.91829187 0.98119593 0.99024242\n",
      " 0.96508729 0.83811879 0.88662928 0.96460795 0.96601826 0.15522876\n",
      " 0.31519103 0.45290044 0.92879832 0.89574546 0.89463615 0.9686442\n",
      " 0.96287328 0.64992523 0.22180851 0.99061757 0.69353151 0.84363729\n",
      " 0.07143823 0.52436173 0.5768196  0.95667863 0.13752192 0.45998234\n",
      " 0.25389922 0.07299647 0.04812618 0.30521989 0.09945314 0.21336623\n",
      " 0.81155896 0.31449655 0.45966387 0.77352566 0.87188137 0.87344509\n",
      " 0.87969273 0.45781186 0.93878108 0.94261563]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 42 TN= 42 FN= 16 FP= 18\n",
      "TP+FP 60\n",
      "precision 0.7\n",
      "recall 0.7241379310344828\n",
      "F1 0.711864406779661\n",
      "acc 0.711864406779661\n",
      "AUCp 0.7120689655172412\n",
      "AUC 0.8126436781609195\n",
      "\n",
      " The epoch is 110, average recall: 0.7241, average precision: 0.7000,average F1: 0.7119, average accuracy: 0.7119, average AUC: 0.8126\n",
      "Train Epoch: 111 [0/43 (0%)]\tTrain Loss: 0.054238\n",
      "Train Epoch: 111 [10/43 (23%)]\tTrain Loss: 0.032520\n",
      "Train Epoch: 111 [20/43 (47%)]\tTrain Loss: 0.055732\n",
      "Train Epoch: 111 [30/43 (70%)]\tTrain Loss: 0.022122\n",
      "Train Epoch: 111 [40/43 (93%)]\tTrain Loss: 0.048882\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.30548695 0.85060269 0.81993425 0.72583449 0.56632715 0.12879644\n",
      " 0.81622714 0.58164316 0.2190969  0.23055677 0.25140601 0.29957274\n",
      " 0.67888147 0.9176681  0.88736826 0.23320937 0.29701453 0.49893954\n",
      " 0.23481701 0.24784008 0.27372402 0.72959673 0.94213957 0.23425031\n",
      " 0.84560072 0.79009634 0.28307417 0.70995677 0.52564758 0.69441289\n",
      " 0.80239671 0.48187831 0.04096535 0.02757679 0.04113804 0.23251629\n",
      " 0.13909699 0.33756208 0.61647898 0.27418682 0.69764405 0.61490315\n",
      " 0.37535948 0.21014191 0.44436103 0.38047874 0.27047643 0.23053379\n",
      " 0.41220021 0.27762026 0.31699157 0.09428361 0.05910997 0.3122507\n",
      " 0.2248341  0.17514676 0.34317908 0.06828713 0.30800134 0.16267624\n",
      " 0.91037977 0.78114724 0.87384421 0.9694984  0.70119363 0.65480983\n",
      " 0.71011984 0.99585837 0.97325915 0.38001421 0.68545163 0.54609472\n",
      " 0.53739405 0.82880902 0.53156942 0.89367133 0.97219151 0.99252677\n",
      " 0.99326515 0.88729596 0.84677982 0.96582317 0.98807389 0.59044731\n",
      " 0.41193151 0.57502395 0.96666873 0.97352612 0.78582174 0.95793968\n",
      " 0.9819048  0.63945377 0.30250123 0.98745883 0.74673742 0.43452328\n",
      " 0.27110481 0.65620834 0.69919783 0.93223113 0.17730667 0.69176126\n",
      " 0.5053972  0.17709473 0.08027519 0.37977397 0.28855285 0.19971231\n",
      " 0.80064082 0.19063716 0.82206017 0.62970471 0.87097061 0.85258168\n",
      " 0.82879442 0.45925161 0.89249808 0.96643442]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 112 [0/43 (0%)]\tTrain Loss: 0.050780\n",
      "Train Epoch: 112 [10/43 (23%)]\tTrain Loss: 0.026752\n",
      "Train Epoch: 112 [20/43 (47%)]\tTrain Loss: 0.035461\n",
      "Train Epoch: 112 [30/43 (70%)]\tTrain Loss: 0.047500\n",
      "Train Epoch: 112 [40/43 (93%)]\tTrain Loss: 0.066124\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.06148054 0.62411821 0.45189324 0.29772162 0.20047902 0.28326276\n",
      " 0.7584793  0.3546243  0.29029986 0.12276252 0.05191834 0.1068615\n",
      " 0.37197316 0.98263943 0.82337022 0.17544092 0.21399052 0.1226404\n",
      " 0.10468934 0.22334135 0.17384931 0.58056748 0.91491783 0.24394473\n",
      " 0.43021837 0.83277017 0.37349334 0.49196574 0.14189489 0.13960582\n",
      " 0.73790145 0.47764045 0.09067315 0.01365875 0.01921891 0.06539759\n",
      " 0.1217908  0.22005585 0.18613525 0.02548316 0.07036851 0.07008868\n",
      " 0.25886592 0.03155101 0.09259673 0.14117201 0.05374324 0.03918009\n",
      " 0.38164932 0.1589739  0.17956349 0.03119263 0.03199154 0.04446167\n",
      " 0.15203443 0.05107223 0.15962975 0.02507527 0.22121185 0.10997737\n",
      " 0.89603752 0.5574308  0.91251957 0.94869661 0.74080521 0.78516948\n",
      " 0.59690136 0.98398954 0.99212146 0.44201082 0.5981617  0.53832364\n",
      " 0.64804274 0.85562819 0.31973764 0.88390106 0.97232753 0.97829539\n",
      " 0.98936391 0.93321341 0.93512529 0.96872765 0.97242343 0.24380721\n",
      " 0.42889196 0.57464534 0.84966326 0.92899936 0.41483217 0.96018386\n",
      " 0.91905063 0.52390999 0.4748114  0.98366457 0.57048345 0.68693596\n",
      " 0.39342067 0.74714094 0.58674341 0.9849236  0.06082647 0.33767089\n",
      " 0.24325849 0.01697942 0.08582578 0.14185627 0.16899087 0.09070964\n",
      " 0.66395986 0.11298461 0.5442062  0.66820246 0.92579776 0.95786291\n",
      " 0.87387753 0.66907263 0.86830306 0.89174604]\n",
      "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 113 [0/43 (0%)]\tTrain Loss: 0.083098\n",
      "Train Epoch: 113 [10/43 (23%)]\tTrain Loss: 0.044443\n",
      "Train Epoch: 113 [20/43 (47%)]\tTrain Loss: 0.049871\n",
      "Train Epoch: 113 [30/43 (70%)]\tTrain Loss: 0.045785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 113 [40/43 (93%)]\tTrain Loss: 0.036927\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.24998745 0.35055044 0.53305072 0.63569671 0.22950631 0.3108862\n",
      " 0.35241234 0.57777846 0.38792393 0.22686338 0.41031    0.24449311\n",
      " 0.7671243  0.66975534 0.31614032 0.2537747  0.4718895  0.4062303\n",
      " 0.26912385 0.38487443 0.13104272 0.82752681 0.97034585 0.24870951\n",
      " 0.65162927 0.88591129 0.17555089 0.76281673 0.34878948 0.23706339\n",
      " 0.82779682 0.42903543 0.0219955  0.2617504  0.08356296 0.1673695\n",
      " 0.27887771 0.47868451 0.63048053 0.12359933 0.22517309 0.61281765\n",
      " 0.27529365 0.14903504 0.41462946 0.22767501 0.11920236 0.11635145\n",
      " 0.47906896 0.34222519 0.22734922 0.23820752 0.05983198 0.11556597\n",
      " 0.32341823 0.13051152 0.45342112 0.04977275 0.24132562 0.24680874\n",
      " 0.74996334 0.69098419 0.86571038 0.90944391 0.78839362 0.56515265\n",
      " 0.44160664 0.98776519 0.98728126 0.65765446 0.51837379 0.46872544\n",
      " 0.58187753 0.85854667 0.65652871 0.66130525 0.98166531 0.97887963\n",
      " 0.94306231 0.87031215 0.85449898 0.97167462 0.9563399  0.22288637\n",
      " 0.3834888  0.45669565 0.80286831 0.82306421 0.48965609 0.92775792\n",
      " 0.98698086 0.7862615  0.63110554 0.99375808 0.50489354 0.33655483\n",
      " 0.32430774 0.43207663 0.59754801 0.9760825  0.1076302  0.34265533\n",
      " 0.16509095 0.11690668 0.14714502 0.33085436 0.18815391 0.27415416\n",
      " 0.87852627 0.19636561 0.64226907 0.58702117 0.91459465 0.94207841\n",
      " 0.94219363 0.45290929 0.7652548  0.86605012]\n",
      "predict [0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "Train Epoch: 114 [0/43 (0%)]\tTrain Loss: 0.069471\n",
      "Train Epoch: 114 [10/43 (23%)]\tTrain Loss: 0.024291\n",
      "Train Epoch: 114 [20/43 (47%)]\tTrain Loss: 0.060111\n",
      "Train Epoch: 114 [30/43 (70%)]\tTrain Loss: 0.042014\n",
      "Train Epoch: 114 [40/43 (93%)]\tTrain Loss: 0.034018\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.15894465 0.82092297 0.41185611 0.55470467 0.09115471 0.13483152\n",
      " 0.7060622  0.40571448 0.35081017 0.24953575 0.1456311  0.1897773\n",
      " 0.35944608 0.87252265 0.56160551 0.13110235 0.43529716 0.2700758\n",
      " 0.04614875 0.14574365 0.24605629 0.48455286 0.89913696 0.3189562\n",
      " 0.55855215 0.8922143  0.24567059 0.50718123 0.10321847 0.03054921\n",
      " 0.58210385 0.12529714 0.01666456 0.36213014 0.01587898 0.02231599\n",
      " 0.02577804 0.26297602 0.10783784 0.02771308 0.05104924 0.07039034\n",
      " 0.16993652 0.15825243 0.13839033 0.1324145  0.08341343 0.08873119\n",
      " 0.67487055 0.13628976 0.19405298 0.0852024  0.02883753 0.0209956\n",
      " 0.28301972 0.07698553 0.27750218 0.01255594 0.30620572 0.08673923\n",
      " 0.96213657 0.76727492 0.93818587 0.96288943 0.45603245 0.49357486\n",
      " 0.46744087 0.99379998 0.95660698 0.25794759 0.63183051 0.31000409\n",
      " 0.47851938 0.6443395  0.08606219 0.73456186 0.99225324 0.98883867\n",
      " 0.98853987 0.92511511 0.9667154  0.93639559 0.91955709 0.18559739\n",
      " 0.12251229 0.70994741 0.91697198 0.8501845  0.30608633 0.95506859\n",
      " 0.91241711 0.60217792 0.37619153 0.37628302 0.20932475 0.18157102\n",
      " 0.15793471 0.45553544 0.60116369 0.92339545 0.04963313 0.35420901\n",
      " 0.22681868 0.04677608 0.1016031  0.13474558 0.18426825 0.42013285\n",
      " 0.37630236 0.35015601 0.19598375 0.25336128 0.82264692 0.69310236\n",
      " 0.95507002 0.62685299 0.76947129 0.93496585]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 115 [0/43 (0%)]\tTrain Loss: 0.075052\n",
      "Train Epoch: 115 [10/43 (23%)]\tTrain Loss: 0.025850\n",
      "Train Epoch: 115 [20/43 (47%)]\tTrain Loss: 0.024806\n",
      "Train Epoch: 115 [30/43 (70%)]\tTrain Loss: 0.021901\n",
      "Train Epoch: 115 [40/43 (93%)]\tTrain Loss: 0.037802\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.12548865 0.86736429 0.72217602 0.83392954 0.22429596 0.14471644\n",
      " 0.74848491 0.91598117 0.34421307 0.2993952  0.39618099 0.41382205\n",
      " 0.59280723 0.75978965 0.43150741 0.38774186 0.61386353 0.30347529\n",
      " 0.49713382 0.1205615  0.25757548 0.65210134 0.95313674 0.36703783\n",
      " 0.73587382 0.26195481 0.14756593 0.80669791 0.12689984 0.44310594\n",
      " 0.80459625 0.51338744 0.17190459 0.09918236 0.08295627 0.13326956\n",
      " 0.4144325  0.1931726  0.56223774 0.04105226 0.05152857 0.16960683\n",
      " 0.08919429 0.16679333 0.25835344 0.38236791 0.25425714 0.3997781\n",
      " 0.82484436 0.21920413 0.7591514  0.086362   0.05560141 0.10884951\n",
      " 0.42367834 0.2131799  0.12270126 0.08068751 0.12050445 0.29613233\n",
      " 0.95638937 0.77725106 0.92526305 0.95654565 0.77573144 0.83824885\n",
      " 0.78720808 0.98739094 0.97736287 0.67288005 0.82271838 0.915748\n",
      " 0.71744448 0.54989749 0.64848882 0.81434447 0.97268707 0.97465408\n",
      " 0.98118228 0.95292205 0.96462393 0.96322513 0.97882277 0.36466277\n",
      " 0.24422175 0.68503809 0.92867893 0.89618731 0.41938972 0.96932685\n",
      " 0.95801574 0.73139554 0.76982969 0.08275166 0.15122689 0.21525383\n",
      " 0.31397682 0.3508555  0.44417295 0.9327665  0.13976084 0.61914253\n",
      " 0.30180219 0.171012   0.27179065 0.26823482 0.17035498 0.25019953\n",
      " 0.78091651 0.09378484 0.960509   0.3792358  0.93871176 0.89515686\n",
      " 0.93967181 0.57424527 0.90389711 0.96969473]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 116 [0/43 (0%)]\tTrain Loss: 0.038965\n",
      "Train Epoch: 116 [10/43 (23%)]\tTrain Loss: 0.041160\n",
      "Train Epoch: 116 [20/43 (47%)]\tTrain Loss: 0.051590\n",
      "Train Epoch: 116 [30/43 (70%)]\tTrain Loss: 0.032014\n",
      "Train Epoch: 116 [40/43 (93%)]\tTrain Loss: 0.052469\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.18823645 0.8705017  0.75547022 0.14854026 0.06132318 0.17192341\n",
      " 0.74281722 0.14255886 0.09698665 0.07190114 0.1589061  0.14260262\n",
      " 0.34859672 0.88566017 0.75169152 0.3121793  0.64537317 0.33364081\n",
      " 0.09232159 0.11023586 0.10278299 0.69862884 0.98622698 0.08246368\n",
      " 0.51191896 0.44260123 0.14615501 0.53407449 0.2188893  0.38316578\n",
      " 0.8330338  0.35831016 0.0282395  0.19931336 0.04078756 0.03660399\n",
      " 0.09114471 0.09281477 0.52711737 0.13393043 0.12139571 0.07060989\n",
      " 0.09667131 0.17797811 0.33341959 0.29027683 0.18027674 0.11848079\n",
      " 0.85376334 0.13184841 0.08391897 0.05780891 0.09767649 0.05720686\n",
      " 0.17135918 0.08621793 0.12232888 0.01341497 0.07386366 0.07436721\n",
      " 0.88613898 0.75609249 0.86966401 0.90794754 0.86643106 0.84352803\n",
      " 0.83212101 0.97777611 0.97541046 0.75682491 0.89000511 0.65559202\n",
      " 0.67748594 0.88861251 0.33047071 0.75390375 0.97765678 0.98131114\n",
      " 0.96874535 0.93442905 0.87223577 0.96416092 0.96149528 0.23885219\n",
      " 0.23029099 0.51362544 0.79626876 0.94529808 0.74237287 0.92174566\n",
      " 0.96411884 0.49424699 0.29209152 0.17493196 0.16135344 0.07649969\n",
      " 0.13827009 0.69650429 0.23497593 0.96359742 0.06876136 0.34815887\n",
      " 0.48529437 0.05754308 0.05517892 0.51974756 0.14751337 0.19603178\n",
      " 0.83710909 0.18287688 0.08934365 0.72196144 0.92690849 0.92059451\n",
      " 0.92881298 0.93019438 0.8103503  0.92105544]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 117 [0/43 (0%)]\tTrain Loss: 0.040314\n",
      "Train Epoch: 117 [10/43 (23%)]\tTrain Loss: 0.056633\n",
      "Train Epoch: 117 [20/43 (47%)]\tTrain Loss: 0.040206\n",
      "Train Epoch: 117 [30/43 (70%)]\tTrain Loss: 0.042594\n",
      "Train Epoch: 117 [40/43 (93%)]\tTrain Loss: 0.055915\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.29303873 0.85247362 0.73420483 0.44191983 0.38038331 0.20838569\n",
      " 0.79736513 0.2835727  0.248504   0.23182081 0.23214956 0.4010469\n",
      " 0.36677521 0.32412767 0.06711879 0.11331771 0.31463698 0.65385145\n",
      " 0.1575129  0.52781779 0.32734981 0.42218825 0.82271582 0.36666861\n",
      " 0.73735726 0.94880605 0.96622503 0.30278435 0.0425422  0.17323579\n",
      " 0.60085136 0.1949475  0.03347496 0.25297427 0.39461941 0.02129105\n",
      " 0.04346077 0.32275313 0.39337385 0.04610646 0.3152495  0.33934075\n",
      " 0.27535027 0.08341254 0.1430746  0.38745925 0.44823119 0.27718651\n",
      " 0.86569959 0.3825672  0.36811566 0.34694645 0.42564014 0.05686891\n",
      " 0.1796273  0.2429744  0.23266837 0.01620432 0.35528174 0.01767507\n",
      " 0.84300089 0.74834651 0.94794899 0.95511174 0.22689033 0.25444195\n",
      " 0.38362727 0.97808206 0.96475297 0.67379433 0.89460659 0.55751586\n",
      " 0.61888433 0.78145379 0.25475851 0.92872876 0.99043345 0.98532677\n",
      " 0.98369241 0.909603   0.80113089 0.96061367 0.97137398 0.3494136\n",
      " 0.3942956  0.64730459 0.82957214 0.9491148  0.08815713 0.75030553\n",
      " 0.97329414 0.51974136 0.2510016  0.98878771 0.29763719 0.32612157\n",
      " 0.36703447 0.66984779 0.40268081 0.92230457 0.07196953 0.73990917\n",
      " 0.2043916  0.07876268 0.37798479 0.15059543 0.19679067 0.34361497\n",
      " 0.35753268 0.38153797 0.3109701  0.75546747 0.94444692 0.87694943\n",
      " 0.95575577 0.73305124 0.82462871 0.9532572 ]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 118 [0/43 (0%)]\tTrain Loss: 0.025097\n",
      "Train Epoch: 118 [10/43 (23%)]\tTrain Loss: 0.032882\n",
      "Train Epoch: 118 [20/43 (47%)]\tTrain Loss: 0.036906\n",
      "Train Epoch: 118 [30/43 (70%)]\tTrain Loss: 0.038373\n",
      "Train Epoch: 118 [40/43 (93%)]\tTrain Loss: 0.024199\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01151081 0.79291111 0.86403495 0.42526743 0.41575924 0.01598405\n",
      " 0.83458716 0.44429329 0.35576469 0.00728427 0.0507846  0.03110264\n",
      " 0.27656516 0.75202614 0.14489064 0.15520889 0.40884435 0.24283259\n",
      " 0.35009146 0.4984498  0.46320888 0.68234223 0.95803487 0.90547693\n",
      " 0.5367415  0.95940387 0.83153236 0.48143712 0.12380549 0.32361835\n",
      " 0.6048274  0.14458773 0.01131058 0.01833057 0.01021883 0.03447402\n",
      " 0.09293702 0.29924291 0.42027837 0.15342407 0.20110813 0.27291125\n",
      " 0.01840257 0.20244682 0.18235913 0.19539987 0.01094187 0.0816891\n",
      " 0.8392272  0.00893019 0.63736224 0.01815748 0.00951753 0.0181616\n",
      " 0.01603641 0.01458679 0.44675019 0.00410233 0.02259434 0.01975302\n",
      " 0.90483779 0.89633298 0.90311205 0.8989768  0.55295819 0.716281\n",
      " 0.70239896 0.9720329  0.85526043 0.58301246 0.85502011 0.70066994\n",
      " 0.83190304 0.81264025 0.51598549 0.81486118 0.91997439 0.97014129\n",
      " 0.94344234 0.80180061 0.87873113 0.95547348 0.91446042 0.16328457\n",
      " 0.40808597 0.59486032 0.92522258 0.96921277 0.49456629 0.91089612\n",
      " 0.94311333 0.57314712 0.34654739 0.97168994 0.95867854 0.7710287\n",
      " 0.01227659 0.79014349 0.57555181 0.96824598 0.05314634 0.6251514\n",
      " 0.18657166 0.19708848 0.06821585 0.09845969 0.7463553  0.32159656\n",
      " 0.83955973 0.39666671 0.7420308  0.48777676 0.96603471 0.94726551\n",
      " 0.93448371 0.67227852 0.7931394  0.91700572]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 119 [0/43 (0%)]\tTrain Loss: 0.043876\n",
      "Train Epoch: 119 [10/43 (23%)]\tTrain Loss: 0.023589\n",
      "Train Epoch: 119 [20/43 (47%)]\tTrain Loss: 0.057047\n",
      "Train Epoch: 119 [30/43 (70%)]\tTrain Loss: 0.041927\n",
      "Train Epoch: 119 [40/43 (93%)]\tTrain Loss: 0.016480\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.05377343 0.7493602  0.57485795 0.67071038 0.64202982 0.00666118\n",
      " 0.51854968 0.44787633 0.45986876 0.01395935 0.21272428 0.03416935\n",
      " 0.78552777 0.57658905 0.39729717 0.06227421 0.30482075 0.00511892\n",
      " 0.00951651 0.18986017 0.05997921 0.22486171 0.72931159 0.6706093\n",
      " 0.21919279 0.67009801 0.51946294 0.17224655 0.0028998  0.07545362\n",
      " 0.59891534 0.07023264 0.00326311 0.00394131 0.05327638 0.01457667\n",
      " 0.08250781 0.55524766 0.05967615 0.01100575 0.00618652 0.010028\n",
      " 0.00902732 0.08387674 0.28816119 0.0656547  0.00501033 0.1329422\n",
      " 0.7503199  0.01041113 0.71632951 0.00697641 0.00410904 0.02379296\n",
      " 0.01435233 0.04031384 0.33041361 0.00656911 0.0136207  0.14090015\n",
      " 0.43164521 0.1574107  0.762936   0.77423537 0.57687545 0.63113236\n",
      " 0.30579945 0.92110711 0.96323884 0.56306267 0.22419196 0.36507863\n",
      " 0.40744856 0.70885026 0.34256965 0.38631797 0.98370981 0.98172587\n",
      " 0.92530328 0.70925546 0.5652231  0.90476    0.94397038 0.0144515\n",
      " 0.01413347 0.29023135 0.51015228 0.59010714 0.02420278 0.73298723\n",
      " 0.84362543 0.81509352 0.6752575  0.90485024 0.31442806 0.55198449\n",
      " 0.01298369 0.06701588 0.24593687 0.84400499 0.0020217  0.19690208\n",
      " 0.0077622  0.00855028 0.08065885 0.00844156 0.00664273 0.61546618\n",
      " 0.57371056 0.21780685 0.40298092 0.70830286 0.8378886  0.67595977\n",
      " 0.24722405 0.34297615 0.74417496 0.93038142]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
      "Train Epoch: 120 [0/43 (0%)]\tTrain Loss: 0.056477\n",
      "Train Epoch: 120 [10/43 (23%)]\tTrain Loss: 0.045319\n",
      "Train Epoch: 120 [20/43 (47%)]\tTrain Loss: 0.036500\n",
      "Train Epoch: 120 [30/43 (70%)]\tTrain Loss: 0.063217\n",
      "Train Epoch: 120 [40/43 (93%)]\tTrain Loss: 0.053225\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.18031023 0.9492771  0.94691366 0.61791295 0.63973671 0.78161454\n",
      " 0.96034306 0.61744058 0.606565   0.82605082 0.22816002 0.79221696\n",
      " 0.48308703 0.98913133 0.90104073 0.53832585 0.79207289 0.71161097\n",
      " 0.76739717 0.53091985 0.43428105 0.74519426 0.99155301 0.95567083\n",
      " 0.73872632 0.99555725 0.91362959 0.86833459 0.71385527 0.93006831\n",
      " 0.63575488 0.63174146 0.27395302 0.28139189 0.16169888 0.15798703\n",
      " 0.36512959 0.66687626 0.58788556 0.53234327 0.27216589 0.43683276\n",
      " 0.85105747 0.35333207 0.48466381 0.24176553 0.36048123 0.4373382\n",
      " 0.83785456 0.91704476 0.73675096 0.48779938 0.38644493 0.25499254\n",
      " 0.82942027 0.13032185 0.75635231 0.22475988 0.80693829 0.23342782\n",
      " 0.8797152  0.8584016  0.91408169 0.93546081 0.67010134 0.78989118\n",
      " 0.86943066 0.9984889  0.99880993 0.75012785 0.63206494 0.873218\n",
      " 0.69242901 0.86043966 0.69806463 0.89797157 0.99799621 0.99635637\n",
      " 0.99230039 0.94204378 0.97181183 0.99553275 0.99141765 0.73612553\n",
      " 0.51041812 0.77975225 0.97426659 0.98384494 0.95355648 0.98547679\n",
      " 0.99473453 0.59261757 0.47235662 0.9981212  0.80305523 0.98607838\n",
      " 0.84645885 0.89639282 0.7942329  0.96555781 0.48631343 0.81016326\n",
      " 0.85381323 0.20073822 0.311849   0.80460173 0.9562192  0.30244109\n",
      " 0.88620508 0.73330879 0.94045055 0.65325582 0.98684406 0.84012336\n",
      " 0.98185849 0.89974135 0.54041427 0.94288254]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 41 TN= 48 FN= 17 FP= 12\n",
      "TP+FP 53\n",
      "precision 0.7735849056603774\n",
      "recall 0.7068965517241379\n",
      "F1 0.7387387387387387\n",
      "acc 0.7542372881355932\n",
      "AUCp 0.753448275862069\n",
      "AUC 0.8198275862068966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 120, average recall: 0.7069, average precision: 0.7736,average F1: 0.7387, average accuracy: 0.7542, average AUC: 0.8198\n",
      "Train Epoch: 121 [0/43 (0%)]\tTrain Loss: 0.064248\n",
      "Train Epoch: 121 [10/43 (23%)]\tTrain Loss: 0.058518\n",
      "Train Epoch: 121 [20/43 (47%)]\tTrain Loss: 0.033921\n",
      "Train Epoch: 121 [30/43 (70%)]\tTrain Loss: 0.034237\n",
      "Train Epoch: 121 [40/43 (93%)]\tTrain Loss: 0.033524\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.09075112 0.76904446 0.61364859 0.342556   0.33743429 0.24259809\n",
      " 0.71680266 0.41032049 0.27291331 0.05617963 0.27470565 0.05678994\n",
      " 0.51551729 0.90297556 0.31013033 0.0597103  0.65413147 0.23125197\n",
      " 0.4722642  0.55264539 0.25891384 0.80318832 0.95441008 0.86514878\n",
      " 0.5520007  0.95003647 0.84278613 0.20740795 0.03204639 0.29173675\n",
      " 0.57122469 0.14409144 0.04158342 0.04206999 0.0845414  0.07884406\n",
      " 0.08926731 0.29940024 0.16151828 0.02983061 0.02093121 0.05238152\n",
      " 0.0424335  0.09061317 0.16004071 0.21399966 0.19891953 0.36042869\n",
      " 0.64512128 0.06331097 0.88188457 0.07473208 0.05012424 0.0479173\n",
      " 0.07728788 0.04554163 0.50006855 0.04479662 0.06293233 0.04574585\n",
      " 0.8408221  0.74668562 0.95477736 0.94168121 0.33124954 0.33297208\n",
      " 0.2430955  0.97840947 0.97135442 0.36240277 0.73424375 0.73791611\n",
      " 0.50250262 0.46347162 0.17092703 0.83808565 0.99549222 0.97524637\n",
      " 0.96851045 0.73081136 0.91743189 0.94382513 0.96782267 0.59376645\n",
      " 0.57663143 0.81088203 0.62753928 0.89249569 0.3256762  0.92868972\n",
      " 0.95645219 0.46303779 0.26599929 0.95867825 0.68263185 0.9316119\n",
      " 0.05062959 0.53165317 0.61542016 0.91266125 0.37540859 0.7277962\n",
      " 0.38524437 0.1267108  0.24535181 0.84754884 0.75479144 0.35461918\n",
      " 0.63315028 0.75232452 0.40252581 0.36379153 0.94655704 0.70835298\n",
      " 0.88435006 0.90982044 0.76227152 0.95385087]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 122 [0/43 (0%)]\tTrain Loss: 0.034278\n",
      "Train Epoch: 122 [10/43 (23%)]\tTrain Loss: 0.072817\n",
      "Train Epoch: 122 [20/43 (47%)]\tTrain Loss: 0.078470\n",
      "Train Epoch: 122 [30/43 (70%)]\tTrain Loss: 0.026626\n",
      "Train Epoch: 122 [40/43 (93%)]\tTrain Loss: 0.072195\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.08248854 0.93167275 0.89410412 0.89581245 0.74176323 0.4462958\n",
      " 0.78062284 0.88002634 0.81664479 0.34525847 0.29666123 0.14878193\n",
      " 0.96205997 0.99058974 0.80881739 0.22338848 0.8142994  0.53658676\n",
      " 0.84790385 0.6537711  0.80607736 0.78756082 0.99199343 0.96646446\n",
      " 0.61105007 0.97051358 0.98774827 0.85055387 0.53433746 0.89916086\n",
      " 0.81935269 0.71262115 0.08818577 0.21258929 0.04617222 0.12018262\n",
      " 0.44659242 0.89507395 0.86548465 0.42676935 0.17302927 0.30661502\n",
      " 0.13780983 0.40638748 0.55620068 0.41846189 0.12317709 0.20730886\n",
      " 0.70959282 0.19379851 0.79964691 0.20072876 0.23500916 0.32306048\n",
      " 0.14397946 0.40897673 0.49471772 0.109593   0.18049906 0.20236841\n",
      " 0.78129762 0.93196309 0.94371986 0.96512079 0.94485426 0.97570318\n",
      " 0.92702496 0.99632359 0.99873632 0.59207708 0.8131364  0.75233841\n",
      " 0.91951442 0.96423727 0.74273765 0.82977921 0.99750769 0.99412221\n",
      " 0.99114472 0.9894644  0.97645682 0.9937849  0.99720275 0.57947326\n",
      " 0.49467602 0.68554115 0.91361487 0.98194879 0.95228595 0.98683053\n",
      " 0.99376279 0.84141999 0.76051271 0.99772304 0.93094856 0.9789139\n",
      " 0.28325403 0.76049513 0.8747974  0.99220031 0.44142398 0.78202057\n",
      " 0.68438017 0.37104756 0.22592457 0.66486341 0.87442917 0.49756223\n",
      " 0.73684132 0.79416043 0.92790639 0.68204021 0.98141116 0.97843617\n",
      " 0.96306157 0.66994911 0.75618118 0.94938892]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 123 [0/43 (0%)]\tTrain Loss: 0.032993\n",
      "Train Epoch: 123 [10/43 (23%)]\tTrain Loss: 0.032256\n",
      "Train Epoch: 123 [20/43 (47%)]\tTrain Loss: 0.047803\n",
      "Train Epoch: 123 [30/43 (70%)]\tTrain Loss: 0.035729\n",
      "Train Epoch: 123 [40/43 (93%)]\tTrain Loss: 0.038007\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.04814155 0.81096351 0.70437658 0.3420988  0.26910508 0.82729548\n",
      " 0.7534036  0.65398288 0.62562913 0.43145654 0.69472355 0.65214074\n",
      " 0.50020033 0.73043454 0.47410128 0.18960881 0.35548356 0.42633861\n",
      " 0.37667635 0.47867453 0.56181121 0.6316402  0.97211593 0.93285382\n",
      " 0.67679328 0.97586185 0.99077564 0.9104057  0.19237393 0.50197726\n",
      " 0.59399265 0.26892394 0.17930496 0.70818388 0.21900421 0.08077479\n",
      " 0.13330583 0.85235643 0.66787833 0.32137331 0.2192281  0.25022236\n",
      " 0.65621501 0.16623767 0.24394654 0.14318091 0.62837756 0.10939201\n",
      " 0.30901691 0.52112049 0.62548864 0.78576058 0.86918491 0.20504011\n",
      " 0.65701663 0.16656734 0.49668533 0.18986413 0.63776672 0.28157681\n",
      " 0.82620263 0.68215591 0.92112601 0.93970251 0.6091094  0.82316154\n",
      " 0.74989778 0.999089   0.9982509  0.3052913  0.76064271 0.79618889\n",
      " 0.68934631 0.83226806 0.67930096 0.64722228 0.98327154 0.99140924\n",
      " 0.98943651 0.93583405 0.96636969 0.96242666 0.99075902 0.49341333\n",
      " 0.55694711 0.71743178 0.9732011  0.92666602 0.95054555 0.98933321\n",
      " 0.98342508 0.58504987 0.37989697 0.99618489 0.75110728 0.98058587\n",
      " 0.87936586 0.86392969 0.40269256 0.97842419 0.25807074 0.56621236\n",
      " 0.73848593 0.13764156 0.14447635 0.68209922 0.69911087 0.39582813\n",
      " 0.62943673 0.43462655 0.46676466 0.54013252 0.97363758 0.92258209\n",
      " 0.97473133 0.78497684 0.7048803  0.9372291 ]\n",
      "predict [0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 124 [0/43 (0%)]\tTrain Loss: 0.046008\n",
      "Train Epoch: 124 [10/43 (23%)]\tTrain Loss: 0.099468\n",
      "Train Epoch: 124 [20/43 (47%)]\tTrain Loss: 0.042770\n",
      "Train Epoch: 124 [30/43 (70%)]\tTrain Loss: 0.038077\n",
      "Train Epoch: 124 [40/43 (93%)]\tTrain Loss: 0.026177\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01293875 0.80375701 0.61020428 0.55957502 0.28096488 0.2116154\n",
      " 0.72243249 0.53287643 0.39782676 0.18819618 0.19848903 0.35374838\n",
      " 0.76748288 0.59847283 0.32604    0.18069063 0.29340547 0.53672361\n",
      " 0.66320086 0.48239204 0.44307142 0.73780507 0.97368151 0.87046677\n",
      " 0.80143416 0.99138451 0.91539419 0.6121583  0.37395489 0.67582643\n",
      " 0.74454945 0.30328172 0.02815331 0.25228649 0.31885752 0.04234725\n",
      " 0.16123426 0.32961085 0.72322696 0.14190625 0.0966365  0.0852957\n",
      " 0.21705987 0.1356277  0.23057106 0.07204022 0.19628273 0.05900675\n",
      " 0.66014791 0.2417635  0.74362999 0.34462357 0.25831017 0.02304972\n",
      " 0.34854543 0.24992825 0.26685488 0.36898112 0.33129126 0.12469421\n",
      " 0.67442393 0.82296783 0.9401589  0.95651609 0.58287019 0.76316649\n",
      " 0.60916686 0.99880087 0.99455136 0.55361336 0.71087015 0.6949259\n",
      " 0.5730961  0.68328238 0.32816318 0.89512897 0.99617058 0.98944795\n",
      " 0.98756313 0.97318989 0.9125669  0.96295655 0.98222232 0.50958997\n",
      " 0.4313904  0.6309526  0.99116021 0.97005242 0.77096236 0.9933002\n",
      " 0.98353553 0.58449441 0.51100999 0.99131221 0.61047447 0.86905289\n",
      " 0.24820761 0.89917207 0.82587856 0.93978488 0.22543301 0.70088619\n",
      " 0.51966327 0.05531611 0.09162518 0.53470939 0.26282173 0.18646893\n",
      " 0.16320412 0.59769458 0.2989932  0.69054335 0.9710263  0.91426218\n",
      " 0.96356434 0.91585517 0.80278987 0.95641154]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 125 [0/43 (0%)]\tTrain Loss: 0.021096\n",
      "Train Epoch: 125 [10/43 (23%)]\tTrain Loss: 0.028440\n",
      "Train Epoch: 125 [20/43 (47%)]\tTrain Loss: 0.029524\n",
      "Train Epoch: 125 [30/43 (70%)]\tTrain Loss: 0.036703\n",
      "Train Epoch: 125 [40/43 (93%)]\tTrain Loss: 0.015966\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.44036454 0.87635607 0.70227867 0.39203954 0.37526825 0.17023611\n",
      " 0.71052247 0.46557996 0.19832525 0.10984065 0.2689881  0.12301596\n",
      " 0.42005873 0.75828648 0.76984531 0.10813435 0.16540475 0.26005256\n",
      " 0.23589027 0.44315964 0.34869426 0.626486   0.97316098 0.86221224\n",
      " 0.49575284 0.98637277 0.17416289 0.76070434 0.25376365 0.41629612\n",
      " 0.67132449 0.28073463 0.02938704 0.31600657 0.01817804 0.07641913\n",
      " 0.03998791 0.22909901 0.21752049 0.24853607 0.11414434 0.2710683\n",
      " 0.223655   0.10619362 0.23128355 0.05790737 0.27157384 0.09203477\n",
      " 0.53339213 0.45935541 0.74149585 0.10464741 0.35456005 0.04985902\n",
      " 0.18225777 0.04111027 0.45185313 0.01174608 0.27148783 0.05216492\n",
      " 0.8330372  0.8692838  0.97496653 0.94895571 0.59983641 0.83579278\n",
      " 0.56843579 0.99578995 0.98977077 0.4704766  0.6209783  0.77642798\n",
      " 0.4990451  0.73442972 0.37629205 0.87987459 0.98431152 0.99240375\n",
      " 0.99053711 0.93867677 0.93981475 0.95335895 0.97058028 0.21445981\n",
      " 0.18636189 0.58812881 0.9574821  0.99172091 0.58039373 0.98898298\n",
      " 0.91682154 0.42924619 0.42006648 0.99299192 0.77045947 0.94287777\n",
      " 0.40060067 0.85325402 0.71243894 0.97804111 0.20834555 0.37116608\n",
      " 0.29465017 0.03204845 0.22250757 0.61574608 0.35461169 0.13454512\n",
      " 0.28965911 0.34110487 0.55763608 0.86494207 0.9869839  0.61207807\n",
      " 0.79170197 0.69082975 0.90404195 0.95005322]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 126 [0/43 (0%)]\tTrain Loss: 0.049277\n",
      "Train Epoch: 126 [10/43 (23%)]\tTrain Loss: 0.026770\n",
      "Train Epoch: 126 [20/43 (47%)]\tTrain Loss: 0.047640\n",
      "Train Epoch: 126 [30/43 (70%)]\tTrain Loss: 0.030352\n",
      "Train Epoch: 126 [40/43 (93%)]\tTrain Loss: 0.021648\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.07488257 0.89085037 0.65318334 0.27156278 0.23614697 0.49200255\n",
      " 0.91114038 0.30394223 0.10879932 0.23198929 0.46784991 0.41124958\n",
      " 0.17543799 0.95907468 0.5293836  0.15681297 0.26498789 0.33737531\n",
      " 0.36208579 0.41180569 0.37115246 0.54115874 0.96025926 0.91328812\n",
      " 0.44444987 0.9425143  0.97617972 0.49903378 0.23764013 0.59636736\n",
      " 0.61350077 0.2408563  0.03832099 0.36403295 0.32477915 0.01174242\n",
      " 0.0701665  0.395834   0.29473418 0.09479247 0.12727372 0.05721519\n",
      " 0.21712475 0.02513263 0.15239437 0.09759583 0.24633741 0.05674782\n",
      " 0.57752496 0.54098094 0.79221022 0.33802333 0.41895655 0.05396295\n",
      " 0.26373202 0.5728901  0.37365711 0.14749029 0.4711974  0.04852682\n",
      " 0.85134244 0.76653248 0.93061024 0.98180491 0.70974934 0.78944874\n",
      " 0.56920916 0.9960342  0.99672526 0.74056983 0.81380278 0.50625592\n",
      " 0.75666243 0.7375986  0.17969279 0.75794011 0.99461973 0.9932453\n",
      " 0.99414295 0.9814139  0.968382   0.98849827 0.9948886  0.48304772\n",
      " 0.3288644  0.77758783 0.85860342 0.965469   0.11246535 0.95498854\n",
      " 0.96911961 0.44649252 0.41951954 0.99800497 0.68475056 0.94234562\n",
      " 0.499493   0.5435729  0.42752984 0.96053648 0.23291612 0.54918438\n",
      " 0.32011524 0.09429958 0.08172729 0.61275226 0.24428321 0.20147547\n",
      " 0.11659651 0.51196277 0.66946435 0.77998751 0.98279577 0.85694516\n",
      " 0.81363773 0.90360498 0.84031194 0.98130953]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 127 [0/43 (0%)]\tTrain Loss: 0.055255\n",
      "Train Epoch: 127 [10/43 (23%)]\tTrain Loss: 0.023188\n",
      "Train Epoch: 127 [20/43 (47%)]\tTrain Loss: 0.053068\n",
      "Train Epoch: 127 [30/43 (70%)]\tTrain Loss: 0.046747\n",
      "Train Epoch: 127 [40/43 (93%)]\tTrain Loss: 0.021580\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.37725386 0.83643389 0.66692775 0.50351667 0.61948365 0.18037947\n",
      " 0.79905856 0.57013416 0.57811385 0.21076687 0.29770172 0.31783211\n",
      " 0.24012643 0.96506393 0.62759978 0.43438926 0.53592306 0.08650696\n",
      " 0.1781092  0.25291052 0.32258078 0.3459993  0.95823032 0.94326991\n",
      " 0.48569924 0.96122593 0.23430951 0.47458568 0.11457017 0.36852187\n",
      " 0.77512962 0.19208896 0.04728559 0.29282701 0.02587935 0.04214458\n",
      " 0.05753435 0.41767603 0.20095927 0.30290359 0.25429884 0.20506877\n",
      " 0.21627527 0.04843377 0.17369545 0.28236407 0.3814297  0.10635754\n",
      " 0.83463848 0.40410107 0.85960954 0.27175733 0.4939442  0.03454692\n",
      " 0.19893108 0.01505478 0.52955151 0.05147137 0.12508021 0.04998802\n",
      " 0.84684974 0.66066676 0.95617646 0.97684836 0.81617689 0.91273701\n",
      " 0.70465833 0.99893707 0.99443227 0.88985425 0.75620818 0.61290854\n",
      " 0.59858018 0.89382088 0.50857359 0.8870222  0.99488729 0.99468201\n",
      " 0.99645138 0.95795345 0.94389784 0.98839033 0.99088901 0.11986689\n",
      " 0.09172732 0.73911673 0.90606481 0.96200716 0.63445878 0.9823966\n",
      " 0.99078864 0.78333944 0.54862523 0.9973169  0.53884572 0.97827607\n",
      " 0.1723159  0.61102116 0.48402783 0.99220848 0.13871677 0.35887855\n",
      " 0.3253659  0.05746127 0.43345883 0.46895897 0.31439358 0.3487455\n",
      " 0.47974047 0.22946729 0.54245782 0.88000351 0.98030955 0.94736189\n",
      " 0.96711761 0.63596034 0.7312485  0.98307288]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 128 [0/43 (0%)]\tTrain Loss: 0.036191\n",
      "Train Epoch: 128 [10/43 (23%)]\tTrain Loss: 0.023455\n",
      "Train Epoch: 128 [20/43 (47%)]\tTrain Loss: 0.043852\n",
      "Train Epoch: 128 [30/43 (70%)]\tTrain Loss: 0.083881\n",
      "Train Epoch: 128 [40/43 (93%)]\tTrain Loss: 0.141290\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01736804 0.94659728 0.82382727 0.71949136 0.35075504 0.07225538\n",
      " 0.88623261 0.54119754 0.1589006  0.03806512 0.65948147 0.07773924\n",
      " 0.7341615  0.97826183 0.53832287 0.31901082 0.68221623 0.24672925\n",
      " 0.61235398 0.36078805 0.18159123 0.77269596 0.94659364 0.91618556\n",
      " 0.759372   0.97237027 0.87183309 0.7519576  0.61295033 0.36021364\n",
      " 0.61531806 0.31755051 0.06564618 0.02930517 0.02742534 0.02597352\n",
      " 0.14040495 0.63420713 0.24791098 0.18979691 0.27839276 0.10690409\n",
      " 0.03772624 0.16283847 0.75371563 0.2337075  0.03778036 0.12417335\n",
      " 0.69428879 0.05691046 0.8019129  0.03921392 0.03793699 0.02760398\n",
      " 0.05273054 0.09671777 0.4802765  0.04504822 0.0271673  0.08779718\n",
      " 0.80225915 0.66082525 0.9503057  0.91390473 0.63395435 0.93734473\n",
      " 0.43038148 0.98968631 0.95365298 0.61254543 0.63390076 0.51931715\n",
      " 0.5410378  0.7351197  0.30972347 0.92394608 0.98620981 0.99252105\n",
      " 0.97950715 0.93255365 0.91660595 0.93627018 0.95982122 0.23187412\n",
      " 0.38310114 0.67853385 0.95825124 0.97530001 0.26617518 0.98660499\n",
      " 0.9502058  0.84097594 0.40422875 0.973369   0.83082449 0.95302123\n",
      " 0.04059171 0.90920192 0.85660994 0.91614223 0.14053942 0.85504633\n",
      " 0.19041054 0.22278319 0.21639177 0.72608882 0.10465488 0.41915792\n",
      " 0.71213013 0.70213068 0.78267199 0.86146051 0.94325775 0.90009296\n",
      " 0.97699386 0.72919255 0.93375206 0.94625026]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 129 [0/43 (0%)]\tTrain Loss: 0.064272\n",
      "Train Epoch: 129 [10/43 (23%)]\tTrain Loss: 0.026571\n",
      "Train Epoch: 129 [20/43 (47%)]\tTrain Loss: 0.032666\n",
      "Train Epoch: 129 [30/43 (70%)]\tTrain Loss: 0.032671\n",
      "Train Epoch: 129 [40/43 (93%)]\tTrain Loss: 0.026869\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02489998 0.87850767 0.68655121 0.62623888 0.4015128  0.02336826\n",
      " 0.74618173 0.76867634 0.274216   0.07189587 0.21911077 0.04891944\n",
      " 0.77850837 0.89231628 0.49223056 0.16198856 0.61127979 0.01896191\n",
      " 0.06332723 0.31043467 0.08937954 0.34908274 0.8674224  0.92233914\n",
      " 0.41659594 0.91914874 0.83485365 0.72947007 0.0300981  0.09933494\n",
      " 0.69396877 0.21619146 0.05042391 0.08686365 0.02500951 0.01479629\n",
      " 0.07225204 0.33782136 0.37222371 0.08756851 0.08387492 0.1547374\n",
      " 0.06227802 0.0692325  0.60990125 0.28086358 0.06292228 0.3260245\n",
      " 0.75479543 0.13603437 0.82520801 0.08541425 0.06853677 0.01613537\n",
      " 0.04074889 0.16698208 0.28952804 0.03197375 0.11622986 0.12172751\n",
      " 0.67024815 0.56905746 0.88333392 0.93199891 0.69879073 0.84095639\n",
      " 0.53475964 0.99179989 0.98880053 0.51011783 0.4931744  0.40671575\n",
      " 0.49139062 0.65221143 0.31206769 0.72036535 0.98741722 0.98761135\n",
      " 0.9808858  0.93936497 0.95076078 0.95119709 0.98104227 0.1023218\n",
      " 0.18464735 0.57178199 0.89353973 0.95425963 0.34962928 0.98416442\n",
      " 0.92858452 0.82668328 0.2473035  0.99151671 0.80393982 0.88824791\n",
      " 0.05643491 0.83394843 0.52232921 0.9328835  0.05818345 0.51680446\n",
      " 0.11402362 0.06204944 0.22393854 0.42972645 0.17600368 0.25825056\n",
      " 0.50135452 0.40868247 0.59459341 0.63288981 0.9825533  0.91952038\n",
      " 0.9531554  0.80213499 0.89759713 0.97290212]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 130 [0/43 (0%)]\tTrain Loss: 0.048595\n",
      "Train Epoch: 130 [10/43 (23%)]\tTrain Loss: 0.032130\n",
      "Train Epoch: 130 [20/43 (47%)]\tTrain Loss: 0.095774\n",
      "Train Epoch: 130 [30/43 (70%)]\tTrain Loss: 0.023976\n",
      "Train Epoch: 130 [40/43 (93%)]\tTrain Loss: 0.044389\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.05395053 0.85644943 0.33341286 0.81991071 0.4399744  0.58278877\n",
      " 0.76104337 0.67381203 0.34068176 0.45206651 0.46872422 0.42576203\n",
      " 0.67856514 0.90851164 0.54922926 0.13731456 0.39354312 0.39652711\n",
      " 0.58684081 0.61238396 0.32955945 0.88462472 0.99229902 0.96216005\n",
      " 0.55254358 0.98410749 0.86220658 0.85052258 0.58958417 0.46694562\n",
      " 0.89733964 0.66048342 0.21181773 0.67811531 0.09617481 0.07809725\n",
      " 0.11945061 0.52627152 0.73592621 0.22243467 0.13348542 0.4211061\n",
      " 0.42555907 0.10963681 0.5179835  0.21267739 0.38592741 0.15835311\n",
      " 0.73936605 0.50120699 0.8559826  0.58924192 0.2364316  0.07794999\n",
      " 0.3352178  0.13828476 0.34824651 0.02227578 0.48614874 0.11862961\n",
      " 0.89559191 0.81249863 0.96341306 0.96561319 0.81856418 0.88475806\n",
      " 0.87215585 0.99850637 0.99809378 0.75647676 0.69370645 0.85570657\n",
      " 0.73529637 0.93103141 0.65914708 0.8947652  0.99436498 0.99202687\n",
      " 0.98810929 0.98508251 0.94752187 0.98052514 0.97847509 0.58913553\n",
      " 0.45112124 0.74047989 0.96902835 0.98845845 0.78443468 0.99118048\n",
      " 0.99533659 0.66216385 0.28997332 0.99722552 0.8771497  0.99565411\n",
      " 0.45641184 0.89044917 0.57265443 0.98488462 0.38753387 0.71964234\n",
      " 0.21353991 0.114337   0.2942467  0.73601192 0.64838302 0.11467865\n",
      " 0.66799468 0.79959571 0.8275817  0.61145216 0.99659389 0.91720158\n",
      " 0.9805131  0.90468389 0.78443301 0.95663095]\n",
      "predict [0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 42 FN= 11 FP= 18\n",
      "TP+FP 65\n",
      "precision 0.7230769230769231\n",
      "recall 0.8103448275862069\n",
      "F1 0.7642276422764227\n",
      "acc 0.7542372881355932\n",
      "AUCp 0.7551724137931034\n",
      "AUC 0.8160919540229885\n",
      "\n",
      " The epoch is 130, average recall: 0.8103, average precision: 0.7231,average F1: 0.7642, average accuracy: 0.7542, average AUC: 0.8161\n",
      "Train Epoch: 131 [0/43 (0%)]\tTrain Loss: 0.058977\n",
      "Train Epoch: 131 [10/43 (23%)]\tTrain Loss: 0.020579\n",
      "Train Epoch: 131 [20/43 (47%)]\tTrain Loss: 0.041695\n",
      "Train Epoch: 131 [30/43 (70%)]\tTrain Loss: 0.041246\n",
      "Train Epoch: 131 [40/43 (93%)]\tTrain Loss: 0.049076\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.06891145 0.88809717 0.36780691 0.59131771 0.27666175 0.12690327\n",
      " 0.77509826 0.49169287 0.20521778 0.1105217  0.13453034 0.17802261\n",
      " 0.38207093 0.97205091 0.68552554 0.06030833 0.34821773 0.32021672\n",
      " 0.52021176 0.36770585 0.16035447 0.52454221 0.98089808 0.95158118\n",
      " 0.28150102 0.9171499  0.92785096 0.57166171 0.17205203 0.3974711\n",
      " 0.70964473 0.46447736 0.0379324  0.05082474 0.02320045 0.04744029\n",
      " 0.03391722 0.45240027 0.6141699  0.08964885 0.04465737 0.15181744\n",
      " 0.12569162 0.03266712 0.12840019 0.12534454 0.09481189 0.165628\n",
      " 0.88173896 0.15465927 0.70279974 0.13310233 0.21852463 0.0095862\n",
      " 0.11237961 0.01356498 0.53153503 0.021055   0.118302   0.06466085\n",
      " 0.88504624 0.55435187 0.92380929 0.95143324 0.64536017 0.90465385\n",
      " 0.60142195 0.99410146 0.99001396 0.51968932 0.76688915 0.78398097\n",
      " 0.65838712 0.77870059 0.38485631 0.79433423 0.99618739 0.99647945\n",
      " 0.98667777 0.97577286 0.9006297  0.96701783 0.98593038 0.4047471\n",
      " 0.35009468 0.69926023 0.97136444 0.97704667 0.69038379 0.91320115\n",
      " 0.97652155 0.55146438 0.26490229 0.99401468 0.77924556 0.97842467\n",
      " 0.11889178 0.77725148 0.83336246 0.98332673 0.07428481 0.4804529\n",
      " 0.40642196 0.14138825 0.12023108 0.33049661 0.30440575 0.30495596\n",
      " 0.15395248 0.60826826 0.75518495 0.66749549 0.99176008 0.8479588\n",
      " 0.90892726 0.78188556 0.80581146 0.96060574]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 132 [0/43 (0%)]\tTrain Loss: 0.053682\n",
      "Train Epoch: 132 [10/43 (23%)]\tTrain Loss: 0.038191\n",
      "Train Epoch: 132 [20/43 (47%)]\tTrain Loss: 0.018406\n",
      "Train Epoch: 132 [30/43 (70%)]\tTrain Loss: 0.051021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 132 [40/43 (93%)]\tTrain Loss: 0.042980\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01128335 0.93891245 0.81375349 0.61715358 0.15153882 0.04522492\n",
      " 0.9009577  0.17678414 0.15529799 0.05950959 0.23268971 0.13820332\n",
      " 0.51499176 0.86403686 0.66979986 0.04279365 0.36921731 0.17851706\n",
      " 0.53462207 0.5105924  0.36163464 0.79716468 0.98100668 0.89669359\n",
      " 0.49121559 0.93661827 0.91742074 0.66321099 0.13017756 0.14630224\n",
      " 0.66015279 0.31089845 0.02995293 0.06051549 0.00521124 0.02159042\n",
      " 0.01995024 0.38573202 0.22349063 0.05261986 0.05080985 0.07465288\n",
      " 0.54681498 0.13228182 0.38624623 0.14333178 0.055291   0.13119559\n",
      " 0.65603077 0.03991875 0.91399932 0.06115311 0.0162871  0.0234275\n",
      " 0.02080747 0.06283341 0.49907234 0.00617491 0.04539358 0.00971544\n",
      " 0.8699261  0.78946143 0.9883697  0.96907449 0.65737092 0.80922204\n",
      " 0.72199374 0.99749655 0.94684452 0.42127874 0.85471803 0.56429213\n",
      " 0.21679281 0.67243892 0.32351628 0.91510928 0.98682016 0.99215215\n",
      " 0.9956494  0.93825346 0.83551317 0.97301531 0.97942346 0.37299255\n",
      " 0.36270574 0.55712748 0.9106403  0.95435709 0.68298537 0.97833222\n",
      " 0.96165466 0.4275333  0.46304554 0.99637151 0.62498504 0.93397117\n",
      " 0.0502463  0.80164146 0.72703844 0.95532906 0.11855379 0.61067802\n",
      " 0.10473038 0.09350652 0.1437505  0.14489023 0.24011166 0.3322469\n",
      " 0.54420537 0.75500983 0.84067714 0.3741     0.8692258  0.85902083\n",
      " 0.95953929 0.64765733 0.8156758  0.96575314]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 133 [0/43 (0%)]\tTrain Loss: 0.027063\n",
      "Train Epoch: 133 [10/43 (23%)]\tTrain Loss: 0.041736\n",
      "Train Epoch: 133 [20/43 (47%)]\tTrain Loss: 0.123781\n",
      "Train Epoch: 133 [30/43 (70%)]\tTrain Loss: 0.054089\n",
      "Train Epoch: 133 [40/43 (93%)]\tTrain Loss: 0.017518\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.18401127 0.86763364 0.82259691 0.81697178 0.83936012 0.31924659\n",
      " 0.83623278 0.94018352 0.76774406 0.25327742 0.28646263 0.15352944\n",
      " 0.86491454 0.98448205 0.90921938 0.87533396 0.91215473 0.53385717\n",
      " 0.09067073 0.55566305 0.29020944 0.79773647 0.98300052 0.92541176\n",
      " 0.75481391 0.98580372 0.9903667  0.87835902 0.62028128 0.54699922\n",
      " 0.93468302 0.3739877  0.04364783 0.13770521 0.40565655 0.16964316\n",
      " 0.14066873 0.27734995 0.60512173 0.73990965 0.788454   0.87009007\n",
      " 0.19648229 0.31508133 0.68987519 0.8176471  0.56756276 0.30138418\n",
      " 0.95806718 0.21079645 0.97522855 0.18259144 0.22778885 0.15381886\n",
      " 0.14890091 0.30563524 0.15769573 0.08223405 0.21411885 0.36687824\n",
      " 0.90634507 0.86437595 0.95863354 0.9553476  0.77281481 0.94967085\n",
      " 0.92439967 0.99539328 0.99358988 0.91139191 0.65162969 0.79066026\n",
      " 0.91477942 0.95081961 0.86179489 0.93758756 0.9924736  0.99804151\n",
      " 0.97354805 0.96278226 0.97074765 0.99484128 0.99103469 0.41357967\n",
      " 0.40589726 0.60379976 0.96657133 0.9608987  0.78068709 0.99123853\n",
      " 0.97269249 0.89128131 0.6352064  0.99272728 0.83253604 0.96310854\n",
      " 0.15885527 0.96820563 0.26917493 0.98733085 0.21835861 0.79886264\n",
      " 0.51917118 0.35776258 0.41786686 0.58865452 0.16033028 0.5191986\n",
      " 0.93125868 0.20662363 0.917023   0.90813684 0.98235804 0.94071895\n",
      " 0.97970986 0.81339788 0.97374433 0.98302716]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 134 [0/43 (0%)]\tTrain Loss: 0.014779\n",
      "Train Epoch: 134 [10/43 (23%)]\tTrain Loss: 0.046783\n",
      "Train Epoch: 134 [20/43 (47%)]\tTrain Loss: 0.022630\n",
      "Train Epoch: 134 [30/43 (70%)]\tTrain Loss: 0.026431\n",
      "Train Epoch: 134 [40/43 (93%)]\tTrain Loss: 0.034732\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.04800997 0.90153354 0.78286731 0.61118996 0.24173853 0.38240248\n",
      " 0.86402351 0.40712199 0.70683223 0.17050789 0.25516954 0.23406084\n",
      " 0.59009135 0.91195816 0.54167134 0.16143334 0.40885559 0.59118062\n",
      " 0.52904052 0.58686757 0.42618707 0.894642   0.99139434 0.94805235\n",
      " 0.70900118 0.98249531 0.96196389 0.88993496 0.36917001 0.73637784\n",
      " 0.84140283 0.49023846 0.22969346 0.27505773 0.37545639 0.0424717\n",
      " 0.0799081  0.50711477 0.36408284 0.22479193 0.36985937 0.5048064\n",
      " 0.37584332 0.35874593 0.67064023 0.60540831 0.44942424 0.2599515\n",
      " 0.82356143 0.39095718 0.95198768 0.42469344 0.4453049  0.38387525\n",
      " 0.41271049 0.46117488 0.2831265  0.41223705 0.46150506 0.54166859\n",
      " 0.91869354 0.93236613 0.98311526 0.98079371 0.77100593 0.88607711\n",
      " 0.58820909 0.99949658 0.99776673 0.80321997 0.74118978 0.59935194\n",
      " 0.54127622 0.8689729  0.75720859 0.95295292 0.99623603 0.99434078\n",
      " 0.99387723 0.94448245 0.94453138 0.95762235 0.9944371  0.60423052\n",
      " 0.63700849 0.88490796 0.97125262 0.98080772 0.82065517 0.98749208\n",
      " 0.98922813 0.75301677 0.3555699  0.99547881 0.65302384 0.98531008\n",
      " 0.38567904 0.90949214 0.56125832 0.98635942 0.1964763  0.80587173\n",
      " 0.498593   0.20533355 0.22065806 0.6470713  0.42931056 0.49061653\n",
      " 0.76230037 0.75140566 0.60858941 0.580778   0.95401347 0.95773321\n",
      " 0.95715928 0.92699921 0.91219819 0.97917956]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 135 [0/43 (0%)]\tTrain Loss: 0.032778\n",
      "Train Epoch: 135 [10/43 (23%)]\tTrain Loss: 0.024743\n",
      "Train Epoch: 135 [20/43 (47%)]\tTrain Loss: 0.032800\n",
      "Train Epoch: 135 [30/43 (70%)]\tTrain Loss: 0.038836\n",
      "Train Epoch: 135 [40/43 (93%)]\tTrain Loss: 0.028725\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01201933 0.60884321 0.40553176 0.4182131  0.29515529 0.12654491\n",
      " 0.65077621 0.49709722 0.33148828 0.05011214 0.09246054 0.09085858\n",
      " 0.45594263 0.75337011 0.26034826 0.08249854 0.2082534  0.10259183\n",
      " 0.30993417 0.45651752 0.44019985 0.69323337 0.96410221 0.7657302\n",
      " 0.39339069 0.94470805 0.81912321 0.61608553 0.30102375 0.29288888\n",
      " 0.76178455 0.4143351  0.02460428 0.10711384 0.10813107 0.01183745\n",
      " 0.02477    0.07880235 0.11475047 0.08294981 0.04492278 0.05906073\n",
      " 0.02550201 0.11319996 0.30522031 0.13979948 0.05145347 0.07207901\n",
      " 0.56144166 0.07744376 0.7935248  0.05034594 0.12701192 0.00854145\n",
      " 0.04500213 0.12509675 0.04011184 0.03207018 0.06784073 0.12103777\n",
      " 0.8989374  0.68509346 0.93097478 0.91369212 0.46424851 0.77151382\n",
      " 0.50454503 0.97806782 0.93353343 0.31729493 0.49577713 0.51519078\n",
      " 0.44523457 0.56564331 0.42589948 0.8362115  0.97492313 0.95820832\n",
      " 0.95303959 0.80228519 0.94561744 0.93943703 0.972031   0.07853904\n",
      " 0.2617577  0.68611163 0.78592628 0.84324628 0.64164931 0.92899525\n",
      " 0.89980632 0.20891081 0.22718014 0.97493923 0.65684748 0.88777941\n",
      " 0.05088936 0.63388544 0.26091725 0.77014196 0.05652995 0.4560262\n",
      " 0.14585875 0.06289839 0.15994491 0.33737227 0.04260983 0.26404575\n",
      " 0.29708266 0.05754704 0.39044344 0.1648405  0.95856684 0.86790603\n",
      " 0.85342467 0.3787106  0.63815796 0.90144819]\n",
      "predict [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 136 [0/43 (0%)]\tTrain Loss: 0.046602\n",
      "Train Epoch: 136 [10/43 (23%)]\tTrain Loss: 0.043815\n",
      "Train Epoch: 136 [20/43 (47%)]\tTrain Loss: 0.028031\n",
      "Train Epoch: 136 [30/43 (70%)]\tTrain Loss: 0.073778\n",
      "Train Epoch: 136 [40/43 (93%)]\tTrain Loss: 0.028572\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.57487792 0.73018223 0.4439894  0.71842688 0.62693256 0.38652778\n",
      " 0.52866399 0.58713698 0.27063704 0.27671286 0.31184313 0.42557085\n",
      " 0.58064097 0.83400142 0.35641426 0.14548218 0.51902324 0.21314152\n",
      " 0.35037723 0.24797489 0.22224212 0.60076421 0.96960855 0.9266485\n",
      " 0.45190498 0.97718322 0.9752059  0.84769362 0.09426317 0.6207329\n",
      " 0.8227421  0.29853281 0.03305876 0.54029965 0.56359667 0.0189708\n",
      " 0.02537675 0.2339392  0.27123791 0.05357867 0.0970309  0.13723442\n",
      " 0.38696504 0.1324724  0.34087765 0.22158447 0.34478742 0.1095242\n",
      " 0.83686602 0.3116509  0.90922374 0.53763443 0.48940399 0.08428306\n",
      " 0.3593353  0.42012084 0.55005014 0.5538975  0.47847915 0.11189836\n",
      " 0.87009585 0.7063539  0.88655186 0.97002631 0.42378291 0.85622752\n",
      " 0.73096824 0.99111968 0.9883309  0.62301862 0.58930504 0.68225461\n",
      " 0.63143176 0.87583691 0.5133062  0.85175484 0.99868435 0.99594563\n",
      " 0.9738192  0.95063549 0.95561802 0.98582435 0.99458569 0.65933514\n",
      " 0.16728091 0.68143743 0.90064323 0.93515658 0.72446513 0.99620217\n",
      " 0.99038577 0.57108235 0.24868576 0.99647814 0.65755683 0.71444166\n",
      " 0.33669552 0.84291309 0.47869986 0.96677631 0.13071239 0.50696039\n",
      " 0.07877685 0.07024919 0.34861523 0.54078442 0.24012035 0.30727291\n",
      " 0.63024426 0.25308141 0.31134728 0.69406742 0.97027069 0.89607245\n",
      " 0.97168779 0.79836458 0.88727683 0.93668145]\n",
      "predict [1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 137 [0/43 (0%)]\tTrain Loss: 0.032085\n",
      "Train Epoch: 137 [10/43 (23%)]\tTrain Loss: 0.041985\n",
      "Train Epoch: 137 [20/43 (47%)]\tTrain Loss: 0.047019\n",
      "Train Epoch: 137 [30/43 (70%)]\tTrain Loss: 0.029932\n",
      "Train Epoch: 137 [40/43 (93%)]\tTrain Loss: 0.035015\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.06264921 0.85909104 0.80348599 0.67410356 0.44494346 0.14648387\n",
      " 0.87394142 0.60275745 0.42541128 0.25447378 0.24275635 0.24628878\n",
      " 0.60901356 0.92429447 0.56348675 0.16925921 0.38644183 0.66443455\n",
      " 0.76712322 0.71647298 0.64737213 0.93310839 0.99807584 0.97060758\n",
      " 0.76060843 0.98776788 0.9778226  0.97720307 0.82841861 0.69417799\n",
      " 0.90127647 0.56483978 0.04361991 0.11837931 0.02751136 0.01849566\n",
      " 0.09274622 0.43218112 0.68887472 0.24300368 0.41968709 0.30134088\n",
      " 0.21327883 0.41681385 0.73892868 0.22560702 0.13688236 0.07797538\n",
      " 0.77998114 0.14940229 0.77104437 0.50473738 0.35738057 0.02363735\n",
      " 0.14530502 0.04927621 0.78203988 0.01542333 0.29691508 0.10317628\n",
      " 0.95024198 0.88932246 0.95335305 0.98205549 0.58760107 0.86257637\n",
      " 0.65436107 0.9961164  0.9937796  0.65564299 0.739214   0.57821763\n",
      " 0.71440047 0.91124809 0.56400579 0.95354509 0.99544686 0.99668992\n",
      " 0.97163391 0.96795613 0.98247945 0.98587453 0.99299687 0.74032015\n",
      " 0.69634932 0.83701992 0.96838838 0.99322444 0.83321756 0.9964233\n",
      " 0.99216264 0.45927787 0.5229578  0.99357295 0.85469604 0.96282458\n",
      " 0.28142083 0.95415324 0.86578661 0.98995751 0.17375103 0.90250403\n",
      " 0.25363144 0.39516738 0.20175181 0.87786764 0.16637805 0.86751878\n",
      " 0.83431131 0.59540957 0.88368315 0.78217894 0.97874397 0.96680564\n",
      " 0.95641339 0.94079065 0.91322708 0.97118992]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 138 [0/43 (0%)]\tTrain Loss: 0.037346\n",
      "Train Epoch: 138 [10/43 (23%)]\tTrain Loss: 0.052388\n",
      "Train Epoch: 138 [20/43 (47%)]\tTrain Loss: 0.018192\n",
      "Train Epoch: 138 [30/43 (70%)]\tTrain Loss: 0.019036\n",
      "Train Epoch: 138 [40/43 (93%)]\tTrain Loss: 0.043142\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.13557598 0.74106443 0.43114471 0.7197783  0.73735553 0.17485726\n",
      " 0.71847928 0.75863975 0.29890373 0.07207382 0.39758334 0.15501821\n",
      " 0.70740283 0.96952426 0.41601986 0.33702612 0.52431929 0.59263504\n",
      " 0.34117961 0.37307897 0.1643932  0.80828595 0.992149   0.94149083\n",
      " 0.78699309 0.98521394 0.13605328 0.90205753 0.41894928 0.6738283\n",
      " 0.88590837 0.38213363 0.01196132 0.30616742 0.24718952 0.02901309\n",
      " 0.10153175 0.63373786 0.60728222 0.36658418 0.47676733 0.15342954\n",
      " 0.14636669 0.32563573 0.66378951 0.63415867 0.23118302 0.15553698\n",
      " 0.86609256 0.25919896 0.93837845 0.20949268 0.17970112 0.02997985\n",
      " 0.12811045 0.39969957 0.34117985 0.02597586 0.1747721  0.26262864\n",
      " 0.91276205 0.95956647 0.97859621 0.9455592  0.52157485 0.82213622\n",
      " 0.63800895 0.99823171 0.99402231 0.8903057  0.6539079  0.76874048\n",
      " 0.66155547 0.95860779 0.59486544 0.91060251 0.99660611 0.99589962\n",
      " 0.99200869 0.96522772 0.95210159 0.99084061 0.99334151 0.4037151\n",
      " 0.62499964 0.84783602 0.96665937 0.96933383 0.63171017 0.99330348\n",
      " 0.9781999  0.67650545 0.45716175 0.9960686  0.79232037 0.33822879\n",
      " 0.25920475 0.82197464 0.76153463 0.95586181 0.33790097 0.80001467\n",
      " 0.19541363 0.17922528 0.6647765  0.64724755 0.21330488 0.51656169\n",
      " 0.67165661 0.13449776 0.77086198 0.69351733 0.98137224 0.91739345\n",
      " 0.97771215 0.77898163 0.8552525  0.9823302 ]\n",
      "predict [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 139 [0/43 (0%)]\tTrain Loss: 0.023140\n",
      "Train Epoch: 139 [10/43 (23%)]\tTrain Loss: 0.041176\n",
      "Train Epoch: 139 [20/43 (47%)]\tTrain Loss: 0.013119\n",
      "Train Epoch: 139 [30/43 (70%)]\tTrain Loss: 0.031873\n",
      "Train Epoch: 139 [40/43 (93%)]\tTrain Loss: 0.045602\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.12648472 0.84500366 0.69758713 0.78968251 0.2470727  0.08428831\n",
      " 0.848836   0.64946961 0.2120118  0.18196392 0.06508435 0.07412178\n",
      " 0.55209202 0.8756575  0.43601424 0.03969669 0.268971   0.33341762\n",
      " 0.82046187 0.21175259 0.0670361  0.93034208 0.97250181 0.11092503\n",
      " 0.79382265 0.97518265 0.0825144  0.85774237 0.5936808  0.64496338\n",
      " 0.93521529 0.73016691 0.06265682 0.06991055 0.17278795 0.01640597\n",
      " 0.03326958 0.10472187 0.33550879 0.29730633 0.30359495 0.32997918\n",
      " 0.10327041 0.20722373 0.72966677 0.13275324 0.12327394 0.09808072\n",
      " 0.61614472 0.07819866 0.07920101 0.06916296 0.13437159 0.2527521\n",
      " 0.12476555 0.19731721 0.12479312 0.10872633 0.10038365 0.18827647\n",
      " 0.86749184 0.88872814 0.96750432 0.98571801 0.77815408 0.89560938\n",
      " 0.64972442 0.99645495 0.98627013 0.76354235 0.90580243 0.77515179\n",
      " 0.58486181 0.88410711 0.54183745 0.94604445 0.99496245 0.99624169\n",
      " 0.96349818 0.9558382  0.94235015 0.98339891 0.98559648 0.43515477\n",
      " 0.60928494 0.74102229 0.91962957 0.95056623 0.65590817 0.95161551\n",
      " 0.98971063 0.78296071 0.64623249 0.99212247 0.0986364  0.07114971\n",
      " 0.12642904 0.95472109 0.67826581 0.96131778 0.29335189 0.91584617\n",
      " 0.4273383  0.24332744 0.33445725 0.65565687 0.10630716 0.31347337\n",
      " 0.58359385 0.12137084 0.1782631  0.54699576 0.96052909 0.95801866\n",
      " 0.97942835 0.81931192 0.86195904 0.97769642]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 140 [0/43 (0%)]\tTrain Loss: 0.032269\n",
      "Train Epoch: 140 [10/43 (23%)]\tTrain Loss: 0.039287\n",
      "Train Epoch: 140 [20/43 (47%)]\tTrain Loss: 0.038676\n",
      "Train Epoch: 140 [30/43 (70%)]\tTrain Loss: 0.046610\n",
      "Train Epoch: 140 [40/43 (93%)]\tTrain Loss: 0.057294\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.29085314 0.79302222 0.76661426 0.79669845 0.21189547 0.13241228\n",
      " 0.91559571 0.82244486 0.39506462 0.1397088  0.43065608 0.19297707\n",
      " 0.7346788  0.93931574 0.76247311 0.20684272 0.71887559 0.338844\n",
      " 0.46995535 0.14935414 0.18248072 0.8799836  0.95226973 0.10888352\n",
      " 0.63468689 0.91318047 0.07081095 0.87581676 0.74524784 0.47243261\n",
      " 0.88175201 0.52864236 0.0445015  0.27240258 0.2230867  0.01906072\n",
      " 0.06302606 0.18622257 0.63044322 0.27727786 0.36740592 0.51507521\n",
      " 0.20178388 0.28267542 0.81145149 0.35691118 0.23236397 0.1905587\n",
      " 0.66993809 0.16656256 0.91248649 0.23647624 0.35711679 0.05792345\n",
      " 0.23360726 0.1508785  0.16830589 0.03178321 0.23369686 0.26412189\n",
      " 0.8419016  0.61363173 0.94595468 0.96148801 0.7927891  0.92030293\n",
      " 0.83681977 0.99280095 0.99147767 0.74997491 0.75347191 0.6153\n",
      " 0.61390644 0.84139812 0.88246655 0.86904079 0.99411643 0.99681997\n",
      " 0.99327433 0.97498465 0.94354177 0.98655117 0.98057395 0.14874727\n",
      " 0.50899321 0.75843138 0.54046112 0.892694   0.76127011 0.96709913\n",
      " 0.96418512 0.7966854  0.68103433 0.98798406 0.17851394 0.10493138\n",
      " 0.22459002 0.48062816 0.34595978 0.90365493 0.30732936 0.76793212\n",
      " 0.23683213 0.2364928  0.42863864 0.39357349 0.20505506 0.54477173\n",
      " 0.81520611 0.23678577 0.80547649 0.72279179 0.97083324 0.94957328\n",
      " 0.95715088 0.88657224 0.81633723 0.95930731]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 40 FN= 11 FP= 20\n",
      "TP+FP 67\n",
      "precision 0.7014925373134329\n",
      "recall 0.8103448275862069\n",
      "F1 0.752\n",
      "acc 0.7372881355932204\n",
      "AUCp 0.7385057471264369\n",
      "AUC 0.8031609195402298\n",
      "\n",
      " The epoch is 140, average recall: 0.8103, average precision: 0.7015,average F1: 0.7520, average accuracy: 0.7373, average AUC: 0.8032\n",
      "Train Epoch: 141 [0/43 (0%)]\tTrain Loss: 0.051889\n",
      "Train Epoch: 141 [10/43 (23%)]\tTrain Loss: 0.034643\n",
      "Train Epoch: 141 [20/43 (47%)]\tTrain Loss: 0.019915\n",
      "Train Epoch: 141 [30/43 (70%)]\tTrain Loss: 0.022731\n",
      "Train Epoch: 141 [40/43 (93%)]\tTrain Loss: 0.062195\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.50499421 0.88381881 0.67955118 0.43688142 0.42784911 0.27499503\n",
      " 0.80934548 0.75381196 0.55440128 0.5039016  0.15866876 0.3038581\n",
      " 0.60407883 0.97944736 0.86065078 0.20726718 0.40115476 0.44099236\n",
      " 0.57446903 0.40907496 0.37079379 0.7194277  0.96830398 0.37704057\n",
      " 0.53841287 0.93820435 0.39035213 0.85981864 0.36438292 0.56999987\n",
      " 0.81820929 0.43749401 0.074496   0.37117815 0.4853324  0.01903264\n",
      " 0.0538191  0.40418163 0.41470364 0.29836324 0.28167492 0.66029882\n",
      " 0.53486723 0.17000122 0.6433233  0.30412111 0.20600781 0.09317347\n",
      " 0.59666175 0.3543179  0.81021267 0.29759482 0.62563205 0.04689769\n",
      " 0.43511161 0.06518539 0.45390251 0.01696032 0.41480958 0.15618165\n",
      " 0.89918816 0.96313441 0.97885108 0.95029777 0.52200091 0.96248066\n",
      " 0.83798295 0.99801046 0.98699552 0.73055369 0.88839352 0.85602015\n",
      " 0.62677586 0.71005177 0.72984034 0.93362868 0.99776101 0.99659085\n",
      " 0.97237825 0.9704361  0.97392219 0.98869562 0.99502724 0.52482873\n",
      " 0.36243474 0.81033802 0.94327533 0.97766602 0.77683318 0.98882467\n",
      " 0.98918903 0.68528074 0.5167399  0.99630392 0.49750811 0.2811887\n",
      " 0.53244239 0.96049279 0.65067673 0.98067862 0.19704849 0.89793873\n",
      " 0.67976981 0.22866277 0.35504261 0.53379768 0.29833603 0.33386096\n",
      " 0.72094142 0.31023031 0.78044051 0.6319719  0.97487122 0.9626596\n",
      " 0.98958683 0.85463965 0.82044178 0.97592556]\n",
      "predict [1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 142 [0/43 (0%)]\tTrain Loss: 0.032248\n",
      "Train Epoch: 142 [10/43 (23%)]\tTrain Loss: 0.019449\n",
      "Train Epoch: 142 [20/43 (47%)]\tTrain Loss: 0.024001\n",
      "Train Epoch: 142 [30/43 (70%)]\tTrain Loss: 0.031251\n",
      "Train Epoch: 142 [40/43 (93%)]\tTrain Loss: 0.039634\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.29922926 0.81936693 0.77643913 0.35238111 0.32209769 0.26479313\n",
      " 0.83028954 0.57711095 0.30005839 0.29941127 0.29081708 0.26768857\n",
      " 0.74988741 0.98925436 0.69198871 0.2121219  0.33155906 0.16063775\n",
      " 0.28135067 0.24283765 0.2559388  0.72337812 0.95460916 0.18902297\n",
      " 0.30552849 0.90675372 0.25511277 0.78427547 0.23149909 0.16270505\n",
      " 0.88715488 0.33619323 0.02351699 0.25165233 0.4765794  0.06236535\n",
      " 0.06837604 0.38775158 0.18489322 0.1603609  0.29051909 0.22916172\n",
      " 0.18252198 0.1832573  0.68780476 0.42799109 0.25976998 0.49000856\n",
      " 0.82373267 0.19231078 0.95076787 0.30067283 0.11641252 0.02890086\n",
      " 0.2798987  0.13360849 0.33909473 0.03428865 0.26285681 0.18906595\n",
      " 0.90906715 0.86686426 0.93862152 0.94549078 0.77791172 0.94907242\n",
      " 0.77385467 0.99823391 0.99754    0.89859104 0.80137438 0.57336819\n",
      " 0.80398142 0.92631608 0.49645364 0.93583    0.99856633 0.99790514\n",
      " 0.98940837 0.9765712  0.89608967 0.99027973 0.99605417 0.65425694\n",
      " 0.37836212 0.81959009 0.89574498 0.97758001 0.7513783  0.97048563\n",
      " 0.97876203 0.7532478  0.75636721 0.99666059 0.33567286 0.39609179\n",
      " 0.22572593 0.88186586 0.38870311 0.91813606 0.27963558 0.72194737\n",
      " 0.12287484 0.08155054 0.50823432 0.64657843 0.20661497 0.35663158\n",
      " 0.48908341 0.23636925 0.60670125 0.76522017 0.9912923  0.9274298\n",
      " 0.9740535  0.94205123 0.92820346 0.939035  ]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 143 [0/43 (0%)]\tTrain Loss: 0.038513\n",
      "Train Epoch: 143 [10/43 (23%)]\tTrain Loss: 0.016590\n",
      "Train Epoch: 143 [20/43 (47%)]\tTrain Loss: 0.040466\n",
      "Train Epoch: 143 [30/43 (70%)]\tTrain Loss: 0.041568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 143 [40/43 (93%)]\tTrain Loss: 0.029769\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.5084784  0.83697933 0.60525    0.75889879 0.43797949 0.5071522\n",
      " 0.82621664 0.68388873 0.43744537 0.46916363 0.45937118 0.65463948\n",
      " 0.53065115 0.98083031 0.62668842 0.12668185 0.2869713  0.42080581\n",
      " 0.36439514 0.5569483  0.68799943 0.86599737 0.95534712 0.61442399\n",
      " 0.59330428 0.97172719 0.43022218 0.88212764 0.58415377 0.27058813\n",
      " 0.94167697 0.47923028 0.13365379 0.57856995 0.18780559 0.02472641\n",
      " 0.05628029 0.70591402 0.58519828 0.13989857 0.18947072 0.29985696\n",
      " 0.43077055 0.20207383 0.69490546 0.2558077  0.10230147 0.15724263\n",
      " 0.77855271 0.59883958 0.82200903 0.61014867 0.13117921 0.04262437\n",
      " 0.596304   0.06545681 0.41820866 0.03299903 0.58383918 0.16990949\n",
      " 0.9452126  0.93604618 0.98191708 0.96037263 0.71654826 0.80589432\n",
      " 0.57914644 0.99784422 0.99817181 0.85628974 0.85536969 0.80963963\n",
      " 0.65146393 0.93377405 0.67800444 0.9440316  0.99793315 0.99701047\n",
      " 0.99073797 0.98767185 0.93804562 0.99052608 0.99391586 0.49598223\n",
      " 0.44213772 0.78277957 0.97415859 0.99127114 0.76259434 0.99334937\n",
      " 0.99451679 0.61951393 0.74351275 0.99654901 0.59280258 0.62121594\n",
      " 0.65496832 0.90372193 0.71039903 0.9758991  0.33422616 0.76491863\n",
      " 0.64524877 0.25286356 0.72972196 0.36909667 0.63598633 0.46328357\n",
      " 0.49203283 0.46145347 0.87799531 0.74992967 0.97752404 0.96934366\n",
      " 0.98462832 0.93154961 0.91385853 0.94995379]\n",
      "predict [1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 144 [0/43 (0%)]\tTrain Loss: 0.060847\n",
      "Train Epoch: 144 [10/43 (23%)]\tTrain Loss: 0.037616\n",
      "Train Epoch: 144 [20/43 (47%)]\tTrain Loss: 0.077870\n",
      "Train Epoch: 144 [30/43 (70%)]\tTrain Loss: 0.026455\n",
      "Train Epoch: 144 [40/43 (93%)]\tTrain Loss: 0.014979\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.39129192 0.88255346 0.9223873  0.61503369 0.21837042 0.17067759\n",
      " 0.90707171 0.58295238 0.25146756 0.20892045 0.22375883 0.13174671\n",
      " 0.34306717 0.94599396 0.74515253 0.14992024 0.57614809 0.60373932\n",
      " 0.61197114 0.24314339 0.23794119 0.91716272 0.97533596 0.24921463\n",
      " 0.6826812  0.94567871 0.17496742 0.88437051 0.33515435 0.46502599\n",
      " 0.88642633 0.54497522 0.07739583 0.26929268 0.33731416 0.04334128\n",
      " 0.12539937 0.2239434  0.5389716  0.3525742  0.33663595 0.18328194\n",
      " 0.16461241 0.12102329 0.75493431 0.17450775 0.08755734 0.04809802\n",
      " 0.82052982 0.26051182 0.84653121 0.15554455 0.05924458 0.02884406\n",
      " 0.21006171 0.03174584 0.14689225 0.01284714 0.19445266 0.09479875\n",
      " 0.92520356 0.8862226  0.93687099 0.95172018 0.53584999 0.84306532\n",
      " 0.80938578 0.99668401 0.9911142  0.62651414 0.86125392 0.81169736\n",
      " 0.7943725  0.87597734 0.68989718 0.94156468 0.99369806 0.99576688\n",
      " 0.99273419 0.94955486 0.98824221 0.97628379 0.98703879 0.40364501\n",
      " 0.54354209 0.89549488 0.92316997 0.96154469 0.72549319 0.99511272\n",
      " 0.99059641 0.49401864 0.31983274 0.99091828 0.26966885 0.2054905\n",
      " 0.31478065 0.91816562 0.69384819 0.95776373 0.1857691  0.84472197\n",
      " 0.3050991  0.17020966 0.25125039 0.67494261 0.1855664  0.21910632\n",
      " 0.54632044 0.16426367 0.67793936 0.57383907 0.96133852 0.91117167\n",
      " 0.97903425 0.94128263 0.81891912 0.98310798]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 145 [0/43 (0%)]\tTrain Loss: 0.030928\n",
      "Train Epoch: 145 [10/43 (23%)]\tTrain Loss: 0.026098\n",
      "Train Epoch: 145 [20/43 (47%)]\tTrain Loss: 0.018520\n",
      "Train Epoch: 145 [30/43 (70%)]\tTrain Loss: 0.029236\n",
      "Train Epoch: 145 [40/43 (93%)]\tTrain Loss: 0.038028\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.55595618 0.891325   0.70483398 0.64875978 0.67681444 0.50279045\n",
      " 0.91371387 0.84592122 0.64163905 0.28632239 0.21466577 0.38579002\n",
      " 0.76949298 0.9804551  0.74755138 0.34155881 0.7750864  0.41928643\n",
      " 0.85054088 0.60092396 0.61967337 0.95813525 0.9946571  0.92870378\n",
      " 0.90452665 0.9915241  0.94244397 0.80846173 0.68872124 0.86037612\n",
      " 0.84892464 0.67043853 0.22323675 0.55716377 0.04979889 0.08438625\n",
      " 0.13375023 0.67552614 0.91052514 0.79991341 0.61720765 0.70796335\n",
      " 0.35223502 0.18529734 0.89408445 0.39673978 0.15322565 0.14268696\n",
      " 0.75199181 0.43081748 0.81618202 0.55965662 0.10172451 0.07690503\n",
      " 0.44729367 0.27192619 0.62474644 0.04312686 0.30730703 0.22149609\n",
      " 0.92322534 0.90136391 0.9102394  0.96415919 0.86486059 0.96908355\n",
      " 0.88049591 0.99963796 0.9988569  0.89416385 0.60853302 0.66533625\n",
      " 0.74102944 0.8264094  0.85902768 0.96688217 0.99856013 0.99707127\n",
      " 0.99100351 0.9901613  0.97953653 0.98620158 0.99601257 0.81191617\n",
      " 0.58330846 0.81203717 0.915856   0.99110669 0.94105464 0.99805069\n",
      " 0.99790949 0.83900285 0.85646403 0.99738079 0.88904148 0.60112727\n",
      " 0.36376992 0.92385441 0.85004908 0.97609961 0.14382295 0.90997219\n",
      " 0.83581692 0.31474963 0.41524309 0.69417006 0.46739793 0.46363935\n",
      " 0.82717741 0.54886395 0.86324859 0.69186926 0.99480301 0.97114438\n",
      " 0.98673207 0.98351812 0.88594162 0.9132579 ]\n",
      "predict [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 146 [0/43 (0%)]\tTrain Loss: 0.022759\n",
      "Train Epoch: 146 [10/43 (23%)]\tTrain Loss: 0.018330\n",
      "Train Epoch: 146 [20/43 (47%)]\tTrain Loss: 0.020177\n",
      "Train Epoch: 146 [30/43 (70%)]\tTrain Loss: 0.045113\n",
      "Train Epoch: 146 [40/43 (93%)]\tTrain Loss: 0.028705\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.29319879 0.91657549 0.80927187 0.34825787 0.37478322 0.35188636\n",
      " 0.7980864  0.61965775 0.43377042 0.20649877 0.35957199 0.41567123\n",
      " 0.50556451 0.97513765 0.32202625 0.08048908 0.3538301  0.52318758\n",
      " 0.44472328 0.42369235 0.53406411 0.80041814 0.98392439 0.33101752\n",
      " 0.79622471 0.97075605 0.59236383 0.56747228 0.27607968 0.24636382\n",
      " 0.74344027 0.37116027 0.20125033 0.55337924 0.43117642 0.02834521\n",
      " 0.05483392 0.54438061 0.54766691 0.44873497 0.25063953 0.40917203\n",
      " 0.48078996 0.13717625 0.60073352 0.22119132 0.47605965 0.12863371\n",
      " 0.71475905 0.55262017 0.80832458 0.46674874 0.67619884 0.03582714\n",
      " 0.37580168 0.0473603  0.59467363 0.05660401 0.348297   0.14643623\n",
      " 0.94975847 0.87494016 0.96015978 0.96960115 0.50102341 0.85044116\n",
      " 0.6370675  0.99798429 0.99341398 0.43540725 0.82929105 0.86519545\n",
      " 0.70271605 0.83686155 0.5513984  0.9306677  0.99778205 0.99747086\n",
      " 0.98709756 0.93697834 0.93243086 0.98550159 0.99006581 0.57051039\n",
      " 0.42306802 0.77746964 0.97631186 0.94449914 0.81641388 0.98617005\n",
      " 0.98271483 0.64772153 0.32643318 0.99774075 0.50262147 0.64688981\n",
      " 0.39652634 0.86220074 0.60464442 0.95226002 0.28095651 0.80902368\n",
      " 0.25825581 0.16743551 0.20307057 0.43436787 0.39966702 0.32588452\n",
      " 0.50881845 0.2818622  0.57681167 0.76741779 0.95344269 0.8909592\n",
      " 0.98087066 0.97859198 0.77334803 0.94334579]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 147 [0/43 (0%)]\tTrain Loss: 0.016429\n",
      "Train Epoch: 147 [10/43 (23%)]\tTrain Loss: 0.036089\n",
      "Train Epoch: 147 [20/43 (47%)]\tTrain Loss: 0.053798\n",
      "Train Epoch: 147 [30/43 (70%)]\tTrain Loss: 0.051492\n",
      "Train Epoch: 147 [40/43 (93%)]\tTrain Loss: 0.057883\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.11005612 0.83776975 0.74211735 0.60860062 0.07613841 0.10314882\n",
      " 0.80880946 0.81507021 0.17187098 0.09007668 0.26451734 0.07929109\n",
      " 0.81565177 0.9243412  0.79367888 0.35916781 0.72939849 0.43981063\n",
      " 0.34162471 0.10341001 0.16035877 0.87530261 0.98349285 0.12595551\n",
      " 0.60050827 0.84278411 0.17164992 0.85710686 0.33540195 0.66923517\n",
      " 0.83215058 0.39628676 0.01832394 0.18822932 0.04855311 0.02379404\n",
      " 0.16308501 0.04714889 0.79359359 0.46679842 0.48530164 0.27977973\n",
      " 0.16008709 0.42866409 0.7731024  0.45201111 0.08392413 0.07511299\n",
      " 0.77639663 0.11920439 0.64771402 0.12110192 0.09174444 0.03184132\n",
      " 0.12370694 0.12340616 0.15820079 0.04216155 0.10442884 0.13963476\n",
      " 0.94562626 0.72966421 0.93679702 0.98331094 0.86472344 0.87885207\n",
      " 0.82551396 0.98484498 0.99144357 0.83255082 0.78709203 0.77083451\n",
      " 0.75207561 0.87407583 0.84244692 0.92583168 0.99454534 0.98676467\n",
      " 0.99343491 0.94009978 0.97121483 0.98910493 0.96293241 0.29417309\n",
      " 0.38808566 0.81880587 0.9438796  0.95559907 0.96825224 0.99448121\n",
      " 0.98841208 0.71861643 0.58749551 0.99500102 0.12622996 0.15567535\n",
      " 0.10490523 0.93677598 0.60060197 0.9387641  0.295439   0.85414994\n",
      " 0.13889997 0.11174077 0.24546438 0.66329569 0.18125199 0.17233522\n",
      " 0.87860417 0.07480452 0.94896728 0.59772629 0.95771909 0.90363574\n",
      " 0.98220074 0.77691817 0.92051941 0.97724152]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 148 [0/43 (0%)]\tTrain Loss: 0.040489\n",
      "Train Epoch: 148 [10/43 (23%)]\tTrain Loss: 0.042978\n",
      "Train Epoch: 148 [20/43 (47%)]\tTrain Loss: 0.064748\n",
      "Train Epoch: 148 [30/43 (70%)]\tTrain Loss: 0.040100\n",
      "Train Epoch: 148 [40/43 (93%)]\tTrain Loss: 0.022222\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.24968788 0.5426966  0.41917327 0.71360517 0.31837323 0.1684759\n",
      " 0.62571138 0.75865722 0.27284178 0.17036231 0.61449438 0.42481586\n",
      " 0.57844961 0.97154701 0.25449988 0.07641608 0.35846993 0.63546389\n",
      " 0.3943862  0.31358033 0.22404557 0.85191721 0.96957481 0.3592543\n",
      " 0.70832491 0.96238029 0.15153779 0.91744745 0.33792996 0.42095637\n",
      " 0.88189536 0.39974475 0.03579877 0.49197468 0.02898773 0.04637708\n",
      " 0.17141069 0.35674062 0.83282179 0.2225312  0.44815794 0.68990856\n",
      " 0.29967675 0.19805738 0.68895316 0.27152973 0.07871559 0.12813739\n",
      " 0.85840207 0.38360909 0.90827322 0.38595095 0.09855235 0.021109\n",
      " 0.32330808 0.12457    0.27468601 0.02183745 0.25753754 0.05719927\n",
      " 0.87398541 0.77376586 0.94673181 0.96688265 0.74197692 0.79192334\n",
      " 0.83612251 0.996732   0.97972065 0.58262503 0.79124182 0.68052208\n",
      " 0.66046709 0.83960193 0.58966917 0.8329947  0.9968971  0.9974618\n",
      " 0.98333091 0.95836365 0.94653428 0.97461277 0.98469573 0.52173084\n",
      " 0.28834906 0.71170717 0.97270614 0.97097689 0.75999355 0.99353987\n",
      " 0.94897854 0.62827158 0.53485513 0.99673235 0.36333919 0.11482007\n",
      " 0.37033969 0.9485144  0.64301163 0.99381083 0.2999562  0.90203238\n",
      " 0.20938936 0.17719989 0.43492362 0.65510345 0.32791418 0.23286298\n",
      " 0.71477878 0.22332472 0.84257287 0.76884305 0.9898414  0.94235587\n",
      " 0.97717559 0.85504359 0.85606784 0.9539538 ]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 149 [0/43 (0%)]\tTrain Loss: 0.060105\n",
      "Train Epoch: 149 [10/43 (23%)]\tTrain Loss: 0.025381\n",
      "Train Epoch: 149 [20/43 (47%)]\tTrain Loss: 0.044633\n",
      "Train Epoch: 149 [30/43 (70%)]\tTrain Loss: 0.028778\n",
      "Train Epoch: 149 [40/43 (93%)]\tTrain Loss: 0.037657\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.32188901 0.68304849 0.43080443 0.66061693 0.35937434 0.43736026\n",
      " 0.77870005 0.51623142 0.30106032 0.34286305 0.22400063 0.33434537\n",
      " 0.52496564 0.9596622  0.24554355 0.14485088 0.23793598 0.46035317\n",
      " 0.33031726 0.71116859 0.44014964 0.84671867 0.965846   0.34584624\n",
      " 0.68186796 0.98883623 0.3098245  0.79348457 0.43648991 0.56068909\n",
      " 0.92761368 0.11426717 0.06942696 0.48647228 0.3454622  0.02746616\n",
      " 0.07774173 0.33799225 0.75633347 0.5106346  0.44299147 0.54632419\n",
      " 0.37587038 0.1510421  0.66651982 0.33647192 0.08073336 0.12332091\n",
      " 0.82432133 0.48424461 0.93557668 0.28754067 0.04152464 0.01224941\n",
      " 0.41420618 0.02696469 0.35480312 0.04620113 0.33069867 0.04107992\n",
      " 0.94629371 0.81684381 0.97685111 0.98566335 0.41498646 0.82996112\n",
      " 0.66874218 0.99935931 0.99547219 0.62687844 0.55562234 0.69971728\n",
      " 0.59750885 0.85301954 0.61824244 0.91764373 0.9970721  0.99720335\n",
      " 0.98364407 0.97357064 0.93250084 0.979909   0.99189925 0.71028435\n",
      " 0.40996325 0.86617208 0.9777289  0.98774999 0.75675815 0.99415487\n",
      " 0.99398285 0.38984466 0.49766195 0.99850863 0.38563627 0.29406238\n",
      " 0.44136634 0.98033524 0.87301791 0.97789222 0.19332244 0.83556575\n",
      " 0.51340699 0.19103183 0.20254457 0.57951778 0.47521028 0.39204505\n",
      " 0.70327491 0.37764347 0.80796111 0.46611798 0.98710817 0.95521909\n",
      " 0.96865213 0.97695482 0.67576647 0.97777098]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 150 [0/43 (0%)]\tTrain Loss: 0.029550\n",
      "Train Epoch: 150 [10/43 (23%)]\tTrain Loss: 0.054326\n",
      "Train Epoch: 150 [20/43 (47%)]\tTrain Loss: 0.082721\n",
      "Train Epoch: 150 [30/43 (70%)]\tTrain Loss: 0.034098\n",
      "Train Epoch: 150 [40/43 (93%)]\tTrain Loss: 0.033954\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.47942847 0.80544609 0.71381789 0.73688787 0.38883692 0.36673898\n",
      " 0.71484894 0.75458533 0.41914994 0.38034433 0.4643347  0.30051985\n",
      " 0.89484596 0.97992903 0.87288225 0.11693031 0.47696757 0.21125041\n",
      " 0.54503149 0.31480128 0.41594252 0.75732315 0.9491685  0.32087487\n",
      " 0.64209503 0.8690958  0.35010812 0.78973573 0.36798042 0.29377562\n",
      " 0.77021229 0.35042652 0.03377314 0.42110276 0.06159935 0.03116912\n",
      " 0.1213446  0.68568063 0.78514445 0.57280791 0.27615231 0.36565739\n",
      " 0.42886621 0.12785488 0.60859466 0.29793286 0.18044105 0.08594103\n",
      " 0.7521863  0.31042856 0.90684009 0.06182665 0.14680819 0.05928206\n",
      " 0.18482114 0.21674493 0.40816301 0.0655057  0.37600049 0.1178543\n",
      " 0.89380813 0.80400026 0.9360112  0.96678835 0.65773648 0.89161688\n",
      " 0.72721064 0.9976567  0.99704933 0.70679748 0.59470761 0.57727921\n",
      " 0.62550515 0.74902552 0.36760622 0.95505214 0.99646628 0.99525839\n",
      " 0.97431016 0.95771617 0.96358192 0.98718911 0.98990178 0.33016297\n",
      " 0.29117262 0.71390772 0.98557729 0.98380208 0.94027382 0.99639136\n",
      " 0.99019647 0.69051313 0.46237442 0.9932059  0.41512311 0.54248852\n",
      " 0.48518148 0.95723212 0.62604171 0.96378672 0.14430884 0.68313956\n",
      " 0.43719032 0.11739951 0.24974951 0.51746792 0.38757497 0.48233464\n",
      " 0.6641804  0.38824329 0.89990205 0.41133142 0.97677952 0.93168533\n",
      " 0.98639894 0.94527429 0.92123675 0.94820279]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 42 FN= 11 FP= 18\n",
      "TP+FP 65\n",
      "precision 0.7230769230769231\n",
      "recall 0.8103448275862069\n",
      "F1 0.7642276422764227\n",
      "acc 0.7542372881355932\n",
      "AUCp 0.7551724137931034\n",
      "AUC 0.8135057471264369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 150, average recall: 0.8103, average precision: 0.7231,average F1: 0.7642, average accuracy: 0.7542, average AUC: 0.8135\n",
      "Train Epoch: 151 [0/43 (0%)]\tTrain Loss: 0.016121\n",
      "Train Epoch: 151 [10/43 (23%)]\tTrain Loss: 0.028706\n",
      "Train Epoch: 151 [20/43 (47%)]\tTrain Loss: 0.017558\n",
      "Train Epoch: 151 [30/43 (70%)]\tTrain Loss: 0.030424\n",
      "Train Epoch: 151 [40/43 (93%)]\tTrain Loss: 0.061835\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.11326509 0.91404563 0.88520855 0.88234949 0.11326797 0.16791523\n",
      " 0.83800185 0.87114227 0.12215164 0.16827764 0.61635524 0.13349737\n",
      " 0.91513515 0.96933573 0.90259188 0.29984057 0.73851657 0.38075602\n",
      " 0.60764974 0.16996805 0.13082802 0.92924833 0.98703754 0.30361387\n",
      " 0.79625994 0.98963869 0.25200453 0.94473612 0.60597032 0.83055341\n",
      " 0.91181999 0.65976787 0.09474147 0.19276427 0.04943654 0.07742319\n",
      " 0.31932399 0.25314835 0.82216257 0.47121578 0.53834128 0.79454386\n",
      " 0.16625211 0.44342351 0.89464545 0.50754696 0.16233706 0.1389294\n",
      " 0.85141975 0.17762159 0.95557666 0.41364464 0.31392539 0.13156255\n",
      " 0.22527961 0.24417715 0.18260573 0.04279777 0.23132722 0.24241294\n",
      " 0.95458645 0.86464727 0.89935809 0.96693391 0.95605475 0.94397199\n",
      " 0.92111278 0.99719381 0.99656397 0.77576947 0.83346128 0.83719927\n",
      " 0.88105607 0.95361537 0.88082606 0.87955803 0.99588156 0.99649566\n",
      " 0.9942531  0.98236299 0.97099179 0.98538291 0.99397993 0.68884647\n",
      " 0.65378904 0.87849957 0.9584415  0.97575879 0.96979302 0.99306548\n",
      " 0.98975509 0.69868803 0.79660159 0.9951036  0.15119326 0.07792342\n",
      " 0.27254879 0.92594349 0.96719331 0.98735875 0.3319732  0.94541538\n",
      " 0.86126214 0.24201357 0.29870069 0.32868415 0.22690365 0.13344729\n",
      " 0.88594413 0.14997974 0.97405666 0.7864396  0.99203193 0.96741062\n",
      " 0.96554971 0.98103839 0.88140464 0.97238803]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 152 [0/43 (0%)]\tTrain Loss: 0.041112\n",
      "Train Epoch: 152 [10/43 (23%)]\tTrain Loss: 0.014654\n",
      "Train Epoch: 152 [20/43 (47%)]\tTrain Loss: 0.041812\n",
      "Train Epoch: 152 [30/43 (70%)]\tTrain Loss: 0.026312\n",
      "Train Epoch: 152 [40/43 (93%)]\tTrain Loss: 0.015699\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.48053461 0.71979904 0.45330897 0.61575848 0.43287858 0.1635956\n",
      " 0.57648271 0.86827797 0.57582015 0.41641212 0.30545557 0.42157581\n",
      " 0.87380695 0.98707098 0.68470937 0.14502351 0.48262382 0.48085836\n",
      " 0.11154318 0.41352549 0.38400656 0.82158023 0.97751981 0.58844417\n",
      " 0.75212741 0.98606455 0.35849053 0.88617343 0.37312651 0.60407352\n",
      " 0.88341707 0.47857285 0.07810941 0.39539358 0.06530513 0.02880386\n",
      " 0.1150142  0.5722717  0.53235924 0.38438755 0.43900982 0.45152017\n",
      " 0.26041719 0.07423423 0.33055919 0.41315982 0.11958885 0.21676394\n",
      " 0.85413808 0.42902178 0.84190464 0.47954288 0.07933172 0.05697769\n",
      " 0.24383874 0.04972803 0.56834543 0.01908794 0.51318216 0.10608288\n",
      " 0.93502492 0.93795586 0.9643628  0.97504836 0.75433135 0.96277791\n",
      " 0.89331353 0.99573177 0.99667573 0.84209079 0.86409926 0.91076505\n",
      " 0.58562094 0.94205838 0.69130391 0.93713236 0.99875653 0.99725121\n",
      " 0.98864424 0.98647445 0.9802115  0.98493469 0.99521905 0.50995916\n",
      " 0.37330151 0.87959051 0.97541523 0.99421883 0.75316417 0.99289674\n",
      " 0.98183882 0.84841061 0.47579473 0.99900657 0.438945   0.52422398\n",
      " 0.60332417 0.83645576 0.86027175 0.97002751 0.32481235 0.89016247\n",
      " 0.3786965  0.05699052 0.42296013 0.39953503 0.37592548 0.6238488\n",
      " 0.50898349 0.14409445 0.81712306 0.625287   0.98762518 0.94509429\n",
      " 0.98782569 0.96842659 0.86860633 0.96255875]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 153 [0/43 (0%)]\tTrain Loss: 0.024776\n",
      "Train Epoch: 153 [10/43 (23%)]\tTrain Loss: 0.020566\n",
      "Train Epoch: 153 [20/43 (47%)]\tTrain Loss: 0.041731\n",
      "Train Epoch: 153 [30/43 (70%)]\tTrain Loss: 0.035005\n",
      "Train Epoch: 153 [40/43 (93%)]\tTrain Loss: 0.032577\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.24012727 0.90906703 0.82915157 0.1441334  0.26754439 0.24189529\n",
      " 0.86771429 0.17004995 0.24794716 0.29423487 0.20405965 0.2402399\n",
      " 0.66401994 0.96654767 0.58830476 0.07773972 0.3135505  0.37043485\n",
      " 0.18031864 0.20068435 0.23277321 0.88080084 0.97741538 0.10112293\n",
      " 0.50804287 0.21301906 0.3263123  0.7032215  0.50214535 0.12826136\n",
      " 0.91967624 0.25405827 0.07836678 0.19817649 0.21625449 0.46697611\n",
      " 0.06963893 0.2871671  0.60418427 0.06865598 0.11895047 0.28414294\n",
      " 0.2532976  0.097387   0.35711324 0.29251835 0.12511569 0.13918233\n",
      " 0.91609335 0.23318644 0.96642727 0.29769796 0.06554899 0.00757631\n",
      " 0.3248468  0.03594323 0.28551599 0.00733062 0.29314381 0.10375071\n",
      " 0.96478146 0.97036415 0.9781757  0.97056931 0.62657315 0.94264221\n",
      " 0.89339536 0.99573082 0.99345219 0.67805481 0.91746527 0.90978056\n",
      " 0.58887476 0.86662829 0.49385121 0.97968918 0.99928844 0.99840504\n",
      " 0.99825984 0.98655325 0.90173751 0.9896006  0.99405998 0.65691692\n",
      " 0.20674668 0.93033278 0.97575808 0.99208695 0.83474839 0.99207616\n",
      " 0.96867776 0.63687402 0.37545884 0.11683694 0.14601763 0.36463076\n",
      " 0.13282079 0.9472723  0.74635434 0.97493762 0.34737211 0.92131841\n",
      " 0.35322767 0.12498533 0.16437988 0.39111915 0.40910143 0.17898847\n",
      " 0.57806361 0.33091491 0.16357465 0.35878852 0.96567321 0.90039498\n",
      " 0.99081343 0.91159028 0.89752096 0.97410959]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 154 [0/43 (0%)]\tTrain Loss: 0.020629\n",
      "Train Epoch: 154 [10/43 (23%)]\tTrain Loss: 0.024439\n",
      "Train Epoch: 154 [20/43 (47%)]\tTrain Loss: 0.037206\n",
      "Train Epoch: 154 [30/43 (70%)]\tTrain Loss: 0.038360\n",
      "Train Epoch: 154 [40/43 (93%)]\tTrain Loss: 0.052399\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.49136713 0.95381725 0.78905439 0.75961268 0.49640137 0.38544345\n",
      " 0.93003875 0.82942122 0.43661064 0.24578096 0.24204125 0.33102074\n",
      " 0.79739779 0.98420262 0.91407621 0.24252956 0.58481479 0.60751241\n",
      " 0.42461914 0.38499805 0.46086958 0.94517481 0.98868877 0.34047204\n",
      " 0.71751863 0.94750834 0.30458215 0.89497542 0.28461394 0.6260801\n",
      " 0.9447189  0.72650921 0.11500079 0.3503564  0.09241589 0.05686472\n",
      " 0.12466828 0.4061411  0.76543832 0.4179785  0.60794801 0.51793218\n",
      " 0.43845373 0.23325559 0.69347423 0.61901027 0.37082353 0.28552079\n",
      " 0.9612993  0.41117215 0.9565295  0.37571138 0.09544045 0.04013408\n",
      " 0.36880592 0.0402583  0.41873062 0.02908836 0.31982625 0.13325103\n",
      " 0.9481802  0.96653605 0.97876692 0.97068632 0.90602273 0.96363604\n",
      " 0.83151901 0.999569   0.99807692 0.95115221 0.9202674  0.94293839\n",
      " 0.86020231 0.92513859 0.7728436  0.9837445  0.9994666  0.99913102\n",
      " 0.99592316 0.99276334 0.98434305 0.99712437 0.99581748 0.89237541\n",
      " 0.66157281 0.91225344 0.97684276 0.98427457 0.97177875 0.98051858\n",
      " 0.99079812 0.7913745  0.51006043 0.99844486 0.58774316 0.50948173\n",
      " 0.43929231 0.98697531 0.76020885 0.99746025 0.30030954 0.94699287\n",
      " 0.84100956 0.13560109 0.51118249 0.71214694 0.41876191 0.3818765\n",
      " 0.74515849 0.26390219 0.8006534  0.69469494 0.99467373 0.97509116\n",
      " 0.99221295 0.98016524 0.9390167  0.98471975]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 155 [0/43 (0%)]\tTrain Loss: 0.013236\n",
      "Train Epoch: 155 [10/43 (23%)]\tTrain Loss: 0.034037\n",
      "Train Epoch: 155 [20/43 (47%)]\tTrain Loss: 0.011666\n",
      "Train Epoch: 155 [30/43 (70%)]\tTrain Loss: 0.019449\n",
      "Train Epoch: 155 [40/43 (93%)]\tTrain Loss: 0.027802\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.29133654 0.59093153 0.324911   0.6619913  0.19059823 0.31870663\n",
      " 0.42622715 0.56889713 0.17675464 0.23218814 0.40181553 0.14672114\n",
      " 0.39170381 0.89406008 0.34762502 0.08743802 0.1690494  0.47959813\n",
      " 0.48841414 0.17039721 0.38422108 0.81070346 0.98349792 0.71048099\n",
      " 0.54639941 0.9679938  0.16602126 0.8856563  0.20276971 0.28808895\n",
      " 0.92014271 0.26613155 0.02895676 0.24360088 0.10732177 0.02553721\n",
      " 0.04450928 0.34992033 0.67984861 0.08225695 0.2221352  0.33235282\n",
      " 0.21317272 0.10208549 0.52361417 0.4171114  0.16136853 0.21934204\n",
      " 0.92088193 0.24745886 0.91464728 0.28174809 0.12718374 0.01180846\n",
      " 0.15237567 0.02784544 0.55670601 0.01163364 0.25012738 0.06206838\n",
      " 0.94348019 0.83558047 0.96536672 0.98072171 0.65533036 0.85258764\n",
      " 0.79844391 0.98854113 0.9935987  0.73700529 0.80203712 0.724558\n",
      " 0.57571191 0.91919273 0.63362885 0.8796252  0.99551588 0.9972052\n",
      " 0.98537433 0.96626872 0.94647509 0.99059278 0.99558723 0.56355602\n",
      " 0.48098522 0.92897224 0.94432658 0.97589409 0.50207013 0.96141452\n",
      " 0.9512822  0.66055888 0.42442232 0.9969998  0.26282772 0.26523319\n",
      " 0.20166734 0.88141286 0.70755261 0.9071936  0.30691475 0.75718766\n",
      " 0.23680578 0.07380015 0.21732828 0.16709591 0.20881467 0.30001992\n",
      " 0.66369039 0.22312963 0.79268056 0.44485903 0.95007366 0.9511041\n",
      " 0.95576715 0.82153606 0.73652685 0.92004383]\n",
      "predict [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 156 [0/43 (0%)]\tTrain Loss: 0.035406\n",
      "Train Epoch: 156 [10/43 (23%)]\tTrain Loss: 0.065916\n",
      "Train Epoch: 156 [20/43 (47%)]\tTrain Loss: 0.025496\n",
      "Train Epoch: 156 [30/43 (70%)]\tTrain Loss: 0.039002\n",
      "Train Epoch: 156 [40/43 (93%)]\tTrain Loss: 0.035385\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.13601409 0.60661852 0.33510342 0.74772114 0.27377585 0.29952052\n",
      " 0.5007636  0.59520143 0.31753951 0.26922742 0.36080849 0.25893915\n",
      " 0.58408624 0.9256866  0.62725776 0.06748436 0.20408864 0.26151386\n",
      " 0.29968411 0.2269709  0.23169219 0.65242153 0.95517778 0.40994459\n",
      " 0.70031613 0.90246618 0.18387394 0.83271825 0.38168913 0.6296711\n",
      " 0.65132439 0.47458285 0.02102488 0.33113405 0.0280464  0.03096863\n",
      " 0.06529915 0.51301825 0.60180372 0.22315039 0.28876391 0.24711171\n",
      " 0.32291529 0.06755327 0.72454387 0.17626584 0.08933155 0.21825525\n",
      " 0.69977438 0.29544416 0.92136133 0.04442238 0.06035906 0.02958574\n",
      " 0.43160921 0.03626012 0.38301298 0.00606855 0.17209528 0.07760329\n",
      " 0.93977654 0.87552214 0.94956529 0.95945442 0.86985594 0.89302826\n",
      " 0.769494   0.9936744  0.99080771 0.65093929 0.65993255 0.5951466\n",
      " 0.63779676 0.89280033 0.58368593 0.88846713 0.99632251 0.99561357\n",
      " 0.9858073  0.94577605 0.94036734 0.98361939 0.99104828 0.59013683\n",
      " 0.25316972 0.68912894 0.98014414 0.9730016  0.76495486 0.98011094\n",
      " 0.98534435 0.68952066 0.5414176  0.98303968 0.3924717  0.30041155\n",
      " 0.33034319 0.91825324 0.77788901 0.941401   0.28644216 0.60660774\n",
      " 0.14156808 0.05441668 0.28353158 0.54392821 0.2589317  0.31416431\n",
      " 0.49766412 0.1994255  0.72898054 0.37216982 0.95862347 0.91480696\n",
      " 0.97503257 0.83085346 0.82705331 0.95758826]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 157 [0/43 (0%)]\tTrain Loss: 0.022252\n",
      "Train Epoch: 157 [10/43 (23%)]\tTrain Loss: 0.042900\n",
      "Train Epoch: 157 [20/43 (47%)]\tTrain Loss: 0.048375\n",
      "Train Epoch: 157 [30/43 (70%)]\tTrain Loss: 0.037954\n",
      "Train Epoch: 157 [40/43 (93%)]\tTrain Loss: 0.060329\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03821588 0.82718426 0.57920963 0.85255641 0.75002122 0.56516415\n",
      " 0.76169348 0.74650192 0.3082628  0.32795358 0.58724791 0.29986838\n",
      " 0.71422487 0.9494949  0.69079876 0.10544617 0.30328456 0.46753839\n",
      " 0.42101094 0.24960381 0.40995273 0.80580282 0.99015886 0.95844591\n",
      " 0.67412108 0.98507428 0.98304582 0.91275537 0.45738652 0.70682061\n",
      " 0.92401415 0.63825369 0.19268574 0.20432438 0.30523914 0.04390071\n",
      " 0.162672   0.57027715 0.87031573 0.49520427 0.61693972 0.48462212\n",
      " 0.30749223 0.27669069 0.47791004 0.3312332  0.23946327 0.13982859\n",
      " 0.80086762 0.39674383 0.95640194 0.44482958 0.08021364 0.06925874\n",
      " 0.28384447 0.04988054 0.59074014 0.01972564 0.20965564 0.20235415\n",
      " 0.96294272 0.95303333 0.95242357 0.95904684 0.82780743 0.83041298\n",
      " 0.82739025 0.99727613 0.98806095 0.87420946 0.93128753 0.94281965\n",
      " 0.71041507 0.95248753 0.87940043 0.94981271 0.9969126  0.99666792\n",
      " 0.99669927 0.9859907  0.96786624 0.99204141 0.99293762 0.52723145\n",
      " 0.62790823 0.90556353 0.95254767 0.97288358 0.86925572 0.9963702\n",
      " 0.99085897 0.78037035 0.62188113 0.99816185 0.95027095 0.98146629\n",
      " 0.50569797 0.9438712  0.74656284 0.97699761 0.35508683 0.93348986\n",
      " 0.5201453  0.0752238  0.16042183 0.43119311 0.29282147 0.42656201\n",
      " 0.93082833 0.57009369 0.77935463 0.76253504 0.98390156 0.9727518\n",
      " 0.99077362 0.94610137 0.90468878 0.98668343]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 158 [0/43 (0%)]\tTrain Loss: 0.060016\n",
      "Train Epoch: 158 [10/43 (23%)]\tTrain Loss: 0.069773\n",
      "Train Epoch: 158 [20/43 (47%)]\tTrain Loss: 0.029287\n",
      "Train Epoch: 158 [30/43 (70%)]\tTrain Loss: 0.052587\n",
      "Train Epoch: 158 [40/43 (93%)]\tTrain Loss: 0.018883\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02086318 0.75710458 0.64746046 0.88237131 0.55158222 0.23633635\n",
      " 0.74838293 0.75947398 0.29249138 0.097897   0.50191396 0.10492122\n",
      " 0.66450495 0.92339474 0.4744696  0.07295972 0.35881034 0.3198272\n",
      " 0.16858372 0.39148593 0.24589019 0.71460462 0.94525307 0.93474436\n",
      " 0.51846355 0.89777362 0.96870148 0.7261346  0.16811541 0.18374175\n",
      " 0.7912218  0.42373866 0.04628712 0.1486859  0.02680576 0.03488841\n",
      " 0.06143705 0.48958388 0.51893765 0.24018207 0.17610332 0.12785891\n",
      " 0.08639151 0.16488364 0.4115667  0.28968287 0.07010138 0.07435495\n",
      " 0.54283249 0.16289057 0.92854869 0.25021827 0.05093163 0.01817477\n",
      " 0.19786975 0.06919415 0.3586106  0.01502317 0.10361823 0.07749325\n",
      " 0.87990522 0.764768   0.97031188 0.98131174 0.74319893 0.89099556\n",
      " 0.78636128 0.99227095 0.99220747 0.64971548 0.70794028 0.67985511\n",
      " 0.5076474  0.79375511 0.69775444 0.90157449 0.99231005 0.99625993\n",
      " 0.98425066 0.97255391 0.93609726 0.94633472 0.97975367 0.59972268\n",
      " 0.25201187 0.79996651 0.86601776 0.93872428 0.58119202 0.98771149\n",
      " 0.87322962 0.65650058 0.64158809 0.9758926  0.90397418 0.92240995\n",
      " 0.16396382 0.75788915 0.32131898 0.96372062 0.09029138 0.817469\n",
      " 0.13053197 0.06185571 0.18617442 0.32308117 0.11725812 0.34198153\n",
      " 0.51032066 0.19210163 0.81315792 0.52532613 0.94253856 0.93007636\n",
      " 0.91916394 0.57561451 0.84817737 0.92725569]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 159 [0/43 (0%)]\tTrain Loss: 0.017540\n",
      "Train Epoch: 159 [10/43 (23%)]\tTrain Loss: 0.028761\n",
      "Train Epoch: 159 [20/43 (47%)]\tTrain Loss: 0.019916\n",
      "Train Epoch: 159 [30/43 (70%)]\tTrain Loss: 0.021738\n",
      "Train Epoch: 159 [40/43 (93%)]\tTrain Loss: 0.019054\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02092414 0.8994205  0.65753233 0.90180033 0.80537206 0.14524175\n",
      " 0.77343327 0.75314194 0.33888546 0.20652866 0.3887184  0.11010303\n",
      " 0.72136956 0.97859943 0.73183978 0.14012401 0.48216981 0.49999091\n",
      " 0.48159689 0.35316733 0.22669612 0.90278172 0.93905288 0.93309253\n",
      " 0.51531494 0.97085428 0.90894037 0.90094471 0.63164169 0.55309922\n",
      " 0.79610252 0.22208068 0.11784628 0.19574763 0.01484804 0.06815073\n",
      " 0.1036533  0.68380225 0.65512162 0.2499641  0.21816927 0.3452687\n",
      " 0.17095415 0.311634   0.73173547 0.26413751 0.06910026 0.04527373\n",
      " 0.72659671 0.17052062 0.82992655 0.05340492 0.06655091 0.04950533\n",
      " 0.18106182 0.11351357 0.3339695  0.0179098  0.21690992 0.22360906\n",
      " 0.93209326 0.87488359 0.93065232 0.95496929 0.82854122 0.91063112\n",
      " 0.73274547 0.99711895 0.99467319 0.73697633 0.4824481  0.70392174\n",
      " 0.65333599 0.79196656 0.61162782 0.893637   0.99751556 0.99691027\n",
      " 0.99389714 0.95450205 0.95073944 0.97982347 0.9953171  0.13914803\n",
      " 0.26613972 0.77777344 0.89384848 0.95076132 0.75992829 0.98490614\n",
      " 0.96849388 0.7216894  0.62843126 0.99232131 0.86267829 0.96730238\n",
      " 0.23241073 0.76288486 0.54923385 0.93630528 0.24263723 0.66502273\n",
      " 0.32711974 0.06731578 0.23364756 0.34298933 0.25372761 0.50903559\n",
      " 0.73618847 0.53943872 0.93433964 0.56770599 0.98339188 0.95201719\n",
      " 0.97968882 0.68708211 0.86921287 0.96040362]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 160 [0/43 (0%)]\tTrain Loss: 0.014683\n",
      "Train Epoch: 160 [10/43 (23%)]\tTrain Loss: 0.047778\n",
      "Train Epoch: 160 [20/43 (47%)]\tTrain Loss: 0.040044\n",
      "Train Epoch: 160 [30/43 (70%)]\tTrain Loss: 0.018550\n",
      "Train Epoch: 160 [40/43 (93%)]\tTrain Loss: 0.016436\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02177129 0.91806632 0.88049954 0.59549451 0.25800487 0.319417\n",
      " 0.83978909 0.65679717 0.46799842 0.26622003 0.33960289 0.24005799\n",
      " 0.52328503 0.95296872 0.64511979 0.0846715  0.21279609 0.5690946\n",
      " 0.29417142 0.50165862 0.39803514 0.78401834 0.94872528 0.87878835\n",
      " 0.63278276 0.94498348 0.32958764 0.79890448 0.3914997  0.44546992\n",
      " 0.77249414 0.51176924 0.19121574 0.28818363 0.02794494 0.0772419\n",
      " 0.12548824 0.42535958 0.61957794 0.1101656  0.1744702  0.25663456\n",
      " 0.31447583 0.09177928 0.49870387 0.21579663 0.1865087  0.14197411\n",
      " 0.76930404 0.3593193  0.78139877 0.36400554 0.041164   0.02924706\n",
      " 0.46156749 0.10355099 0.43964896 0.01729008 0.28744897 0.17830858\n",
      " 0.96024638 0.93396109 0.96832252 0.98090291 0.61688519 0.91747844\n",
      " 0.84732926 0.9947567  0.99678576 0.77152771 0.78869492 0.74110001\n",
      " 0.48220095 0.8739742  0.54549587 0.88506913 0.99876946 0.99834943\n",
      " 0.99716562 0.97123396 0.91762769 0.9628405  0.99163657 0.1728982\n",
      " 0.34782574 0.83416694 0.96863323 0.98287994 0.43645754 0.99461257\n",
      " 0.94151694 0.75874352 0.69582576 0.99501812 0.82611859 0.9225471\n",
      " 0.27475137 0.83988667 0.52081108 0.9741478  0.18823922 0.84988439\n",
      " 0.38155675 0.05724474 0.1006539  0.41008216 0.34788758 0.15571462\n",
      " 0.35084215 0.29563865 0.58199    0.35201132 0.93361551 0.89483821\n",
      " 0.9873302  0.85119516 0.85333681 0.94693667]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 47 TN= 41 FN= 11 FP= 19\n",
      "TP+FP 66\n",
      "precision 0.7121212121212122\n",
      "recall 0.8103448275862069\n",
      "F1 0.7580645161290323\n",
      "acc 0.7457627118644068\n",
      "AUCp 0.7468390804597701\n",
      "AUC 0.8224137931034483\n",
      "\n",
      " The epoch is 160, average recall: 0.8103, average precision: 0.7121,average F1: 0.7581, average accuracy: 0.7458, average AUC: 0.8224\n",
      "Train Epoch: 161 [0/43 (0%)]\tTrain Loss: 0.030543\n",
      "Train Epoch: 161 [10/43 (23%)]\tTrain Loss: 0.019814\n",
      "Train Epoch: 161 [20/43 (47%)]\tTrain Loss: 0.016231\n",
      "Train Epoch: 161 [30/43 (70%)]\tTrain Loss: 0.024335\n",
      "Train Epoch: 161 [40/43 (93%)]\tTrain Loss: 0.032647\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.13280451 0.93021584 0.58626813 0.62387115 0.08086926 0.09026173\n",
      " 0.81397617 0.6576308  0.12942778 0.13368014 0.10555667 0.11069916\n",
      " 0.36855742 0.8763026  0.55245852 0.0521931  0.16788429 0.20975909\n",
      " 0.1560436  0.1007288  0.10278687 0.62111974 0.93756825 0.04727282\n",
      " 0.73016578 0.86137509 0.06708433 0.82632709 0.16634789 0.1841529\n",
      " 0.83171958 0.44911671 0.0331905  0.1139264  0.08734119 0.00709486\n",
      " 0.01464248 0.06642548 0.24763493 0.03761435 0.04606283 0.05307073\n",
      " 0.08928256 0.02653584 0.50737613 0.15289472 0.07534862 0.0718082\n",
      " 0.11869515 0.13798721 0.06314337 0.04688962 0.08686082 0.01927867\n",
      " 0.08068261 0.0396965  0.08212043 0.00527171 0.10263353 0.0752633\n",
      " 0.9434607  0.9119634  0.98193592 0.99425513 0.77508318 0.90691769\n",
      " 0.66216749 0.99380577 0.990049   0.09966576 0.64578176 0.71946186\n",
      " 0.44264278 0.81577241 0.44489133 0.88932389 0.98978347 0.99404162\n",
      " 0.9883464  0.88466114 0.92666847 0.97819591 0.99466407 0.64941162\n",
      " 0.37181354 0.81578153 0.83270586 0.96289307 0.80150628 0.94393289\n",
      " 0.93472886 0.68018055 0.45999393 0.96204871 0.84667015 0.09285326\n",
      " 0.06893008 0.87405217 0.62026465 0.94725651 0.26010272 0.74328232\n",
      " 0.14076866 0.02598431 0.06359082 0.07770336 0.10919602 0.05292781\n",
      " 0.41741273 0.10890419 0.04556559 0.32437381 0.96016949 0.8921532\n",
      " 0.94995898 0.80748808 0.73889613 0.963624  ]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 162 [0/43 (0%)]\tTrain Loss: 0.043252\n",
      "Train Epoch: 162 [10/43 (23%)]\tTrain Loss: 0.029335\n",
      "Train Epoch: 162 [20/43 (47%)]\tTrain Loss: 0.031080\n",
      "Train Epoch: 162 [30/43 (70%)]\tTrain Loss: 0.017242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 162 [40/43 (93%)]\tTrain Loss: 0.020568\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01031722 0.80154985 0.80544436 0.62956578 0.37129474 0.04145182\n",
      " 0.61181456 0.68953103 0.19717583 0.08035912 0.14160395 0.10366776\n",
      " 0.34134564 0.89201909 0.14135218 0.06889826 0.25900632 0.208212\n",
      " 0.30029094 0.24926651 0.18358521 0.57727849 0.92616022 0.98159546\n",
      " 0.52182198 0.96901435 0.82277137 0.74111491 0.23660903 0.09293732\n",
      " 0.79508364 0.47725475 0.02230855 0.00658077 0.0048038  0.0207728\n",
      " 0.03471011 0.40137687 0.61684704 0.05483265 0.13323425 0.12435061\n",
      " 0.16426507 0.04259455 0.28255463 0.07190502 0.01681416 0.034745\n",
      " 0.7453143  0.12707682 0.83280194 0.02649073 0.00841789 0.00965212\n",
      " 0.07848447 0.00727244 0.24355508 0.00817565 0.09516928 0.07712061\n",
      " 0.9337061  0.77653617 0.96759462 0.966663   0.59189117 0.84957469\n",
      " 0.80359411 0.99725157 0.94364685 0.36606771 0.73931003 0.79368865\n",
      " 0.35995376 0.86751902 0.39367113 0.90178335 0.99647164 0.99552828\n",
      " 0.98635662 0.95081615 0.79723114 0.97147453 0.98434597 0.54508817\n",
      " 0.45654324 0.78519559 0.96075636 0.96543646 0.86621571 0.98411024\n",
      " 0.9599517  0.25506514 0.34921223 0.98632753 0.80826122 0.93520463\n",
      " 0.22929977 0.91822398 0.42069399 0.94384921 0.16019107 0.78460276\n",
      " 0.10716838 0.05840258 0.03993329 0.11922987 0.0744412  0.21258304\n",
      " 0.35654142 0.50248134 0.66116428 0.62488467 0.97860456 0.90314502\n",
      " 0.96393293 0.58566105 0.71314293 0.9590646 ]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 163 [0/43 (0%)]\tTrain Loss: 0.025695\n",
      "Train Epoch: 163 [10/43 (23%)]\tTrain Loss: 0.048547\n",
      "Train Epoch: 163 [20/43 (47%)]\tTrain Loss: 0.020865\n",
      "Train Epoch: 163 [30/43 (70%)]\tTrain Loss: 0.038910\n",
      "Train Epoch: 163 [40/43 (93%)]\tTrain Loss: 0.023238\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03515138 0.95282918 0.82877773 0.78576148 0.36736926 0.1533954\n",
      " 0.83146757 0.71033096 0.23095021 0.25011879 0.27583644 0.23295927\n",
      " 0.35769919 0.94767052 0.62440604 0.15963298 0.57692027 0.48777631\n",
      " 0.43013668 0.49658573 0.31725425 0.85622793 0.9886834  0.92235291\n",
      " 0.73751456 0.9850921  0.94911516 0.85526925 0.63345695 0.56255698\n",
      " 0.91662818 0.23312996 0.0526286  0.06061025 0.01523644 0.03894684\n",
      " 0.04660825 0.56650692 0.72593325 0.34481919 0.38937578 0.42089826\n",
      " 0.35867298 0.09957268 0.57781792 0.20291743 0.10761482 0.08420222\n",
      " 0.78404331 0.2142159  0.91891301 0.35588133 0.04114286 0.02794768\n",
      " 0.2165743  0.02915732 0.6241681  0.02517427 0.33661106 0.08846918\n",
      " 0.95776522 0.97921294 0.96298635 0.96639311 0.72987384 0.91196889\n",
      " 0.85697407 0.9951089  0.9951632  0.54474163 0.8969025  0.93001324\n",
      " 0.64282775 0.85863304 0.64778513 0.95121253 0.99788767 0.99890339\n",
      " 0.99283642 0.99016106 0.95096147 0.98994058 0.99682969 0.78572971\n",
      " 0.63877988 0.88557702 0.9649635  0.99563295 0.71920282 0.9878456\n",
      " 0.97226399 0.56615466 0.52873021 0.99852091 0.83511192 0.97934055\n",
      " 0.41968074 0.98446989 0.82686698 0.99025553 0.31140378 0.854994\n",
      " 0.31883177 0.22897473 0.15910034 0.76321864 0.43574995 0.39755929\n",
      " 0.35579443 0.57779258 0.80516762 0.84364152 0.98580629 0.96533006\n",
      " 0.96235102 0.72892761 0.85430616 0.97137624]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 164 [0/43 (0%)]\tTrain Loss: 0.016500\n",
      "Train Epoch: 164 [10/43 (23%)]\tTrain Loss: 0.017745\n",
      "Train Epoch: 164 [20/43 (47%)]\tTrain Loss: 0.047829\n",
      "Train Epoch: 164 [30/43 (70%)]\tTrain Loss: 0.043829\n",
      "Train Epoch: 164 [40/43 (93%)]\tTrain Loss: 0.024474\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03867453 0.80160445 0.50042665 0.83715612 0.40196338 0.03726547\n",
      " 0.66489875 0.69227135 0.30666652 0.21721236 0.13356034 0.22393516\n",
      " 0.31275368 0.95332026 0.64100599 0.07966751 0.1889503  0.20258798\n",
      " 0.38391295 0.38014925 0.42240939 0.84311211 0.97189867 0.95673615\n",
      " 0.63781679 0.97761267 0.94745421 0.72047853 0.35884294 0.15968978\n",
      " 0.81408376 0.36086062 0.04817599 0.49756229 0.02114826 0.04466754\n",
      " 0.05120404 0.3412534  0.80000901 0.23964752 0.11491329 0.1677541\n",
      " 0.38919881 0.07681966 0.50390726 0.10540099 0.03892899 0.08005736\n",
      " 0.72570509 0.17115189 0.85341895 0.41520888 0.0696578  0.02412257\n",
      " 0.33222538 0.03234088 0.33289137 0.01414123 0.15508351 0.12079109\n",
      " 0.96526182 0.95844972 0.97416568 0.97503835 0.7757166  0.88170618\n",
      " 0.72293437 0.9984712  0.99766207 0.79314369 0.70589495 0.51321012\n",
      " 0.3549464  0.78806823 0.44205347 0.93247378 0.99644178 0.99617249\n",
      " 0.9886778  0.95828384 0.94533503 0.96798795 0.98456717 0.37228012\n",
      " 0.44450355 0.64387411 0.9714371  0.97242874 0.92359531 0.98663682\n",
      " 0.95973206 0.4000175  0.33508891 0.99912959 0.95801717 0.97175807\n",
      " 0.20112069 0.9279142  0.81226963 0.96366483 0.16707399 0.70165074\n",
      " 0.15959913 0.09544131 0.13345411 0.18693087 0.38274494 0.30469069\n",
      " 0.32464871 0.37096554 0.62254763 0.66327894 0.96989477 0.94112927\n",
      " 0.95732147 0.88618708 0.90380925 0.97601378]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 165 [0/43 (0%)]\tTrain Loss: 0.031568\n",
      "Train Epoch: 165 [10/43 (23%)]\tTrain Loss: 0.017165\n",
      "Train Epoch: 165 [20/43 (47%)]\tTrain Loss: 0.019426\n",
      "Train Epoch: 165 [30/43 (70%)]\tTrain Loss: 0.008461\n",
      "Train Epoch: 165 [40/43 (93%)]\tTrain Loss: 0.019858\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02192146 0.89736813 0.32929739 0.81198674 0.50848931 0.06224612\n",
      " 0.88906807 0.71331567 0.2185626  0.2055753  0.15401435 0.21397324\n",
      " 0.57988143 0.92676914 0.43801063 0.09822793 0.50016218 0.23432358\n",
      " 0.38532376 0.45272964 0.3372578  0.86717194 0.9945274  0.97073597\n",
      " 0.49785143 0.95191401 0.81637377 0.79469645 0.29291046 0.5052461\n",
      " 0.783638   0.45045134 0.0833153  0.28065953 0.29133767 0.0642988\n",
      " 0.03528488 0.62138826 0.76256233 0.17137395 0.16685523 0.24542426\n",
      " 0.16686291 0.12684304 0.45801941 0.24801174 0.20738922 0.05435517\n",
      " 0.66704291 0.14999439 0.34605664 0.12197152 0.21840191 0.02713688\n",
      " 0.17901286 0.06688377 0.44651201 0.00897778 0.18037905 0.10755138\n",
      " 0.96711117 0.9364081  0.96438861 0.96198243 0.63902938 0.86793685\n",
      " 0.90011352 0.99876249 0.99763334 0.65979761 0.60221952 0.57271641\n",
      " 0.5825392  0.71589559 0.73368472 0.92719257 0.99888498 0.9942106\n",
      " 0.99264693 0.9509837  0.98181146 0.99076414 0.98849291 0.63892758\n",
      " 0.31075805 0.85324669 0.97837365 0.99569523 0.84638065 0.97313887\n",
      " 0.97385919 0.72932762 0.56964427 0.99613971 0.91859883 0.99081928\n",
      " 0.14827429 0.9227131  0.60858923 0.98220634 0.15635738 0.6619944\n",
      " 0.14247189 0.10034709 0.16540086 0.30356747 0.14406356 0.27618021\n",
      " 0.34160078 0.56147724 0.32855752 0.54647875 0.96702421 0.92367607\n",
      " 0.95550549 0.70521379 0.81022072 0.9446575 ]\n",
      "predict [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 166 [0/43 (0%)]\tTrain Loss: 0.011363\n",
      "Train Epoch: 166 [10/43 (23%)]\tTrain Loss: 0.040697\n",
      "Train Epoch: 166 [20/43 (47%)]\tTrain Loss: 0.045639\n",
      "Train Epoch: 166 [30/43 (70%)]\tTrain Loss: 0.038844\n",
      "Train Epoch: 166 [40/43 (93%)]\tTrain Loss: 0.016608\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02646523 0.94380641 0.84695697 0.67289454 0.45666337 0.0730187\n",
      " 0.92051178 0.74003434 0.27540055 0.26113945 0.38802105 0.34726143\n",
      " 0.4300935  0.97375715 0.61111301 0.19040281 0.35078436 0.3658478\n",
      " 0.37401295 0.4369702  0.32187277 0.90839911 0.99290496 0.97796339\n",
      " 0.76352346 0.99809295 0.98484945 0.9484033  0.47117779 0.36295286\n",
      " 0.81632906 0.43254071 0.08836708 0.4161258  0.0343346  0.03073859\n",
      " 0.11827399 0.53071654 0.69115204 0.0875538  0.17827842 0.14416018\n",
      " 0.45912728 0.22901082 0.6153475  0.16472805 0.61110801 0.16443381\n",
      " 0.75233763 0.54441786 0.86562455 0.49574178 0.11697028 0.01166789\n",
      " 0.45808917 0.04216722 0.53778535 0.01590358 0.48930499 0.08721327\n",
      " 0.9578535  0.96400845 0.99216789 0.99213409 0.78496015 0.90903038\n",
      " 0.79287988 0.99937773 0.99716657 0.43021375 0.88285434 0.81172049\n",
      " 0.36109143 0.71173596 0.42986846 0.90195256 0.99837482 0.99956292\n",
      " 0.99157506 0.98945773 0.97140831 0.98905867 0.98409528 0.83227849\n",
      " 0.52572513 0.87814021 0.97089571 0.99078208 0.89380991 0.98965609\n",
      " 0.98876369 0.37477556 0.51933599 0.99897671 0.96680617 0.98860312\n",
      " 0.56919861 0.98298299 0.68108779 0.9720847  0.26848522 0.88273036\n",
      " 0.45606434 0.14409772 0.20042928 0.23438954 0.48343733 0.45090064\n",
      " 0.64433658 0.58306503 0.82981998 0.82063997 0.9824248  0.9457556\n",
      " 0.97191173 0.86604261 0.88682568 0.97774029]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 167 [0/43 (0%)]\tTrain Loss: 0.044185\n",
      "Train Epoch: 167 [10/43 (23%)]\tTrain Loss: 0.055805\n",
      "Train Epoch: 167 [20/43 (47%)]\tTrain Loss: 0.023552\n",
      "Train Epoch: 167 [30/43 (70%)]\tTrain Loss: 0.038176\n",
      "Train Epoch: 167 [40/43 (93%)]\tTrain Loss: 0.029009\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01054897 0.82116175 0.67582411 0.70156205 0.78579634 0.08652823\n",
      " 0.81293786 0.83906823 0.36991981 0.11164141 0.16827042 0.25401095\n",
      " 0.52574718 0.89638162 0.51784253 0.22233897 0.53865588 0.1772484\n",
      " 0.1382955  0.38923848 0.31026313 0.84544718 0.96684706 0.94058686\n",
      " 0.55390233 0.98205459 0.90036249 0.58043361 0.24159899 0.2969923\n",
      " 0.76705283 0.36767781 0.0307823  0.15016049 0.02315339 0.02929561\n",
      " 0.06083579 0.40860826 0.55704182 0.07911082 0.04342518 0.07917657\n",
      " 0.19468148 0.08885022 0.45876586 0.13251695 0.18774864 0.13186051\n",
      " 0.82737952 0.26976991 0.61297429 0.21423967 0.04472521 0.02087105\n",
      " 0.20298092 0.09745033 0.29335821 0.00471596 0.22934107 0.23535247\n",
      " 0.94602638 0.92272294 0.91180849 0.97113055 0.72751892 0.833395\n",
      " 0.86844307 0.9952454  0.98249751 0.585778   0.78549123 0.53361744\n",
      " 0.3341952  0.80716258 0.57828993 0.88773197 0.99430853 0.99648833\n",
      " 0.98084188 0.98688912 0.88474882 0.97064567 0.9928484  0.4628669\n",
      " 0.26784742 0.83366424 0.95306212 0.95948416 0.69378138 0.94691837\n",
      " 0.97749478 0.42084652 0.39793736 0.99480903 0.89907449 0.9692266\n",
      " 0.22359067 0.82059467 0.73638409 0.94331896 0.05725755 0.59270316\n",
      " 0.11895957 0.04430735 0.13595803 0.34879997 0.21519716 0.41687915\n",
      " 0.32142171 0.47051308 0.78637719 0.64234275 0.96744317 0.9492206\n",
      " 0.97011763 0.84149897 0.92344862 0.96018815]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 168 [0/43 (0%)]\tTrain Loss: 0.023572\n",
      "Train Epoch: 168 [10/43 (23%)]\tTrain Loss: 0.013134\n",
      "Train Epoch: 168 [20/43 (47%)]\tTrain Loss: 0.036897\n",
      "Train Epoch: 168 [30/43 (70%)]\tTrain Loss: 0.081761\n",
      "Train Epoch: 168 [40/43 (93%)]\tTrain Loss: 0.025151\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01408818 0.85581678 0.56185389 0.84673107 0.5367493  0.51497138\n",
      " 0.77921271 0.55708623 0.23979066 0.22333318 0.48924541 0.40326905\n",
      " 0.37432969 0.9673866  0.54361123 0.14278004 0.27915755 0.13822289\n",
      " 0.42986923 0.36820939 0.31658998 0.6256026  0.989115   0.94165295\n",
      " 0.62947452 0.98630548 0.95171767 0.71659273 0.3026914  0.48799583\n",
      " 0.57313281 0.41125903 0.06313711 0.316522   0.03922668 0.04060234\n",
      " 0.03831386 0.45413902 0.50819451 0.07696623 0.14669344 0.15431215\n",
      " 0.2687234  0.0705797  0.53871667 0.09281094 0.37520847 0.10525204\n",
      " 0.75640166 0.38406286 0.60657763 0.55822444 0.05299442 0.03553304\n",
      " 0.4152776  0.11622996 0.26151329 0.02715194 0.65694505 0.08727951\n",
      " 0.96746051 0.9287768  0.95036441 0.98399711 0.61911637 0.71437293\n",
      " 0.55573744 0.99853706 0.99325716 0.2725091  0.59425735 0.6354261\n",
      " 0.3646937  0.85569888 0.52411073 0.92776418 0.9984017  0.99731952\n",
      " 0.99331188 0.93478996 0.95862335 0.97514844 0.99034929 0.46180272\n",
      " 0.26826543 0.89569098 0.94935626 0.98835993 0.89496434 0.98619312\n",
      " 0.97627187 0.64510334 0.34734356 0.99795508 0.8864696  0.98771834\n",
      " 0.36381719 0.94851339 0.62619638 0.97579581 0.29737416 0.73027599\n",
      " 0.12761261 0.08716431 0.27180314 0.32537293 0.41732058 0.22206627\n",
      " 0.48667958 0.49889013 0.47214067 0.71878105 0.98906064 0.91209382\n",
      " 0.96582395 0.73476857 0.91274077 0.97117674]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 169 [0/43 (0%)]\tTrain Loss: 0.029963\n",
      "Train Epoch: 169 [10/43 (23%)]\tTrain Loss: 0.037106\n",
      "Train Epoch: 169 [20/43 (47%)]\tTrain Loss: 0.024724\n",
      "Train Epoch: 169 [30/43 (70%)]\tTrain Loss: 0.026711\n",
      "Train Epoch: 169 [40/43 (93%)]\tTrain Loss: 0.030410\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02175706 0.64901316 0.29629654 0.82714063 0.27887338 0.07271368\n",
      " 0.53220749 0.6858148  0.26283035 0.21673346 0.37653646 0.35418832\n",
      " 0.47936243 0.96440488 0.64708871 0.10070442 0.41113314 0.22942485\n",
      " 0.41165859 0.35647455 0.32359663 0.72651976 0.99490064 0.97697848\n",
      " 0.75308895 0.99082899 0.98600787 0.82758331 0.30657408 0.23119131\n",
      " 0.62063694 0.31231827 0.04212808 0.04553722 0.02222505 0.0336231\n",
      " 0.06208528 0.63881713 0.75930685 0.26357442 0.2012829  0.29746363\n",
      " 0.56927031 0.09840482 0.47160003 0.14298704 0.09050188 0.06210148\n",
      " 0.72244912 0.38841009 0.88057292 0.21113707 0.09338034 0.03453732\n",
      " 0.44291356 0.06120772 0.53178513 0.02658806 0.2616035  0.12299277\n",
      " 0.92377973 0.90069705 0.98078924 0.9746713  0.70443141 0.83378172\n",
      " 0.80052471 0.99866962 0.99671906 0.52104062 0.83271229 0.78257501\n",
      " 0.36573416 0.77570361 0.567514   0.91182303 0.99729282 0.99690574\n",
      " 0.98081172 0.95786107 0.98620284 0.98326081 0.99473888 0.54663587\n",
      " 0.62892586 0.83104253 0.9635908  0.99214292 0.81025898 0.98639852\n",
      " 0.99547821 0.19560887 0.28322905 0.99261683 0.88983738 0.97906768\n",
      " 0.39625069 0.93253535 0.7857762  0.96730924 0.29962203 0.72387362\n",
      " 0.17152418 0.09455614 0.15257838 0.55592585 0.47889113 0.34124097\n",
      " 0.56004608 0.71515328 0.86627549 0.71292943 0.9939813  0.93405801\n",
      " 0.97994447 0.90804374 0.85142422 0.96269011]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 170 [0/43 (0%)]\tTrain Loss: 0.016184\n",
      "Train Epoch: 170 [10/43 (23%)]\tTrain Loss: 0.033874\n",
      "Train Epoch: 170 [20/43 (47%)]\tTrain Loss: 0.033226\n",
      "Train Epoch: 170 [30/43 (70%)]\tTrain Loss: 0.019129\n",
      "Train Epoch: 170 [40/43 (93%)]\tTrain Loss: 0.019709\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.07114079 0.80695248 0.39181674 0.59591019 0.60517037 0.40941048\n",
      " 0.54839361 0.83163375 0.26187241 0.19452648 0.39397752 0.27952555\n",
      " 0.45012897 0.95984066 0.73073661 0.13360643 0.35940483 0.28028494\n",
      " 0.16590758 0.21581905 0.44718447 0.88881809 0.97768468 0.97763568\n",
      " 0.73632544 0.96391213 0.38576692 0.79946381 0.30130664 0.38670957\n",
      " 0.74524689 0.49199942 0.06435042 0.25195363 0.02277919 0.05667478\n",
      " 0.072272   0.33712244 0.50310665 0.08051515 0.12860514 0.23847768\n",
      " 0.24788783 0.07547954 0.30017924 0.10200579 0.06267171 0.05038961\n",
      " 0.61758912 0.33205029 0.9247061  0.03013241 0.05723752 0.01737576\n",
      " 0.34962419 0.06174445 0.37488264 0.03245891 0.43129107 0.13049124\n",
      " 0.94780082 0.97847134 0.97769815 0.97992253 0.62740725 0.87228394\n",
      " 0.78220147 0.99816811 0.98926955 0.7288602  0.82394576 0.63839668\n",
      " 0.53237468 0.84796435 0.55054802 0.93546629 0.99485034 0.99857652\n",
      " 0.99326199 0.95938575 0.93131256 0.97941577 0.99244577 0.69910419\n",
      " 0.33658528 0.85206068 0.97839707 0.96743059 0.94035345 0.98588008\n",
      " 0.97979647 0.2742283  0.26176596 0.99597663 0.87265068 0.97946966\n",
      " 0.47714421 0.9354859  0.82583708 0.96431798 0.3432993  0.86069703\n",
      " 0.4554683  0.09048589 0.28228709 0.29726461 0.2733773  0.16267678\n",
      " 0.35771447 0.20207246 0.82549131 0.69215673 0.97074908 0.91655266\n",
      " 0.94335341 0.88045996 0.87110585 0.97502059]\n",
      "predict [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 44 TN= 42 FN= 14 FP= 18\n",
      "TP+FP 62\n",
      "precision 0.7096774193548387\n",
      "recall 0.7586206896551724\n",
      "F1 0.7333333333333333\n",
      "acc 0.7288135593220338\n",
      "AUCp 0.7293103448275862\n",
      "AUC 0.8218390804597702\n",
      "\n",
      " The epoch is 170, average recall: 0.7586, average precision: 0.7097,average F1: 0.7333, average accuracy: 0.7288, average AUC: 0.8218\n",
      "Train Epoch: 171 [0/43 (0%)]\tTrain Loss: 0.035292\n",
      "Train Epoch: 171 [10/43 (23%)]\tTrain Loss: 0.040410\n",
      "Train Epoch: 171 [20/43 (47%)]\tTrain Loss: 0.060368\n",
      "Train Epoch: 171 [30/43 (70%)]\tTrain Loss: 0.017790\n",
      "Train Epoch: 171 [40/43 (93%)]\tTrain Loss: 0.028444\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03309615 0.82957321 0.23694295 0.76508176 0.72248572 0.11572672\n",
      " 0.63198686 0.81382734 0.16333725 0.11307289 0.15905175 0.1342703\n",
      " 0.30440786 0.96959126 0.48156983 0.17372224 0.48249727 0.25791031\n",
      " 0.15669198 0.09501223 0.18821825 0.7319259  0.92545456 0.56847912\n",
      " 0.52086246 0.91541451 0.25455049 0.73876435 0.20116554 0.25143081\n",
      " 0.74363065 0.30545568 0.05481727 0.17752364 0.21474344 0.03350196\n",
      " 0.01921885 0.15737109 0.74678391 0.13168511 0.173887   0.09312755\n",
      " 0.16107221 0.06807695 0.28923175 0.0526793  0.0860766  0.03741622\n",
      " 0.65741038 0.20227759 0.31771818 0.11000536 0.04987318 0.01248853\n",
      " 0.19275665 0.01590042 0.20208946 0.01147623 0.17578998 0.08703744\n",
      " 0.87485659 0.79573566 0.93943125 0.95038742 0.64875317 0.88662767\n",
      " 0.66531563 0.99315453 0.99014914 0.50674051 0.70748442 0.72171456\n",
      " 0.45027542 0.86420554 0.5852896  0.73819995 0.99286181 0.99498367\n",
      " 0.98946488 0.97175962 0.94054699 0.97692639 0.97358853 0.24539025\n",
      " 0.17546149 0.88725889 0.95066941 0.93777454 0.8861562  0.97285581\n",
      " 0.98100853 0.41263419 0.34020498 0.99677867 0.88066119 0.98155057\n",
      " 0.17081082 0.89637971 0.68457401 0.98392117 0.11420219 0.51714873\n",
      " 0.20549312 0.03806583 0.19850992 0.08010726 0.13298924 0.23502882\n",
      " 0.1819631  0.1279645  0.14773247 0.54558468 0.93146551 0.93989813\n",
      " 0.88684291 0.65099871 0.74631435 0.95378536]\n",
      "predict [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 172 [0/43 (0%)]\tTrain Loss: 0.040680\n",
      "Train Epoch: 172 [10/43 (23%)]\tTrain Loss: 0.013792\n",
      "Train Epoch: 172 [20/43 (47%)]\tTrain Loss: 0.042358\n",
      "Train Epoch: 172 [30/43 (70%)]\tTrain Loss: 0.030634\n",
      "Train Epoch: 172 [40/43 (93%)]\tTrain Loss: 0.033725\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02061909 0.41952881 0.23633306 0.60142457 0.37189078 0.2962113\n",
      " 0.47867486 0.56677508 0.23897694 0.13340652 0.34980711 0.2379078\n",
      " 0.46360385 0.94297463 0.3849425  0.10414888 0.2382046  0.2177307\n",
      " 0.08128088 0.13430826 0.29965279 0.73631459 0.96631926 0.22525804\n",
      " 0.43195799 0.87421864 0.25975978 0.66070682 0.27726656 0.22687446\n",
      " 0.74249518 0.24005447 0.02685597 0.29181087 0.17580101 0.02868311\n",
      " 0.03281142 0.1936616  0.47375226 0.03164223 0.17969228 0.22989038\n",
      " 0.2264787  0.03868461 0.27163371 0.13966696 0.03606291 0.07617845\n",
      " 0.78646332 0.21570745 0.74479878 0.24330841 0.0588184  0.01802107\n",
      " 0.22947079 0.05349208 0.2189998  0.01499487 0.30237007 0.03387012\n",
      " 0.92828447 0.95114726 0.96089208 0.9807443  0.73626345 0.86432719\n",
      " 0.72848862 0.99821389 0.9963026  0.64051849 0.69260669 0.56563479\n",
      " 0.58205742 0.7211588  0.70112222 0.91920358 0.99739432 0.99700183\n",
      " 0.99279767 0.96352738 0.93040448 0.97987068 0.99331361 0.46982417\n",
      " 0.28420457 0.84879965 0.94039357 0.95446134 0.86803877 0.99281281\n",
      " 0.96460199 0.35311651 0.31723887 0.99437642 0.87221318 0.96895427\n",
      " 0.31564254 0.67094427 0.36704832 0.91684908 0.11102082 0.61245781\n",
      " 0.08061985 0.0441501  0.26086694 0.04649851 0.25936687 0.24336891\n",
      " 0.28135255 0.23121084 0.30477658 0.34210557 0.96221095 0.90734142\n",
      " 0.96523529 0.81719524 0.73414063 0.88634205]\n",
      "predict [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 173 [0/43 (0%)]\tTrain Loss: 0.015838\n",
      "Train Epoch: 173 [10/43 (23%)]\tTrain Loss: 0.034948\n",
      "Train Epoch: 173 [20/43 (47%)]\tTrain Loss: 0.063640\n",
      "Train Epoch: 173 [30/43 (70%)]\tTrain Loss: 0.016427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 173 [40/43 (93%)]\tTrain Loss: 0.067740\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01596376 0.38470182 0.52815497 0.65298963 0.57680249 0.06936537\n",
      " 0.4163872  0.53873235 0.20865342 0.21516693 0.29444242 0.1875872\n",
      " 0.34601665 0.96860248 0.31705415 0.17410351 0.16517229 0.36911002\n",
      " 0.13966653 0.24284986 0.12219547 0.70480812 0.96667337 0.98358881\n",
      " 0.44402006 0.9426828  0.9527359  0.78238225 0.25362849 0.2950961\n",
      " 0.63258284 0.17067294 0.08341714 0.29215121 0.01293477 0.03300825\n",
      " 0.04817617 0.43338609 0.48265961 0.04391507 0.28869209 0.21749559\n",
      " 0.43863139 0.05369141 0.11087961 0.06514797 0.09382028 0.0440552\n",
      " 0.68594784 0.24617343 0.78419012 0.06904949 0.05354442 0.01949619\n",
      " 0.32552835 0.05647209 0.32504243 0.009096   0.40055445 0.16125749\n",
      " 0.93718553 0.93247092 0.97537929 0.9665767  0.72214162 0.87623173\n",
      " 0.72351402 0.9972319  0.99061197 0.4307175  0.66641343 0.71671593\n",
      " 0.68671811 0.7531935  0.41764861 0.93947214 0.99851793 0.99800354\n",
      " 0.99166107 0.94688356 0.91948634 0.97468048 0.9835301  0.41499159\n",
      " 0.20441902 0.88620579 0.92860079 0.93037593 0.86935222 0.9868145\n",
      " 0.98501736 0.38389933 0.28533247 0.99439842 0.92579603 0.95257503\n",
      " 0.43678361 0.81648088 0.43091124 0.97121555 0.21585144 0.57702959\n",
      " 0.06343348 0.04428119 0.0953884  0.09950246 0.18253069 0.16957472\n",
      " 0.20436093 0.43166015 0.62444609 0.79369223 0.9819296  0.89212269\n",
      " 0.95809132 0.79135197 0.80520594 0.9796887 ]\n",
      "predict [0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 174 [0/43 (0%)]\tTrain Loss: 0.031082\n",
      "Train Epoch: 174 [10/43 (23%)]\tTrain Loss: 0.011003\n",
      "Train Epoch: 174 [20/43 (47%)]\tTrain Loss: 0.079239\n",
      "Train Epoch: 174 [30/43 (70%)]\tTrain Loss: 0.057544\n",
      "Train Epoch: 174 [40/43 (93%)]\tTrain Loss: 0.010667\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.41204405 0.87518537 0.65363419 0.50901639 0.51850641 0.33072573\n",
      " 0.87061077 0.60898203 0.68456    0.30370161 0.28208995 0.35257286\n",
      " 0.38265508 0.9926945  0.56544703 0.14480138 0.21281427 0.22611293\n",
      " 0.13650344 0.49404934 0.35554007 0.73494184 0.94237089 0.1911287\n",
      " 0.66880685 0.93231159 0.37077883 0.65334004 0.17927645 0.19923224\n",
      " 0.80569649 0.1200035  0.09066248 0.45200014 0.57505137 0.01535834\n",
      " 0.03519313 0.60124993 0.37146521 0.05629253 0.1127585  0.0869796\n",
      " 0.30551296 0.03647986 0.1440026  0.04323641 0.36717695 0.07065871\n",
      " 0.63755435 0.54505044 0.54652566 0.42554763 0.02841365 0.01709718\n",
      " 0.38030237 0.02958066 0.37993479 0.01439319 0.24416097 0.10747244\n",
      " 0.96302849 0.94215447 0.97161579 0.97732842 0.66548043 0.86202168\n",
      " 0.76368296 0.99701512 0.99370992 0.65123236 0.68821472 0.61844659\n",
      " 0.4680872  0.70973706 0.36103252 0.89783657 0.99819297 0.99840766\n",
      " 0.99542266 0.95290244 0.92650014 0.98858941 0.99458128 0.51842171\n",
      " 0.30348045 0.75356126 0.94734466 0.97474039 0.81659263 0.99271762\n",
      " 0.93442291 0.45522064 0.21499866 0.99805766 0.93042785 0.67350906\n",
      " 0.52127272 0.79544055 0.53706396 0.97869313 0.18675561 0.69162536\n",
      " 0.05849789 0.02346523 0.21269783 0.12412108 0.37176836 0.43442172\n",
      " 0.25713119 0.33578056 0.39984652 0.58326072 0.90470839 0.88587379\n",
      " 0.94699878 0.83048987 0.92194581 0.97451711]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 175 [0/43 (0%)]\tTrain Loss: 0.023884\n",
      "Train Epoch: 175 [10/43 (23%)]\tTrain Loss: 0.071807\n",
      "Train Epoch: 175 [20/43 (47%)]\tTrain Loss: 0.020628\n",
      "Train Epoch: 175 [30/43 (70%)]\tTrain Loss: 0.022111\n",
      "Train Epoch: 175 [40/43 (93%)]\tTrain Loss: 0.042371\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02711314 0.9096396  0.72334737 0.60001463 0.39921516 0.07479994\n",
      " 0.87991673 0.69782645 0.07959218 0.17208508 0.38407537 0.43484658\n",
      " 0.48862091 0.9767909  0.54352307 0.17709047 0.29801828 0.37251857\n",
      " 0.29643691 0.29896525 0.21748956 0.81156963 0.9927302  0.95707375\n",
      " 0.60636795 0.98160809 0.96656299 0.80280566 0.30369172 0.53544247\n",
      " 0.81093717 0.55947    0.08581022 0.38536733 0.02168696 0.03361949\n",
      " 0.02729785 0.41290978 0.73207647 0.07132727 0.09811779 0.41830137\n",
      " 0.24134806 0.12716599 0.58932376 0.18761685 0.05826748 0.04540464\n",
      " 0.7547121  0.37852186 0.93316269 0.35543847 0.03945422 0.02455742\n",
      " 0.34430489 0.05248831 0.44603887 0.01881932 0.36205271 0.11832538\n",
      " 0.96421272 0.97571427 0.9918623  0.98042953 0.67735922 0.87964225\n",
      " 0.73389113 0.9992761  0.99387819 0.67201704 0.9001745  0.89200819\n",
      " 0.65132904 0.86423928 0.5524193  0.94547766 0.99547237 0.99812514\n",
      " 0.99266064 0.98288077 0.94997531 0.97342354 0.99241805 0.4791182\n",
      " 0.48071837 0.90251809 0.95591313 0.99472493 0.91060984 0.9882443\n",
      " 0.98438603 0.46268004 0.39210257 0.99702209 0.94311827 0.98877198\n",
      " 0.38908622 0.9752261  0.67822391 0.97975713 0.25116041 0.62968564\n",
      " 0.36737224 0.11671634 0.10062186 0.51799065 0.23034708 0.21257077\n",
      " 0.3994627  0.30319414 0.63116205 0.8225643  0.97831774 0.94528806\n",
      " 0.96898288 0.89357162 0.75422478 0.97061557]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 176 [0/43 (0%)]\tTrain Loss: 0.022237\n",
      "Train Epoch: 176 [10/43 (23%)]\tTrain Loss: 0.032759\n",
      "Train Epoch: 176 [20/43 (47%)]\tTrain Loss: 0.043663\n",
      "Train Epoch: 176 [30/43 (70%)]\tTrain Loss: 0.038949\n",
      "Train Epoch: 176 [40/43 (93%)]\tTrain Loss: 0.052491\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02848789 0.78982812 0.64918226 0.61384183 0.37051934 0.04865815\n",
      " 0.56681758 0.58900452 0.18697608 0.21693657 0.37961751 0.3676731\n",
      " 0.42563188 0.97790408 0.66406399 0.11359915 0.25779378 0.36642548\n",
      " 0.32975635 0.4272337  0.10143059 0.83239812 0.987122   0.9533118\n",
      " 0.61581212 0.98769635 0.91907573 0.77904004 0.4147633  0.21658137\n",
      " 0.897228   0.32876596 0.12259398 0.32485518 0.01621693 0.04490061\n",
      " 0.02645073 0.45301497 0.80219179 0.05438402 0.20014112 0.1459296\n",
      " 0.23774344 0.13082227 0.5695886  0.1058182  0.07065012 0.04409621\n",
      " 0.82488102 0.25800037 0.91056329 0.28963605 0.09592284 0.02413893\n",
      " 0.31342652 0.0755246  0.42100409 0.01736861 0.25204554 0.13056101\n",
      " 0.95986766 0.93505514 0.98659688 0.99344629 0.80262202 0.92460078\n",
      " 0.8117792  0.99855584 0.9921692  0.74327964 0.89693385 0.73404288\n",
      " 0.35646904 0.84779418 0.67737794 0.92162251 0.99853468 0.99733067\n",
      " 0.98900747 0.97226328 0.96313339 0.98754162 0.99500811 0.43294397\n",
      " 0.41493824 0.92302144 0.97663838 0.97992992 0.74610603 0.97863901\n",
      " 0.96925586 0.28492856 0.3054474  0.99613416 0.93829399 0.95639485\n",
      " 0.23895301 0.91462117 0.74162412 0.97038639 0.23056449 0.83318424\n",
      " 0.28635105 0.08902517 0.10336755 0.37177882 0.22954847 0.23846237\n",
      " 0.4533872  0.24884814 0.5671072  0.655972   0.98772603 0.95903707\n",
      " 0.93407071 0.86590749 0.80796361 0.9585712 ]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 177 [0/43 (0%)]\tTrain Loss: 0.053955\n",
      "Train Epoch: 177 [10/43 (23%)]\tTrain Loss: 0.039914\n",
      "Train Epoch: 177 [20/43 (47%)]\tTrain Loss: 0.015046\n",
      "Train Epoch: 177 [30/43 (70%)]\tTrain Loss: 0.041381\n",
      "Train Epoch: 177 [40/43 (93%)]\tTrain Loss: 0.012097\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.04162074 0.89638215 0.64306104 0.6067825  0.68403059 0.01352093\n",
      " 0.57273757 0.65307605 0.2786431  0.13711709 0.34776151 0.25650275\n",
      " 0.31427035 0.94367158 0.61702406 0.07220562 0.35098413 0.41459462\n",
      " 0.19811283 0.44421268 0.14705217 0.88254428 0.98271346 0.97574389\n",
      " 0.63856435 0.98312169 0.94626647 0.58727252 0.26777819 0.49657902\n",
      " 0.79137176 0.26222989 0.05979678 0.43387604 0.0161204  0.06555215\n",
      " 0.04974046 0.41030052 0.69289291 0.16981705 0.23590779 0.24588743\n",
      " 0.29015633 0.06773727 0.45240369 0.22404832 0.15584369 0.07637298\n",
      " 0.86540741 0.26090601 0.85947031 0.25664756 0.06262825 0.02457038\n",
      " 0.29553348 0.03786396 0.42355195 0.04932763 0.37015224 0.08792531\n",
      " 0.97974992 0.99382073 0.97878623 0.98165983 0.71307874 0.87794554\n",
      " 0.82447755 0.99875629 0.99746084 0.71377188 0.80956817 0.62035131\n",
      " 0.52725202 0.85512042 0.5453276  0.96630901 0.99900252 0.99785703\n",
      " 0.99605799 0.97751951 0.97057503 0.98704565 0.9913817  0.60808092\n",
      " 0.50430465 0.9557271  0.97897387 0.98960006 0.88709444 0.99013233\n",
      " 0.98238051 0.49715239 0.3798627  0.99700755 0.91917378 0.97946781\n",
      " 0.34527218 0.89054435 0.58637244 0.99058127 0.20378561 0.67884088\n",
      " 0.13786024 0.05323602 0.09129162 0.31624034 0.25029185 0.17922471\n",
      " 0.55392075 0.25601041 0.71555483 0.67147297 0.97812313 0.98643595\n",
      " 0.98856926 0.91576529 0.92312461 0.94538009]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 178 [0/43 (0%)]\tTrain Loss: 0.010911\n",
      "Train Epoch: 178 [10/43 (23%)]\tTrain Loss: 0.012214\n",
      "Train Epoch: 178 [20/43 (47%)]\tTrain Loss: 0.037223\n",
      "Train Epoch: 178 [30/43 (70%)]\tTrain Loss: 0.024587\n",
      "Train Epoch: 178 [40/43 (93%)]\tTrain Loss: 0.021483\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02774785 0.91324031 0.6127854  0.66535622 0.63801032 0.09934983\n",
      " 0.7812075  0.47392911 0.1344195  0.4575491  0.25571904 0.17619176\n",
      " 0.42993775 0.96155524 0.47381723 0.17954008 0.43097457 0.48497811\n",
      " 0.51912743 0.21384117 0.15351915 0.81549811 0.99465692 0.9803955\n",
      " 0.57452339 0.98941898 0.98802173 0.66486973 0.55251253 0.39284036\n",
      " 0.70659429 0.31227905 0.02784504 0.30891213 0.0304471  0.06180105\n",
      " 0.11024146 0.3414405  0.74004734 0.3982504  0.19318958 0.47746834\n",
      " 0.24872598 0.15738924 0.50553423 0.23128243 0.08926436 0.05308045\n",
      " 0.64028931 0.30046907 0.80399561 0.34760824 0.02785037 0.03211817\n",
      " 0.2303202  0.01876555 0.39220402 0.0262404  0.25920388 0.07043849\n",
      " 0.97143745 0.94537926 0.98133653 0.97833872 0.67355025 0.86035973\n",
      " 0.86248338 0.99618691 0.99608576 0.55294377 0.86561579 0.8373754\n",
      " 0.67224896 0.78561646 0.67059857 0.89148694 0.99654382 0.99537247\n",
      " 0.97959715 0.98611844 0.96127892 0.97156107 0.99033797 0.587394\n",
      " 0.26446474 0.87471658 0.83324492 0.95267475 0.67070884 0.96560305\n",
      " 0.98725885 0.40301317 0.22986293 0.99751723 0.89267796 0.97425663\n",
      " 0.52722305 0.86974031 0.76116663 0.99423802 0.23497203 0.69236636\n",
      " 0.40733534 0.07460824 0.11487262 0.32901332 0.23260824 0.35475805\n",
      " 0.50449306 0.54914927 0.69579822 0.7354632  0.96865851 0.96958679\n",
      " 0.97685426 0.82044226 0.77824956 0.96076101]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 179 [0/43 (0%)]\tTrain Loss: 0.031984\n",
      "Train Epoch: 179 [10/43 (23%)]\tTrain Loss: 0.018623\n",
      "Train Epoch: 179 [20/43 (47%)]\tTrain Loss: 0.034124\n",
      "Train Epoch: 179 [30/43 (70%)]\tTrain Loss: 0.032207\n",
      "Train Epoch: 179 [40/43 (93%)]\tTrain Loss: 0.021957\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01919917 0.91793203 0.62153536 0.64954263 0.62207979 0.22863919\n",
      " 0.69342047 0.54660034 0.18162529 0.20461701 0.31685942 0.26931682\n",
      " 0.52285117 0.97465885 0.43542552 0.11092781 0.31641123 0.32246736\n",
      " 0.44785225 0.50165379 0.16256106 0.84461582 0.98736155 0.9929865\n",
      " 0.75613648 0.95305729 0.93473905 0.71944129 0.25077638 0.32787642\n",
      " 0.78349072 0.30064079 0.04534291 0.27569342 0.1997772  0.06943601\n",
      " 0.04328584 0.35943374 0.71781886 0.22539568 0.09931047 0.25084007\n",
      " 0.21159565 0.12633306 0.31380594 0.14443074 0.34619164 0.08879691\n",
      " 0.74985719 0.39949808 0.24146143 0.16429992 0.0325053  0.03099672\n",
      " 0.2083672  0.03949411 0.1526956  0.02088126 0.28499353 0.135849\n",
      " 0.97096854 0.94635445 0.9885667  0.98570991 0.82964331 0.85821193\n",
      " 0.86103785 0.99744666 0.9965263  0.65331405 0.67447203 0.76769739\n",
      " 0.58101916 0.89288688 0.60341817 0.96724153 0.99723238 0.99723744\n",
      " 0.99243248 0.98163754 0.95188779 0.98628312 0.99413836 0.44628805\n",
      " 0.47204474 0.89158767 0.96679091 0.99157822 0.91737711 0.98065019\n",
      " 0.96699488 0.29776421 0.26044315 0.99741673 0.88788682 0.96438462\n",
      " 0.32307345 0.87425011 0.61794263 0.98733354 0.43270358 0.71097898\n",
      " 0.34735608 0.08836067 0.22141451 0.23289619 0.23412916 0.46339527\n",
      " 0.44364291 0.22415735 0.12853338 0.68675703 0.98795599 0.91569871\n",
      " 0.96232784 0.93611002 0.88193226 0.96689034]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 180 [0/43 (0%)]\tTrain Loss: 0.016391\n",
      "Train Epoch: 180 [10/43 (23%)]\tTrain Loss: 0.035613\n",
      "Train Epoch: 180 [20/43 (47%)]\tTrain Loss: 0.037993\n",
      "Train Epoch: 180 [30/43 (70%)]\tTrain Loss: 0.023368\n",
      "Train Epoch: 180 [40/43 (93%)]\tTrain Loss: 0.019973\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02522041 0.81799936 0.60484576 0.85503733 0.61144745 0.07879938\n",
      " 0.87582082 0.70547736 0.38220844 0.17756082 0.37652236 0.25409165\n",
      " 0.46243891 0.97084069 0.7532618  0.16153967 0.70991427 0.5939458\n",
      " 0.34088179 0.37042069 0.30670771 0.83365071 0.9897669  0.98212528\n",
      " 0.53475177 0.98404551 0.97262853 0.87837493 0.49373549 0.46239364\n",
      " 0.70010012 0.54714227 0.08589806 0.37771574 0.0130985  0.05903054\n",
      " 0.08455753 0.58248866 0.63509983 0.15567581 0.22062862 0.2629303\n",
      " 0.12397089 0.06162285 0.54506356 0.10317212 0.04789936 0.15602839\n",
      " 0.80989826 0.17390367 0.88980991 0.21807997 0.06772124 0.01575874\n",
      " 0.13668332 0.03558481 0.49216864 0.00848598 0.19421668 0.12492373\n",
      " 0.94804901 0.96804887 0.98353821 0.96826744 0.74243546 0.91596007\n",
      " 0.74168313 0.99706727 0.99565816 0.50535101 0.65256226 0.84027517\n",
      " 0.67486203 0.80943483 0.59995258 0.95615935 0.99563158 0.9974699\n",
      " 0.98907274 0.96784884 0.97734797 0.96714717 0.99225509 0.62784094\n",
      " 0.23882326 0.93832242 0.91627747 0.99305809 0.85819387 0.96121162\n",
      " 0.99442947 0.38200727 0.25887877 0.9847436  0.90464747 0.94563055\n",
      " 0.38986248 0.93253183 0.44843322 0.97453523 0.18555929 0.84125352\n",
      " 0.53918755 0.14308846 0.12733476 0.49333671 0.18586746 0.3919439\n",
      " 0.62792838 0.49987832 0.75094038 0.63394684 0.99238741 0.94376725\n",
      " 0.97358453 0.91092491 0.74504781 0.9888587 ]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 44 TN= 42 FN= 14 FP= 18\n",
      "TP+FP 62\n",
      "precision 0.7096774193548387\n",
      "recall 0.7586206896551724\n",
      "F1 0.7333333333333333\n",
      "acc 0.7288135593220338\n",
      "AUCp 0.7293103448275862\n",
      "AUC 0.8166666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The epoch is 180, average recall: 0.7586, average precision: 0.7097,average F1: 0.7333, average accuracy: 0.7288, average AUC: 0.8167\n",
      "Train Epoch: 181 [0/43 (0%)]\tTrain Loss: 0.018763\n",
      "Train Epoch: 181 [10/43 (23%)]\tTrain Loss: 0.046061\n",
      "Train Epoch: 181 [20/43 (47%)]\tTrain Loss: 0.031179\n",
      "Train Epoch: 181 [30/43 (70%)]\tTrain Loss: 0.036948\n",
      "Train Epoch: 181 [40/43 (93%)]\tTrain Loss: 0.028840\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02460057 0.80346167 0.7021575  0.46161738 0.4378202  0.03779376\n",
      " 0.60314941 0.57843369 0.10495534 0.2476337  0.25091046 0.22204374\n",
      " 0.38197958 0.9370687  0.33734465 0.13080932 0.44878504 0.23284216\n",
      " 0.70794976 0.36202988 0.28681985 0.93366796 0.98700863 0.95684749\n",
      " 0.60662329 0.97679764 0.94956023 0.85727787 0.5754227  0.22098085\n",
      " 0.79726148 0.26651797 0.0531072  0.25475076 0.03623258 0.03942497\n",
      " 0.08408256 0.58414394 0.71101093 0.25348634 0.16357553 0.19180483\n",
      " 0.30696115 0.08077839 0.51649451 0.16327442 0.13299884 0.1006406\n",
      " 0.56536341 0.23403953 0.79546404 0.18346305 0.03901449 0.03081996\n",
      " 0.19580185 0.0552842  0.43964297 0.01495091 0.3386648  0.1004248\n",
      " 0.96604431 0.96957099 0.98595595 0.9877823  0.63630885 0.88423991\n",
      " 0.83464956 0.99792033 0.98971701 0.58669496 0.80991048 0.52889341\n",
      " 0.5680868  0.63547885 0.60679787 0.91196233 0.99772626 0.9964149\n",
      " 0.99223858 0.94425964 0.91132718 0.97478479 0.9841218  0.63247544\n",
      " 0.53359437 0.86958289 0.89937592 0.97010034 0.90617728 0.97820926\n",
      " 0.94370437 0.28221369 0.18648089 0.99791807 0.86576515 0.98120958\n",
      " 0.29481483 0.89299607 0.7533887  0.99312311 0.0906489  0.82238126\n",
      " 0.20994808 0.04008115 0.1269564  0.31060496 0.23583582 0.43650419\n",
      " 0.40678608 0.14607766 0.45823422 0.56003284 0.97735035 0.93546665\n",
      " 0.97499597 0.93828714 0.83401752 0.9704231 ]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 182 [0/43 (0%)]\tTrain Loss: 0.053429\n",
      "Train Epoch: 182 [10/43 (23%)]\tTrain Loss: 0.038516\n",
      "Train Epoch: 182 [20/43 (47%)]\tTrain Loss: 0.019036\n",
      "Train Epoch: 182 [30/43 (70%)]\tTrain Loss: 0.069836\n",
      "Train Epoch: 182 [40/43 (93%)]\tTrain Loss: 0.033496\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02543674 0.82504362 0.56015468 0.45291138 0.33188576 0.05010381\n",
      " 0.55550224 0.72212398 0.22926784 0.24314485 0.39026108 0.30358106\n",
      " 0.57158637 0.94978088 0.53149259 0.24523605 0.37980378 0.38725054\n",
      " 0.37247059 0.50140649 0.27225173 0.85274136 0.98460859 0.98006207\n",
      " 0.72044152 0.9745748  0.95259923 0.69703573 0.28782049 0.49915358\n",
      " 0.84638429 0.34764516 0.0484979  0.36205012 0.02232076 0.0407309\n",
      " 0.07580875 0.5825879  0.72716528 0.14787315 0.1976995  0.2057254\n",
      " 0.21986419 0.06610324 0.53421384 0.23351519 0.10583837 0.09872083\n",
      " 0.81893235 0.42222065 0.88896149 0.08987255 0.07160636 0.02428336\n",
      " 0.25791457 0.05330184 0.34401616 0.02305529 0.25897187 0.11566307\n",
      " 0.94644177 0.99157786 0.98693138 0.98559988 0.60453445 0.8690502\n",
      " 0.72392297 0.99886703 0.99629021 0.6816563  0.71914679 0.59536439\n",
      " 0.52801222 0.69470239 0.59354842 0.96371299 0.99354446 0.99771082\n",
      " 0.99280107 0.96556002 0.94578129 0.98456502 0.98676443 0.73384845\n",
      " 0.3521136  0.90710562 0.95440769 0.9928335  0.81948537 0.99658138\n",
      " 0.98438871 0.48973861 0.3075912  0.993186   0.81760234 0.9648748\n",
      " 0.40319479 0.95016998 0.62265003 0.97587186 0.27917972 0.8013767\n",
      " 0.48754144 0.09249256 0.2743682  0.25122491 0.2677342  0.37348115\n",
      " 0.52114558 0.30057925 0.60790616 0.67797565 0.96615064 0.94373327\n",
      " 0.92733395 0.93882805 0.9013018  0.97635853]\n",
      "predict [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 183 [0/43 (0%)]\tTrain Loss: 0.020522\n",
      "Train Epoch: 183 [10/43 (23%)]\tTrain Loss: 0.032781\n",
      "Train Epoch: 183 [20/43 (47%)]\tTrain Loss: 0.084928\n",
      "Train Epoch: 183 [30/43 (70%)]\tTrain Loss: 0.019772\n",
      "Train Epoch: 183 [40/43 (93%)]\tTrain Loss: 0.029063\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02364251 0.93413824 0.69489688 0.75526083 0.37930739 0.10141092\n",
      " 0.82429922 0.58948088 0.23329884 0.2626144  0.29081124 0.32707199\n",
      " 0.61652637 0.9510932  0.76393127 0.15244992 0.64764774 0.37249538\n",
      " 0.55820572 0.49108213 0.45344597 0.85676813 0.9845286  0.98235035\n",
      " 0.70722312 0.9921599  0.98383069 0.89676124 0.67431843 0.66247451\n",
      " 0.77417535 0.46681374 0.11531983 0.52595061 0.03678774 0.03766481\n",
      " 0.06114484 0.54937923 0.91233903 0.34536296 0.16069397 0.15871154\n",
      " 0.41190597 0.1165918  0.61310607 0.20468226 0.07547902 0.15748066\n",
      " 0.85303777 0.24815474 0.90131181 0.41916057 0.05391005 0.0154509\n",
      " 0.26020056 0.07999309 0.5512132  0.02865106 0.29854056 0.18545701\n",
      " 0.97602135 0.9705866  0.98863471 0.99272692 0.72193903 0.92073143\n",
      " 0.80098808 0.99916041 0.99313474 0.76359409 0.85930425 0.74098033\n",
      " 0.64660078 0.82437909 0.58841676 0.92101163 0.99839717 0.99741447\n",
      " 0.99522531 0.989057   0.97238445 0.98594022 0.9935993  0.68210655\n",
      " 0.70563996 0.89048922 0.98592883 0.98847896 0.82066768 0.99216533\n",
      " 0.99747699 0.45729998 0.3098298  0.99792564 0.95349729 0.9859212\n",
      " 0.50103271 0.93897825 0.78182602 0.99015129 0.55239564 0.79263473\n",
      " 0.4852393  0.11343598 0.18023013 0.41762447 0.18660475 0.59581965\n",
      " 0.65530241 0.54054856 0.93091208 0.85299838 0.99061137 0.9742431\n",
      " 0.96948671 0.98427767 0.92158967 0.98187155]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 184 [0/43 (0%)]\tTrain Loss: 0.020368\n",
      "Train Epoch: 184 [10/43 (23%)]\tTrain Loss: 0.024641\n",
      "Train Epoch: 184 [20/43 (47%)]\tTrain Loss: 0.060587\n",
      "Train Epoch: 184 [30/43 (70%)]\tTrain Loss: 0.017759\n",
      "Train Epoch: 184 [40/43 (93%)]\tTrain Loss: 0.032521\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03498997 0.74347407 0.5865252  0.73451436 0.42096943 0.07510698\n",
      " 0.87077558 0.73151225 0.23564565 0.2065742  0.4762229  0.20472021\n",
      " 0.65998954 0.96543157 0.63156205 0.13700768 0.35958847 0.31016564\n",
      " 0.47039893 0.39183119 0.28559461 0.67984271 0.99409831 0.957219\n",
      " 0.65021354 0.97858882 0.97688955 0.85834831 0.7028904  0.49548101\n",
      " 0.86169058 0.53973597 0.07781139 0.20701262 0.02852694 0.06095106\n",
      " 0.07523631 0.57058537 0.8527236  0.42463043 0.40120319 0.35389155\n",
      " 0.28847098 0.06171785 0.56246656 0.25988382 0.13318373 0.05956607\n",
      " 0.77289498 0.26656222 0.77114457 0.28572795 0.07020468 0.01891834\n",
      " 0.18426192 0.07576657 0.54917639 0.03423782 0.25107208 0.07754648\n",
      " 0.97952205 0.97816843 0.9931854  0.99268758 0.70128518 0.93338925\n",
      " 0.82681721 0.99817574 0.99530411 0.71383947 0.93233502 0.83568919\n",
      " 0.58453679 0.69607198 0.50282305 0.94865876 0.99857175 0.99876827\n",
      " 0.99402231 0.97014165 0.93988562 0.9882201  0.9917596  0.74682677\n",
      " 0.59280145 0.90362293 0.98043764 0.96976298 0.84654534 0.99339139\n",
      " 0.97645116 0.67538017 0.50720233 0.99772185 0.9284544  0.98261917\n",
      " 0.27668205 0.98262596 0.89885235 0.99528694 0.51054901 0.81055892\n",
      " 0.48268983 0.07086186 0.12134056 0.29069433 0.2116615  0.46470791\n",
      " 0.46001351 0.42180902 0.75348568 0.74329382 0.97531432 0.95430541\n",
      " 0.97545433 0.97173274 0.87694448 0.97462511]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 185 [0/43 (0%)]\tTrain Loss: 0.033383\n",
      "Train Epoch: 185 [10/43 (23%)]\tTrain Loss: 0.025414\n",
      "Train Epoch: 185 [20/43 (47%)]\tTrain Loss: 0.038649\n",
      "Train Epoch: 185 [30/43 (70%)]\tTrain Loss: 0.027508\n",
      "Train Epoch: 185 [40/43 (93%)]\tTrain Loss: 0.037506\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02825007 0.80298913 0.72568911 0.55536985 0.45143107 0.07619738\n",
      " 0.75281626 0.67176157 0.19174954 0.22446042 0.32617635 0.17858391\n",
      " 0.52907705 0.98029971 0.60669476 0.09787136 0.39999831 0.51250523\n",
      " 0.39678356 0.48320517 0.21795541 0.80919379 0.95795041 0.95285994\n",
      " 0.52750045 0.99104369 0.96072459 0.57353514 0.18586224 0.35277602\n",
      " 0.70110655 0.35951248 0.07585472 0.02654902 0.04260891 0.04396938\n",
      " 0.0449081  0.37596494 0.61039621 0.15045826 0.25840771 0.47205141\n",
      " 0.28929269 0.04667585 0.35392118 0.18825175 0.19542277 0.11820434\n",
      " 0.76369131 0.30230075 0.91605991 0.01594529 0.03853018 0.03241684\n",
      " 0.184342   0.05663948 0.56380659 0.02331538 0.42848971 0.18725438\n",
      " 0.96385849 0.98325932 0.98103482 0.98666418 0.69627261 0.86560988\n",
      " 0.71418238 0.99629837 0.99335843 0.51194006 0.67612332 0.63326174\n",
      " 0.4702712  0.61589581 0.51954669 0.97092932 0.9988625  0.99581617\n",
      " 0.99327868 0.96443731 0.95901459 0.98829287 0.99310368 0.54985213\n",
      " 0.47083652 0.90047008 0.95327991 0.98062104 0.86002409 0.9752847\n",
      " 0.96900046 0.21157582 0.27269012 0.99750453 0.87767476 0.97042447\n",
      " 0.24278386 0.93555915 0.74033308 0.97664195 0.23652245 0.73269045\n",
      " 0.19922094 0.07933791 0.13396785 0.44008702 0.20514201 0.3315706\n",
      " 0.4400568  0.31235221 0.76155871 0.67443526 0.98706484 0.91368163\n",
      " 0.98381984 0.95819861 0.9260636  0.96508425]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 186 [0/43 (0%)]\tTrain Loss: 0.043869\n",
      "Train Epoch: 186 [10/43 (23%)]\tTrain Loss: 0.025224\n",
      "Train Epoch: 186 [20/43 (47%)]\tTrain Loss: 0.022403\n",
      "Train Epoch: 186 [30/43 (70%)]\tTrain Loss: 0.012675\n",
      "Train Epoch: 186 [40/43 (93%)]\tTrain Loss: 0.012859\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02941706 0.56298023 0.46979082 0.32564017 0.30494258 0.05191244\n",
      " 0.66785771 0.5272665  0.17742492 0.14995234 0.28522572 0.18447888\n",
      " 0.52867454 0.97921926 0.26677498 0.09258752 0.24121003 0.3392455\n",
      " 0.63707608 0.30493981 0.19216219 0.80320537 0.95662332 0.93652523\n",
      " 0.62714326 0.96212375 0.97522855 0.592264   0.37513921 0.44525126\n",
      " 0.81797558 0.18541646 0.0532757  0.22497301 0.04136732 0.0288857\n",
      " 0.04286858 0.31021893 0.69321454 0.14299323 0.10195819 0.29694203\n",
      " 0.23301123 0.063202   0.37676725 0.12163591 0.03905651 0.08605754\n",
      " 0.73874277 0.18973626 0.84421378 0.02498203 0.02132449 0.01968572\n",
      " 0.17411794 0.04486548 0.31636688 0.0138768  0.29976434 0.0704736\n",
      " 0.95607686 0.98319948 0.98039699 0.99036789 0.65666193 0.83425486\n",
      " 0.81233442 0.99803716 0.99199378 0.68852967 0.72047335 0.62150043\n",
      " 0.31267709 0.55019933 0.41130167 0.91277665 0.99640852 0.99821031\n",
      " 0.9914214  0.9706322  0.9169957  0.97913319 0.99013102 0.47772416\n",
      " 0.48720685 0.82774657 0.94381458 0.98893422 0.85952622 0.97502488\n",
      " 0.98507035 0.23296446 0.1651863  0.99669647 0.73120743 0.97579169\n",
      " 0.48915982 0.85820156 0.65149611 0.9737587  0.25613633 0.64876395\n",
      " 0.32163632 0.02973009 0.11647164 0.22738653 0.18389255 0.17075722\n",
      " 0.53047961 0.37696931 0.48716763 0.59036744 0.96752787 0.878811\n",
      " 0.93629658 0.94489342 0.87735134 0.95253444]\n",
      "predict [0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 187 [0/43 (0%)]\tTrain Loss: 0.025567\n",
      "Train Epoch: 187 [10/43 (23%)]\tTrain Loss: 0.024922\n",
      "Train Epoch: 187 [20/43 (47%)]\tTrain Loss: 0.035279\n",
      "Train Epoch: 187 [30/43 (70%)]\tTrain Loss: 0.018101\n",
      "Train Epoch: 187 [40/43 (93%)]\tTrain Loss: 0.023408\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03521286 0.80292684 0.61566669 0.62073785 0.54881191 0.10866138\n",
      " 0.72395611 0.69025642 0.11209345 0.37737975 0.35158047 0.31167072\n",
      " 0.51177663 0.97543031 0.69049376 0.23761126 0.59053558 0.37163222\n",
      " 0.72489309 0.43748635 0.43845627 0.79525852 0.99309641 0.9672569\n",
      " 0.79703784 0.99095422 0.97276109 0.74909401 0.39554375 0.53862745\n",
      " 0.89941412 0.49528289 0.04210116 0.33543047 0.10443169 0.07469614\n",
      " 0.05452766 0.47732836 0.82272726 0.31781441 0.39262033 0.42817041\n",
      " 0.21149057 0.11829741 0.38537619 0.14607722 0.35598627 0.10499074\n",
      " 0.88144583 0.27748454 0.38784596 0.20639946 0.05666751 0.02610459\n",
      " 0.20130321 0.05077026 0.40717199 0.0178308  0.2777805  0.24186432\n",
      " 0.97319084 0.9883908  0.98811626 0.98774302 0.64605129 0.89028341\n",
      " 0.8599695  0.99837542 0.99603373 0.63890547 0.96084464 0.86805916\n",
      " 0.47585335 0.8598929  0.6060288  0.96897507 0.99795085 0.99826008\n",
      " 0.99683833 0.96232313 0.97738522 0.99416107 0.98306537 0.57434094\n",
      " 0.6538434  0.93210435 0.96584529 0.98434675 0.93294525 0.9861151\n",
      " 0.9797489  0.57275963 0.27118841 0.9983381  0.94440442 0.97684962\n",
      " 0.3284654  0.95107639 0.84492791 0.97057313 0.38580826 0.82228833\n",
      " 0.57231426 0.05348076 0.16068864 0.55040276 0.33567306 0.23502353\n",
      " 0.65198189 0.14651552 0.33296728 0.78069383 0.98432767 0.96217662\n",
      " 0.98819345 0.97670186 0.88653433 0.97535557]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 188 [0/43 (0%)]\tTrain Loss: 0.033713\n",
      "Train Epoch: 188 [10/43 (23%)]\tTrain Loss: 0.018949\n",
      "Train Epoch: 188 [20/43 (47%)]\tTrain Loss: 0.046021\n",
      "Train Epoch: 188 [30/43 (70%)]\tTrain Loss: 0.051725\n",
      "Train Epoch: 188 [40/43 (93%)]\tTrain Loss: 0.025273\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02332002 0.86174643 0.49991125 0.52621913 0.41754931 0.29247037\n",
      " 0.71784067 0.50459522 0.08783196 0.30992556 0.23342927 0.1625018\n",
      " 0.42110896 0.98635083 0.50472426 0.08412243 0.21207291 0.16437669\n",
      " 0.41498083 0.26776496 0.27158278 0.78504723 0.9782244  0.9714604\n",
      " 0.47445434 0.95252144 0.80618864 0.69273734 0.35413909 0.38885698\n",
      " 0.76350248 0.11901947 0.02860516 0.36869165 0.28520021 0.03451581\n",
      " 0.086634   0.28292161 0.48075461 0.05431443 0.15581363 0.17494886\n",
      " 0.25222605 0.03921152 0.27449167 0.11653139 0.21991105 0.04661438\n",
      " 0.64523745 0.24328957 0.19819467 0.29685074 0.0310413  0.01675043\n",
      " 0.29120308 0.03682261 0.2673533  0.01524477 0.09820244 0.0810957\n",
      " 0.96775579 0.95732474 0.99086398 0.97707307 0.54654217 0.84283763\n",
      " 0.483634   0.9980312  0.99735743 0.57289493 0.7471087  0.63405055\n",
      " 0.60104454 0.6914078  0.53237057 0.92774296 0.99570835 0.99530572\n",
      " 0.98004568 0.96314901 0.94610798 0.98040205 0.99359715 0.45607859\n",
      " 0.38864705 0.83496636 0.94623393 0.95199072 0.89812398 0.9864254\n",
      " 0.97914189 0.20879157 0.19920501 0.99745244 0.86942196 0.98515332\n",
      " 0.38983282 0.84724015 0.63018978 0.98983884 0.32545856 0.47091982\n",
      " 0.17754242 0.04076001 0.13975716 0.11336002 0.22663532 0.58791429\n",
      " 0.36264494 0.3408545  0.28248268 0.6863448  0.97070599 0.90060967\n",
      " 0.94328552 0.77271432 0.86604303 0.93085498]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 189 [0/43 (0%)]\tTrain Loss: 0.042469\n",
      "Train Epoch: 189 [10/43 (23%)]\tTrain Loss: 0.012843\n",
      "Train Epoch: 189 [20/43 (47%)]\tTrain Loss: 0.022443\n",
      "Train Epoch: 189 [30/43 (70%)]\tTrain Loss: 0.043888\n",
      "Train Epoch: 189 [40/43 (93%)]\tTrain Loss: 0.033100\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01328193 0.69193715 0.5929029  0.68221354 0.49716696 0.03629524\n",
      " 0.62488794 0.68127251 0.0748594  0.25314867 0.2072417  0.20207646\n",
      " 0.48423162 0.95951766 0.4289425  0.11805585 0.38703319 0.21128282\n",
      " 0.33412933 0.52392375 0.35274088 0.87881076 0.99167597 0.98494577\n",
      " 0.40263504 0.99101627 0.95672393 0.83008105 0.20936176 0.16795613\n",
      " 0.73515725 0.21364988 0.02101615 0.28936607 0.0635904  0.0551951\n",
      " 0.05174936 0.36773443 0.82122731 0.1374127  0.1263334  0.24497633\n",
      " 0.31419992 0.06171891 0.51780266 0.09314752 0.4543258  0.03396673\n",
      " 0.6968931  0.18084624 0.28739277 0.24535286 0.04806207 0.04391523\n",
      " 0.14419016 0.03123149 0.21148148 0.01404689 0.2435354  0.08767275\n",
      " 0.97166944 0.91243482 0.98184144 0.98820835 0.76754045 0.84435815\n",
      " 0.59941715 0.99847323 0.99357384 0.40886706 0.86304551 0.71282673\n",
      " 0.3871142  0.63043129 0.59204632 0.92269778 0.99656492 0.99391657\n",
      " 0.98887271 0.95867133 0.94445068 0.95883709 0.9801206  0.63507146\n",
      " 0.40367001 0.88509411 0.92584956 0.98910999 0.87088668 0.98684096\n",
      " 0.98432702 0.29490328 0.34570575 0.99612349 0.943308   0.9787547\n",
      " 0.32104507 0.93242198 0.65562356 0.98218542 0.40647882 0.64725304\n",
      " 0.29682994 0.08332426 0.12247627 0.24649316 0.28758004 0.294451\n",
      " 0.64495218 0.25589603 0.47659844 0.66274768 0.95929283 0.9471162\n",
      " 0.93027222 0.98441643 0.82254803 0.97676688]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 190 [0/43 (0%)]\tTrain Loss: 0.050117\n",
      "Train Epoch: 190 [10/43 (23%)]\tTrain Loss: 0.085668\n",
      "Train Epoch: 190 [20/43 (47%)]\tTrain Loss: 0.014105\n",
      "Train Epoch: 190 [30/43 (70%)]\tTrain Loss: 0.018115\n",
      "Train Epoch: 190 [40/43 (93%)]\tTrain Loss: 0.022384\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02328948 0.74125105 0.53964764 0.64904732 0.28611901 0.04638828\n",
      " 0.76015216 0.49445578 0.26288024 0.27532175 0.33799466 0.34675959\n",
      " 0.42977169 0.93943048 0.4489947  0.07598336 0.40129426 0.40030882\n",
      " 0.53988844 0.23931395 0.15758251 0.82960951 0.98050135 0.9762966\n",
      " 0.58412349 0.98323405 0.97603613 0.5767293  0.30534285 0.30528274\n",
      " 0.60272044 0.24869247 0.02829005 0.35450891 0.01882383 0.02243069\n",
      " 0.06162897 0.34444973 0.59970158 0.07638302 0.28153348 0.32788032\n",
      " 0.22143544 0.15925066 0.47525531 0.09548105 0.16425557 0.09156082\n",
      " 0.60981774 0.24322638 0.90616602 0.20315336 0.01878795 0.02360447\n",
      " 0.24222507 0.0232948  0.47573996 0.03037331 0.3899796  0.06908938\n",
      " 0.94339609 0.92710507 0.9797591  0.98426312 0.5468322  0.92354774\n",
      " 0.72120976 0.99801862 0.99609274 0.48462817 0.72164059 0.69836634\n",
      " 0.42594969 0.70978326 0.37342542 0.96092045 0.99812204 0.99710888\n",
      " 0.99399096 0.97877502 0.94226635 0.97017878 0.98433119 0.36319482\n",
      " 0.25467607 0.8903566  0.97747117 0.98441309 0.86248958 0.97678441\n",
      " 0.97163433 0.31864291 0.25769532 0.99624664 0.78621525 0.98578298\n",
      " 0.31723848 0.84049463 0.66368324 0.98981547 0.22375654 0.69283831\n",
      " 0.30630553 0.06935111 0.05538864 0.32424775 0.33740011 0.21137904\n",
      " 0.76734406 0.3629584  0.5998069  0.74380529 0.96891093 0.9298588\n",
      " 0.95493269 0.808321   0.76783085 0.97813946]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 44 TN= 41 FN= 14 FP= 19\n",
      "TP+FP 63\n",
      "precision 0.6984126984126984\n",
      "recall 0.7586206896551724\n",
      "F1 0.7272727272727273\n",
      "acc 0.7203389830508474\n",
      "AUCp 0.7209770114942529\n",
      "AUC 0.8120689655172413\n",
      "\n",
      " The epoch is 190, average recall: 0.7586, average precision: 0.6984,average F1: 0.7273, average accuracy: 0.7203, average AUC: 0.8121\n",
      "Train Epoch: 191 [0/43 (0%)]\tTrain Loss: 0.031580\n",
      "Train Epoch: 191 [10/43 (23%)]\tTrain Loss: 0.016493\n",
      "Train Epoch: 191 [20/43 (47%)]\tTrain Loss: 0.034675\n",
      "Train Epoch: 191 [30/43 (70%)]\tTrain Loss: 0.031352\n",
      "Train Epoch: 191 [40/43 (93%)]\tTrain Loss: 0.014870\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02515136 0.79061121 0.80205852 0.71966445 0.42000553 0.04851599\n",
      " 0.73986888 0.6004861  0.32710379 0.21404463 0.17705232 0.21687311\n",
      " 0.43040568 0.97965091 0.63630265 0.17258842 0.31356692 0.21154566\n",
      " 0.38330826 0.53710675 0.30317885 0.81532031 0.98554176 0.97881067\n",
      " 0.58974725 0.97849268 0.95967495 0.86139286 0.33088151 0.24840114\n",
      " 0.63089567 0.29052007 0.02109241 0.27725366 0.02641051 0.04038879\n",
      " 0.03510514 0.50662947 0.77766955 0.0657449  0.13255887 0.13425329\n",
      " 0.17768222 0.08880078 0.41208345 0.15070908 0.17025234 0.07453778\n",
      " 0.79450816 0.20860095 0.64775664 0.34480089 0.0249727  0.0187222\n",
      " 0.21037282 0.10016517 0.1766942  0.01708653 0.25320184 0.10684296\n",
      " 0.93393189 0.96882296 0.97606033 0.99076325 0.70619643 0.87249362\n",
      " 0.69733548 0.99939895 0.99506181 0.69810385 0.70835447 0.50920898\n",
      " 0.46167612 0.80775541 0.70103985 0.94816178 0.99729222 0.99751842\n",
      " 0.99065572 0.96999985 0.9418776  0.98489285 0.9812004  0.65317136\n",
      " 0.54922307 0.89080197 0.97082609 0.98522598 0.90636742 0.98280048\n",
      " 0.97674781 0.40726182 0.20968758 0.99542254 0.91190988 0.96022707\n",
      " 0.18725675 0.96272975 0.53068745 0.9889248  0.22660221 0.76362783\n",
      " 0.25356334 0.04036367 0.09707849 0.34219608 0.19306548 0.36230576\n",
      " 0.59921163 0.19450179 0.55155587 0.66332138 0.97854006 0.95086426\n",
      " 0.96816838 0.86830837 0.9036454  0.97333312]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 192 [0/43 (0%)]\tTrain Loss: 0.040535\n",
      "Train Epoch: 192 [10/43 (23%)]\tTrain Loss: 0.016443\n",
      "Train Epoch: 192 [20/43 (47%)]\tTrain Loss: 0.013137\n",
      "Train Epoch: 192 [30/43 (70%)]\tTrain Loss: 0.018825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 192 [40/43 (93%)]\tTrain Loss: 0.019761\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02346364 0.65588337 0.26207733 0.37604019 0.35774311 0.02887155\n",
      " 0.35695201 0.65856373 0.15184616 0.19823164 0.12027183 0.25438538\n",
      " 0.34287557 0.97952718 0.23320086 0.15939564 0.2617825  0.20637543\n",
      " 0.41089842 0.33841497 0.20849937 0.84908658 0.96245646 0.93313932\n",
      " 0.61919075 0.98516774 0.9697035  0.54291379 0.2467763  0.3540549\n",
      " 0.79630756 0.24333255 0.03262888 0.15062912 0.04752053 0.02511956\n",
      " 0.03308868 0.30198756 0.5411306  0.22383523 0.14902778 0.1759408\n",
      " 0.23185846 0.04052818 0.18104103 0.06136071 0.30986908 0.08216152\n",
      " 0.68277037 0.20344765 0.48942515 0.46568325 0.04698183 0.02681755\n",
      " 0.16940729 0.02719005 0.15907507 0.0047189  0.2307459  0.06326276\n",
      " 0.91710126 0.97156107 0.97329897 0.97615957 0.57681239 0.89277178\n",
      " 0.77146387 0.99846184 0.99469984 0.37469447 0.79834163 0.75801247\n",
      " 0.5174166  0.66145122 0.44723621 0.87340283 0.98719692 0.99214488\n",
      " 0.98057383 0.95525247 0.93079495 0.9764142  0.98674393 0.5096485\n",
      " 0.34670162 0.83734077 0.94765234 0.98658723 0.84293151 0.98545039\n",
      " 0.98854625 0.31842187 0.22891538 0.99814928 0.89948857 0.94496274\n",
      " 0.22861779 0.77376646 0.56288439 0.9724372  0.12533034 0.4665361\n",
      " 0.11042728 0.03273346 0.06200005 0.16423099 0.20446111 0.27574781\n",
      " 0.3609837  0.17118615 0.48802072 0.65807229 0.97982842 0.92369884\n",
      " 0.95308161 0.97714591 0.87072867 0.96634865]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 193 [0/43 (0%)]\tTrain Loss: 0.047171\n",
      "Train Epoch: 193 [10/43 (23%)]\tTrain Loss: 0.074179\n",
      "Train Epoch: 193 [20/43 (47%)]\tTrain Loss: 0.013703\n",
      "Train Epoch: 193 [30/43 (70%)]\tTrain Loss: 0.049650\n",
      "Train Epoch: 193 [40/43 (93%)]\tTrain Loss: 0.019354\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.0226355  0.73946005 0.67472738 0.7818898  0.56412971 0.53112102\n",
      " 0.56496102 0.69094563 0.20449902 0.24460134 0.25062582 0.28316346\n",
      " 0.47876123 0.97080874 0.62558538 0.18452489 0.38230208 0.34685475\n",
      " 0.27333564 0.20633228 0.39681792 0.86135954 0.89621556 0.97861862\n",
      " 0.72558057 0.88734376 0.27106136 0.69244564 0.30966812 0.32016888\n",
      " 0.89347976 0.38148203 0.0215025  0.17857602 0.03632396 0.02239364\n",
      " 0.08652914 0.43513715 0.45611721 0.16153206 0.24590716 0.24292669\n",
      " 0.27549496 0.09494741 0.54915929 0.16391596 0.10265696 0.04283955\n",
      " 0.72629488 0.30585471 0.51875317 0.24796666 0.05079647 0.01362405\n",
      " 0.24024928 0.03869525 0.14057453 0.02002784 0.24148239 0.06111184\n",
      " 0.94810057 0.95231646 0.98519903 0.9695313  0.80803478 0.86439592\n",
      " 0.73904425 0.99847525 0.99342752 0.74432445 0.74764895 0.75549394\n",
      " 0.30645978 0.75943238 0.65626967 0.92592108 0.99643433 0.99520999\n",
      " 0.99448276 0.97589672 0.95162797 0.97484267 0.97537154 0.55060947\n",
      " 0.42372045 0.83509392 0.96339846 0.97720188 0.70634013 0.97422862\n",
      " 0.98448086 0.34862009 0.34095994 0.99316311 0.83545816 0.97635967\n",
      " 0.33724692 0.86477256 0.68985909 0.97722864 0.27007169 0.44592124\n",
      " 0.42043254 0.0756685  0.15371691 0.25423798 0.29941216 0.40365329\n",
      " 0.71710652 0.30090919 0.81050026 0.73674929 0.94982171 0.91297156\n",
      " 0.93942934 0.62609011 0.89825958 0.97408152]\n",
      "predict [0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 194 [0/43 (0%)]\tTrain Loss: 0.026452\n",
      "Train Epoch: 194 [10/43 (23%)]\tTrain Loss: 0.012211\n",
      "Train Epoch: 194 [20/43 (47%)]\tTrain Loss: 0.020930\n",
      "Train Epoch: 194 [30/43 (70%)]\tTrain Loss: 0.017098\n",
      "Train Epoch: 194 [40/43 (93%)]\tTrain Loss: 0.029652\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03560147 0.93312883 0.61333549 0.73639512 0.46798202 0.31327373\n",
      " 0.67262888 0.61253017 0.11256665 0.24482629 0.33758169 0.23747876\n",
      " 0.46925402 0.98219013 0.64630717 0.13772683 0.16453589 0.44928494\n",
      " 0.21853276 0.37245271 0.24050654 0.8631348  0.97038472 0.96514124\n",
      " 0.70016396 0.98332548 0.94947296 0.85266733 0.45713571 0.31841967\n",
      " 0.77882731 0.28045905 0.04351089 0.24487408 0.33621675 0.03273498\n",
      " 0.11463176 0.40280259 0.5185228  0.24375524 0.09920253 0.38957027\n",
      " 0.19671398 0.06093972 0.63018328 0.10644063 0.16312352 0.15608555\n",
      " 0.76880115 0.26898301 0.23450203 0.20997845 0.03862052 0.03172322\n",
      " 0.20159467 0.06984716 0.18062676 0.01405898 0.22131814 0.08233283\n",
      " 0.97858584 0.95439458 0.98535687 0.97898591 0.66705614 0.90309411\n",
      " 0.86150545 0.99748659 0.98809183 0.59926623 0.77006817 0.8268258\n",
      " 0.61666757 0.76755112 0.40075019 0.96068871 0.99436778 0.99598712\n",
      " 0.99455547 0.97815609 0.9618836  0.97301763 0.98377359 0.6645658\n",
      " 0.36703929 0.8870104  0.86972618 0.98155606 0.8476072  0.98906773\n",
      " 0.97293526 0.26886281 0.44087252 0.99861085 0.94719994 0.97319621\n",
      " 0.34844595 0.82650095 0.56416804 0.94109339 0.44109911 0.65423495\n",
      " 0.21692027 0.22511591 0.08489374 0.45206985 0.22899602 0.30089676\n",
      " 0.44135782 0.23575023 0.1570459  0.78286093 0.97182685 0.96357208\n",
      " 0.98730087 0.77555233 0.89926553 0.96755141]\n",
      "predict [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 195 [0/43 (0%)]\tTrain Loss: 0.050326\n",
      "Train Epoch: 195 [10/43 (23%)]\tTrain Loss: 0.019904\n",
      "Train Epoch: 195 [20/43 (47%)]\tTrain Loss: 0.030974\n",
      "Train Epoch: 195 [30/43 (70%)]\tTrain Loss: 0.023577\n",
      "Train Epoch: 195 [40/43 (93%)]\tTrain Loss: 0.021508\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.03183762 0.91319716 0.64870256 0.68623835 0.6083883  0.04576993\n",
      " 0.93593234 0.61858809 0.23505828 0.21199472 0.30717069 0.19875957\n",
      " 0.42287844 0.96092433 0.39297765 0.16370986 0.37233406 0.39293122\n",
      " 0.39936599 0.44550848 0.35995308 0.87022883 0.98702973 0.98571765\n",
      " 0.78723383 0.96720093 0.93906242 0.83186281 0.49696773 0.60676372\n",
      " 0.81528443 0.3363075  0.03910821 0.2884329  0.01478277 0.03898536\n",
      " 0.05104438 0.38589185 0.53555542 0.13647249 0.26377678 0.17926016\n",
      " 0.16693538 0.10431827 0.7027058  0.11638452 0.07140291 0.14660408\n",
      " 0.78834605 0.26970372 0.8758378  0.32311633 0.0844906  0.02831487\n",
      " 0.19719994 0.0485421  0.29524764 0.03647409 0.15992916 0.10316325\n",
      " 0.96642315 0.95892185 0.97435546 0.96798003 0.81633425 0.78726715\n",
      " 0.81218702 0.99851567 0.98962921 0.65299875 0.82500631 0.75829512\n",
      " 0.50290126 0.54362041 0.68894988 0.94451565 0.99840707 0.99489248\n",
      " 0.99736553 0.95892811 0.95354664 0.97847718 0.98899788 0.75827789\n",
      " 0.36199176 0.86621207 0.9565146  0.951527   0.90184152 0.99110729\n",
      " 0.97757852 0.4829244  0.27116466 0.99757379 0.9172501  0.97880846\n",
      " 0.22921796 0.89286572 0.71798897 0.95863879 0.35910854 0.77019542\n",
      " 0.37218526 0.1184909  0.16075063 0.31930941 0.16127752 0.50534606\n",
      " 0.57783312 0.17030674 0.67819387 0.79367143 0.94908983 0.89746076\n",
      " 0.9881224  0.97342366 0.83794683 0.96772051]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 196 [0/43 (0%)]\tTrain Loss: 0.013596\n",
      "Train Epoch: 196 [10/43 (23%)]\tTrain Loss: 0.023604\n",
      "Train Epoch: 196 [20/43 (47%)]\tTrain Loss: 0.023160\n",
      "Train Epoch: 196 [30/43 (70%)]\tTrain Loss: 0.029178\n",
      "Train Epoch: 196 [40/43 (93%)]\tTrain Loss: 0.038220\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02144272 0.88222212 0.73570806 0.54732096 0.6348961  0.22183397\n",
      " 0.86897051 0.73223239 0.25426733 0.16699494 0.22647578 0.18367314\n",
      " 0.63823217 0.95812142 0.59374905 0.12951903 0.36041147 0.4092713\n",
      " 0.2267203  0.50068915 0.20600484 0.79956353 0.99088836 0.87135297\n",
      " 0.63244992 0.96228302 0.95659673 0.64520061 0.30110931 0.25087509\n",
      " 0.85055643 0.33705461 0.07075122 0.20276204 0.2158694  0.02884849\n",
      " 0.06590239 0.18597431 0.76401514 0.17685884 0.17049167 0.22402038\n",
      " 0.26684663 0.10314129 0.40698484 0.19970828 0.38424209 0.13807295\n",
      " 0.76761341 0.30142224 0.17695667 0.19013284 0.35799557 0.01834839\n",
      " 0.25075004 0.0575896  0.10398189 0.01142852 0.25123641 0.10859896\n",
      " 0.93202531 0.97749102 0.98934668 0.98757428 0.51039714 0.80007851\n",
      " 0.77469546 0.99777168 0.98284322 0.73911148 0.60014945 0.85062295\n",
      " 0.39546299 0.66656601 0.30430552 0.9593516  0.99778616 0.99539989\n",
      " 0.98979527 0.95729393 0.97254431 0.98511916 0.99129748 0.77944815\n",
      " 0.53524208 0.91857952 0.97984022 0.99451691 0.96616757 0.98589063\n",
      " 0.97028595 0.44448385 0.33555943 0.99644047 0.89851797 0.9674924\n",
      " 0.31738034 0.83792603 0.92448348 0.98181331 0.15782587 0.88561743\n",
      " 0.19511747 0.06861872 0.30153003 0.23811027 0.14597371 0.17217308\n",
      " 0.67057902 0.26578915 0.16534896 0.81564158 0.97847795 0.95058191\n",
      " 0.98232275 0.98397982 0.8499642  0.9723016 ]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 197 [0/43 (0%)]\tTrain Loss: 0.023443\n",
      "Train Epoch: 197 [10/43 (23%)]\tTrain Loss: 0.024099\n",
      "Train Epoch: 197 [20/43 (47%)]\tTrain Loss: 0.019337\n",
      "Train Epoch: 197 [30/43 (70%)]\tTrain Loss: 0.026715\n",
      "Train Epoch: 197 [40/43 (93%)]\tTrain Loss: 0.017780\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.0165322  0.76627094 0.64564872 0.74098891 0.53896296 0.19869617\n",
      " 0.64396942 0.57147497 0.13431418 0.18002081 0.38127095 0.30896357\n",
      " 0.58956921 0.96348375 0.57770902 0.1159242  0.31969166 0.19941084\n",
      " 0.19939785 0.28561345 0.15001097 0.77110732 0.98579222 0.97701728\n",
      " 0.58859128 0.95240176 0.98146582 0.63798797 0.27769774 0.33759266\n",
      " 0.73831797 0.15663491 0.03570142 0.30739269 0.03151445 0.0700568\n",
      " 0.04069546 0.55182868 0.46791655 0.10887613 0.21094248 0.17630398\n",
      " 0.23676324 0.10149141 0.45774335 0.21723837 0.18335196 0.06388284\n",
      " 0.78018653 0.22787146 0.6842792  0.27624196 0.05207545 0.02443925\n",
      " 0.21769807 0.06704131 0.3747763  0.01011877 0.26593763 0.07259668\n",
      " 0.97075093 0.92428935 0.98367161 0.97422862 0.61023754 0.82668757\n",
      " 0.57208133 0.99807322 0.99410361 0.67207783 0.80349106 0.54985654\n",
      " 0.57551527 0.71660894 0.50860006 0.96596396 0.99695194 0.99797207\n",
      " 0.98973376 0.96109682 0.97342807 0.98510486 0.98309511 0.50185561\n",
      " 0.28284964 0.87280208 0.94432926 0.99425465 0.81727034 0.99569076\n",
      " 0.99482983 0.23079309 0.51564783 0.99724352 0.90307969 0.99139118\n",
      " 0.29774505 0.91500187 0.53590965 0.97255319 0.26761413 0.73485613\n",
      " 0.19427314 0.05385325 0.12668942 0.25477397 0.21451949 0.28317416\n",
      " 0.54514432 0.15170397 0.38878584 0.71385789 0.97554547 0.92985117\n",
      " 0.96266699 0.90850145 0.8634041  0.98016161]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 198 [0/43 (0%)]\tTrain Loss: 0.047186\n",
      "Train Epoch: 198 [10/43 (23%)]\tTrain Loss: 0.014118\n",
      "Train Epoch: 198 [20/43 (47%)]\tTrain Loss: 0.034726\n",
      "Train Epoch: 198 [30/43 (70%)]\tTrain Loss: 0.034288\n",
      "Train Epoch: 198 [40/43 (93%)]\tTrain Loss: 0.033645\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.02739531 0.81578261 0.58429903 0.6674543  0.54254836 0.05013755\n",
      " 0.78881359 0.7133835  0.18318084 0.19124824 0.35914442 0.20908941\n",
      " 0.44458583 0.97549385 0.51722699 0.08161391 0.60784417 0.26250154\n",
      " 0.41167608 0.38985246 0.47666669 0.85292071 0.9796409  0.96903127\n",
      " 0.46971235 0.94926101 0.98014909 0.85398102 0.38986608 0.47247297\n",
      " 0.70460105 0.30357814 0.04552129 0.07453246 0.01272112 0.03523599\n",
      " 0.0635169  0.4643648  0.84517789 0.17158295 0.12597486 0.33135852\n",
      " 0.29947788 0.15906972 0.59868097 0.111568   0.08974652 0.03329696\n",
      " 0.72699755 0.34889245 0.79527098 0.27014589 0.03671844 0.01131825\n",
      " 0.1920542  0.06302404 0.41916108 0.01648866 0.31596184 0.11661939\n",
      " 0.95766616 0.97421432 0.9847067  0.98884088 0.71434748 0.89963663\n",
      " 0.81494749 0.99576181 0.9926905  0.62096089 0.853333   0.83683509\n",
      " 0.7356168  0.79772514 0.47430632 0.90155137 0.9962759  0.99831319\n",
      " 0.98574024 0.96967053 0.94410861 0.96765143 0.99084079 0.64290363\n",
      " 0.4712773  0.81320721 0.95848119 0.9765678  0.82659322 0.98606282\n",
      " 0.97674096 0.27534965 0.28450924 0.99679011 0.9061116  0.97546458\n",
      " 0.51987863 0.86567444 0.69867259 0.97744828 0.19052851 0.71642178\n",
      " 0.26084208 0.21608649 0.18042007 0.28546312 0.22475651 0.60121238\n",
      " 0.81727469 0.55346864 0.89943421 0.84764385 0.97756767 0.96146178\n",
      " 0.9831292  0.96999454 0.88632727 0.98816842]\n",
      "predict [0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Train Epoch: 199 [0/43 (0%)]\tTrain Loss: 0.040692\n",
      "Train Epoch: 199 [10/43 (23%)]\tTrain Loss: 0.019073\n",
      "Train Epoch: 199 [20/43 (47%)]\tTrain Loss: 0.058828\n",
      "Train Epoch: 199 [30/43 (70%)]\tTrain Loss: 0.016133\n",
      "Train Epoch: 199 [40/43 (93%)]\tTrain Loss: 0.065518\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01771369 0.69520944 0.49952978 0.55689114 0.25569448 0.05696703\n",
      " 0.57783884 0.63530278 0.11890057 0.23522861 0.32431671 0.10787294\n",
      " 0.43811572 0.97748315 0.30379346 0.10654123 0.35329977 0.43899611\n",
      " 0.31889325 0.26264989 0.25561368 0.67697513 0.97633672 0.93915349\n",
      " 0.37972125 0.95043319 0.940965   0.62186408 0.08183037 0.15243748\n",
      " 0.60956722 0.11760198 0.04931363 0.25156003 0.01250355 0.0355743\n",
      " 0.02759437 0.3602041  0.47917473 0.03855065 0.04622473 0.10704858\n",
      " 0.27760136 0.08162496 0.42582342 0.08112271 0.03615165 0.04617482\n",
      " 0.70071566 0.28238174 0.626993   0.40531144 0.04599334 0.00833503\n",
      " 0.2757507  0.01373161 0.19149375 0.01052853 0.16477272 0.09644183\n",
      " 0.95071167 0.91619736 0.96355671 0.94034088 0.62885547 0.84684461\n",
      " 0.65774792 0.99805439 0.99038261 0.30819443 0.72483605 0.63273478\n",
      " 0.38352123 0.57736182 0.27020112 0.87288141 0.99760371 0.99701118\n",
      " 0.98673147 0.95393384 0.89909148 0.97427779 0.98114717 0.16904663\n",
      " 0.37790152 0.66364121 0.96082217 0.98699641 0.64329541 0.98167622\n",
      " 0.97247994 0.29219037 0.19097877 0.99780506 0.88947475 0.94359565\n",
      " 0.28501678 0.84808707 0.48447967 0.96200848 0.13367602 0.60116279\n",
      " 0.09549534 0.03663158 0.06317258 0.15514307 0.27331275 0.11821475\n",
      " 0.38229206 0.20877481 0.45967415 0.44070327 0.97156477 0.88856441\n",
      " 0.93648064 0.84652507 0.79789615 0.96010089]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 200 [0/43 (0%)]\tTrain Loss: 0.024166\n",
      "Train Epoch: 200 [10/43 (23%)]\tTrain Loss: 0.037137\n",
      "Train Epoch: 200 [20/43 (47%)]\tTrain Loss: 0.021726\n",
      "Train Epoch: 200 [30/43 (70%)]\tTrain Loss: 0.018062\n",
      "Train Epoch: 200 [40/43 (93%)]\tTrain Loss: 0.014728\n",
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.01681352 0.84763169 0.4346666  0.51092649 0.3018083  0.28555039\n",
      " 0.63390833 0.54263073 0.15481611 0.18130475 0.31867221 0.25248218\n",
      " 0.34886125 0.96562684 0.36014846 0.10464563 0.32704434 0.1200177\n",
      " 0.32609168 0.23243466 0.27429426 0.79797715 0.95176411 0.981547\n",
      " 0.73161471 0.96946543 0.93793255 0.56643718 0.19117865 0.36258459\n",
      " 0.76179403 0.28175032 0.03339989 0.31047916 0.32409546 0.01261747\n",
      " 0.03841074 0.28175378 0.68534744 0.10649451 0.10994457 0.26622093\n",
      " 0.22018503 0.09072109 0.33824471 0.06921151 0.22047576 0.06077511\n",
      " 0.66672313 0.26661831 0.23370358 0.28286782 0.01752269 0.0232108\n",
      " 0.21648571 0.03347191 0.33798051 0.00729529 0.19422886 0.06922145\n",
      " 0.90689957 0.90630192 0.97988361 0.97065037 0.65830159 0.87311023\n",
      " 0.7016201  0.99880254 0.99302292 0.5048933  0.67302734 0.67321146\n",
      " 0.53044277 0.4626644  0.54709405 0.89200914 0.99647647 0.99452066\n",
      " 0.99063873 0.97256958 0.91890597 0.96005619 0.99257624 0.2107482\n",
      " 0.36342219 0.86103052 0.8422212  0.99078536 0.79103249 0.9827053\n",
      " 0.95778143 0.36676311 0.24115942 0.99547702 0.87183028 0.96222532\n",
      " 0.16646169 0.85905522 0.75035518 0.95966893 0.21105771 0.63836092\n",
      " 0.16794816 0.09694853 0.17931958 0.21510869 0.3308512  0.2941272\n",
      " 0.40314531 0.13161054 0.27082387 0.63258249 0.962125   0.91181433\n",
      " 0.96395141 0.8651908  0.84624976 0.96776319]\n",
      "predict [0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vote_pred [0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "targetlist [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "TP= 44 TN= 42 FN= 14 FP= 18\n",
      "TP+FP 62\n",
      "precision 0.7096774193548387\n",
      "recall 0.7586206896551724\n",
      "F1 0.7333333333333333\n",
      "acc 0.7288135593220338\n",
      "AUCp 0.7293103448275862\n",
      "AUC 0.8166666666666668\n",
      "\n",
      " The epoch is 200, average recall: 0.7586, average precision: 0.7097,average F1: 0.7333, average accuracy: 0.7288, average AUC: 0.8167\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "bs = batchsize\n",
    "votenum = 10\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0# FP = 0\n",
    "vote_pred = np.zeros(valset.__len__())\n",
    "vote_score = np.zeros(valset.__len__())\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9, weight_decay=1e-4)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "total_epoch = 200\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epoch)\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
    "# scheduler = StepLR(optimizer, step_size=1)\n",
    "\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "\n",
    "    scheduler.step()\n",
    "    train(optimizer, epoch)\n",
    "\n",
    "    targetlist, scorelist, predlist = val(epoch)\n",
    "    print('target', targetlist)\n",
    "    print('score', scorelist)\n",
    "    print('predict', predlist)\n",
    "    vote_pred = vote_pred + predlist\n",
    "    vote_score = vote_score + scorelist\n",
    "\n",
    "    if epoch % votenum == 0:\n",
    "        # major vote\n",
    "        vote_pred[vote_pred <= (votenum / 2)] = 0\n",
    "        vote_pred[vote_pred > (votenum / 2)] = 1\n",
    "        vote_score = vote_score / votenum\n",
    "\n",
    "        print('vote_pred', vote_pred)\n",
    "        print('targetlist', targetlist)\n",
    "        TP = ((vote_pred == 1) & (targetlist == 1)).sum()\n",
    "        TN = ((vote_pred == 0) & (targetlist == 0)).sum()\n",
    "        FN = ((vote_pred == 0) & (targetlist == 1)).sum()\n",
    "        FP = ((vote_pred == 1) & (targetlist == 0)).sum()\n",
    "\n",
    "        print('TP=', TP, 'TN=', TN, 'FN=', FN, 'FP=', FP)\n",
    "        print('TP+FP', TP + FP)\n",
    "        p = TP / (TP + FP)\n",
    "        print('precision', p)\n",
    "        p = TP / (TP + FP)\n",
    "        r = TP / (TP + FN)\n",
    "        print('recall', r)\n",
    "        F1 = 2 * r * p / (r + p)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        print('F1', F1)\n",
    "        print('acc', acc)\n",
    "        AUC = roc_auc_score(targetlist, vote_score)\n",
    "        print('AUCp', roc_auc_score(targetlist, vote_pred))\n",
    "        print('AUC', AUC)\n",
    "\n",
    "        #         if epoch == total_epoch:\n",
    "        torch.save(model.state_dict(), \"save_model_dense1/moco_covid_myDensenet++.pt\")\n",
    "\n",
    "        vote_pred = np.zeros(valset.__len__())\n",
    "        vote_score = np.zeros(valset.__len__())\n",
    "        print('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "            epoch, r, p, F1, acc, AUC))\n",
    "\n",
    "#         f = open('model_result/medical_transfer/{}_{}.txt'.format(modelname,alpha_name), 'a+')\n",
    "#         f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "# average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "#         epoch, r, p, F1, acc, AUC))\n",
    "#         f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "score [0.11530774 0.90626496 0.07680441 0.20843104 0.04172014 0.09918072\n",
      " 0.18876237 0.46347854 0.42841253 0.5834192  0.17875092 0.0404198\n",
      " 0.25836387 0.15636902 0.45350969 0.42124134 0.16159938 0.16489233\n",
      " 0.16034035 0.22888568 0.27367544 0.04541839 0.12521885 0.03307899\n",
      " 0.03238962 0.06689053 0.18618716 0.37136933 0.47643703 0.20104778\n",
      " 0.10333908 0.84698534 0.88879699 0.93505472 0.98174518 0.87196505\n",
      " 0.67578518 0.77180004 0.24406065 0.23317066 0.20602481 0.38958454\n",
      " 0.48421976 0.06134374 0.14798145 0.17642103 0.1750109  0.0538472\n",
      " 0.36867413 0.18131806 0.1382827  0.23432776 0.20498449 0.25968152\n",
      " 0.33849999 0.29366308 0.34447721 0.22438435 0.96949995 0.98493052\n",
      " 0.25163993 0.14386946 0.16837512 0.12349762 0.49773359 0.24450344\n",
      " 0.22794004 0.19100371 0.56218821 0.90061796 0.11724696 0.42629895\n",
      " 0.04915501 0.43890047 0.14875393 0.01547772 0.15812111 0.43489796\n",
      " 0.41354898 0.34572825 0.06859185 0.42144847 0.65204018 0.72370744\n",
      " 0.32229114 0.10274016 0.54530209 0.27009389 0.15599941 0.56400526\n",
      " 0.80915034 0.94714856 0.59110421 0.96450472 0.9143225  0.61275721\n",
      " 0.47899416 0.8834675  0.98438823 0.95852572 0.91912103 0.99110806\n",
      " 0.96639699 0.49200726 0.89952666 0.96846205 0.91344148 0.96977127\n",
      " 0.98191595 0.9746874  0.96967262 0.99550796 0.99201828 0.96291763\n",
      " 0.97650242 0.99120128 0.5040316  0.99773312 0.99889648 0.99640584\n",
      " 0.99348962 0.99664527 0.99221659 0.99587566 0.99038357 0.98654222\n",
      " 0.96406019 0.95291156 0.95694691 0.99510878 0.9851318  0.98392236\n",
      " 0.9827801  0.98998219 0.9853875  0.94633687 0.97951251 0.98685592\n",
      " 0.98597312 0.99199367 0.98938835 0.99030602 0.96572667 0.81528693\n",
      " 0.87295592 0.78996611 0.92796963 0.91808218 0.46656933 0.99126667\n",
      " 0.23936321 0.34056026 0.87698162 0.98260236 0.81888658 0.91030478\n",
      " 0.94566816 0.96903741 0.92732829 0.13572541 0.26220936 0.35643983\n",
      " 0.89104635 0.99740261 0.31125262 0.79227054 0.56007141 0.9321847\n",
      " 0.98349416 0.3120423  0.26302817 0.27588224 0.15333198 0.88846177\n",
      " 0.22880869 0.65847844 0.69193733 0.80684835 0.81879103 0.15622115\n",
      " 0.27656931 0.23610069 0.34244964 0.87007457 0.96756178 0.96834612\n",
      " 0.98470265 0.10014883 0.16561532 0.26001278 0.07756793 0.21014979\n",
      " 0.96193016 0.10999706 0.85269767 0.69932044 0.9948144  0.99390066\n",
      " 0.98341841 0.98817784 0.86559612 0.69316268 0.28785953]\n",
      "predict [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "TP= 81 TN= 74 FN= 24 FP= 24\n",
      "TP+FP 105\n",
      "precision 0.7714285714285715\n",
      "recall 0.7714285714285715\n",
      "F1 0.7714285714285715\n",
      "acc 0.7635467980295566\n",
      "AUC 0.8346938775510205\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "bs = 10\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "epoch = 1\n",
    "r_list = []\n",
    "p_list = []\n",
    "acc_list = []\n",
    "AUC_list = []\n",
    "# TP = 0\n",
    "# TN = 0\n",
    "# FN = 0\n",
    "# FP = 0\n",
    "vote_pred = np.zeros(testset.__len__())\n",
    "vote_score = np.zeros(testset.__len__())\n",
    "\n",
    "targetlist, scorelist, predlist = test(epoch)\n",
    "print('target', targetlist)\n",
    "print('score', scorelist)\n",
    "print('predict', predlist)\n",
    "vote_pred = vote_pred + predlist\n",
    "vote_score = vote_score + scorelist\n",
    "\n",
    "TP = ((predlist == 1) & (targetlist == 1)).sum()\n",
    "\n",
    "TN = ((predlist == 0) & (targetlist == 0)).sum()\n",
    "FN = ((predlist == 0) & (targetlist == 1)).sum()\n",
    "FP = ((predlist == 1) & (targetlist == 0)).sum()\n",
    "\n",
    "print('TP=', TP, 'TN=', TN, 'FN=', FN, 'FP=', FP)\n",
    "print('TP+FP', TP + FP)\n",
    "p = TP / (TP + FP)\n",
    "print('precision', p)\n",
    "p = TP / (TP + FP)\n",
    "r = TP / (TP + FN)\n",
    "print('recall', r)\n",
    "F1 = 2 * r * p / (r + p)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "print('F1', F1)\n",
    "print('acc', acc)\n",
    "AUC = roc_auc_score(targetlist, vote_score)\n",
    "print('AUC', AUC)\n",
    "\n",
    "# f = open(f'model_result/medical_transfer/test_{modelname}_{alpha_name}_LUNA_moco_CT_moco.txt', 'a+')\n",
    "# f.write('\\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\\\n",
    "# average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(\n",
    "# epoch, r, p, F1, acc, AUC))\n",
    "# f.close()\n",
    "# torch.save(model.state_dict(), \"model_backup/medical_transfer/{}_{}_covid_moco_covid.pt\".format(modelname,alpha_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
